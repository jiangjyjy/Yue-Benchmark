"A 6-sided die is rolled 15 times and the results are: side 1 comes up 0 times; side 2: 1 time; side 3: 2 times; side 4: 3 times; side 5: 4 times; side 6: 5 times. Based on these results, what is the probability of side 3 coming up when using Add-1 Smoothing?",2.0/15,1.0/7,3.0/16,1.0/5,B
自然影像中最常見的影像數據擴增是哪一種？,隨意剪裁同埋水平翻轉,隨機剪裁同埋垂直翻轉,海報化,唔使猶豫，快啲做啦。,A
你而家係評審世界最潮機器學習嘅會議論文，見到有人投稿聲稱以下嘅嘢。邊啲你會考慮接受？,我個方法嘅訓練誤差係低過所有之前嘅方法！,我嘅方法可以做到比之前所有嘅方法都低嘅測試誤差！（註：當正則化參數λ被選擇咗，以至最小化測試誤差嘅時候。）,我個方法嘅測試錯誤率低過所有之前嘅方法！（註：當選擇正則化參數λ使交叉驗證錯誤最小化時。）,我嘅方法係可以做到交叉驗證誤差比所有之前嘅方法都低！（註：當正規化參數λ係選擇咁樣去最小化交叉驗證誤差嘅時候。）,C
要想达到0/1損失的估計值低於真實0/1損失的1%（機率95%），根據胡夫丁不等式，獨立同分布的測試集必須有多少個例子？,大約有10個例子,大約100個例子,由100至500个例子之间,多過1000個例子,D
傳統嚟講，當我哋進行決策樹學習嘅時候，如果係真實值嘅輸入屬性，我哋通常會根據屬性係超過定係低於某個閾值嚟進行二分法分割。Pat建議，應該改用多路分割，每個分支對應屬性嘅每一個唔同值。喺以下列表中，揀一個最大嘅問題同Pat嘅建議有關：,咁樣太貴㗎，算嘢好貴㗎。,可能會導致一個決策樹在訓練集和測試集上表現不佳。,可能會做出一個決策樹，喺訓練集上表現好，但係係測試集上表現唔係咁好。,可能會導致一個決策樹在測試集上表現得很好，但在訓練集上表現得很差。,C
