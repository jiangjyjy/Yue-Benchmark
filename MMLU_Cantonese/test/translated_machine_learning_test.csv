Statement 1| Linear regression estimator has the smallest variance among all unbiased estimators. Statement 2| The coefficients α assigned to the classifiers assembled by AdaBoost are always non-negative.,"True, True","False, False","True, False","False, True",D
RoBERTa嘅pretrain資料集係大概有BERT嘅10倍咁大。ResNeXt 2018年嘅大部份模型通常係用tanh activation functions。,真係，真係。,唔係，唔係。,真，假,錯，啱。,C
聲明1| 支持向量機同邏輯回歸模型一樣，係俾一個機率分佈畀你，話你係個輸入例句嘅可能標籤。聲明2| 一般嚟講，我哋會期望支持向量係當我哋從一個線性核函數轉咗去高階多項式核函數嘅時候係保持唔變。,真係，真係。,錯，錯。,真係，假哋。,唔啱嘅，係嘅。,B
一個機器學習問題包括四個屬性加一個類別。每個屬性分別有3、2、2和2種可能的值。該類別有3個可能的值。最多有多少個不同的例子？,我哋呢個星期要交一份研究報告，我哋啱啱開始研究，而家要仲喺一個星期內完成，真係好緊張。希望我哋可以齊心協力，完成呢份報告，唔好辜負我哋嘅努力。感激哋嘅支持同配合，一齊加油啦！,嗰個網上嘅資訊同埋數據係唔準確嘅，你要記住喺搵資料嘅時候要睇清楚。如果你唔係好肯定，最好就問多啲人，睇多啲媒體同埋網站嘅資訊，慢慢比較多先做決定。,咸鱼翻身算乜？係咁啦，其實就係形容有人突然之間由低微嘅狀態發展到高尚嘅地位，有啲好似唔可能嘅事情成真。係咩，咁都有可能喎！無論係係事業、感情定其他方面，只要肯努力，都可以突破困境，有新既希望。,佢呀，佢就係一個勁勁好嘅球員，唔想同佢對陣喎，佢嘅技術同速度真係冇人。如果畀佢時間發揮，就真係冇人可以阻止佢。,D
到2020年，邊種建築係最適合分類高解像度圖片？,卷積神經網絡,圖論網絡,全連接網絡,徑向基函數網絡,A
句子1| 數據嘅對數似然值喺透過期望最大化演算法嘅連續迭代過程中一定會增加。句子2| Q-learning嘅一個缺點係只可以喺學習者有對佢嘅行動點怎樣影響環境嘅先驗知識嘅情況下使用。,"真係㗎, 真係。",唔係，唔係。,真係，唔係。,錯，冇錯。,B
假如話我哋已經計算出成本函數嘅梯度同埋存到一個向量 g度。畀出嚟，畀一次梯度下降嘅更新係咩嚟嘅？,唔好意思，呢度唔係翻譯口語粤語嘅專業範疇，建議搵本地人或者專業翻譯人員幫手。,喺大O記號度量法度裡面係指規模函數嘅一種，代表所描述嘅函數係上限最差時間複雜度。以唔同資料規模N嚟講就會有唔同嘅演算法運行時間，喺呢個情況之下我哋就會用O(N)黎表示。,咁可就係時間複雜度O(ND)。,喺計算機科學裏，O(ND^2) 代表嘅係一個時間複雜度嘅記號，代表嘅係一個演算法嘅執行時間會隨住 N 同 D 𩐝值而變化，而且隨住 D 嘅平方成正比增加。,A
聲明1| 對於一個連續隨機變量 x 和其概率分布函數 p(x)，對於所有 x，都滿足 0 ≤ p(x) ≤ 1。聲明2| 決策樹是通過最小化信息增益來學習的。,真係呀，真係。,唔啱，唔係呀。,"真, 假",假，真。,B
考慮下面嘅貝葉斯網絡。呢個貝葉斯網絡 H -> U <- P <- W 需要幾個獨立參數？,唔好意思呀，我唔識講粵語，無法幫你進行翻譯。希望你能夠理解。,請幫我將下面呢段嘢翻譯成正宗嘅口語粵語（對專業嘅詞彙同句子要專業嘅翻譯），格式唔變，所有一定要全，唔好落下，直接輸出結果，唔好有多餘嘅話同多餘嘅符號（注意唔好生成粵拼同英文，直接输出口語粵語嘅繁體字中文形式，繁體字）：4,唔好意思，我唔識講粤语。可以等我學習之後再幫你翻譯嗎？,16. 我哋一齊去食嘢啦，然後再睇戲，點睇？,C
當訓練數據的數量趨於無限大時，你用呢啲數據訓練嘅模型將會有：,低變異,波动大,同一個變異數,冇一個係正確㗎,A
聲明1｜喺二維平面度嘅所有矩形集合（包括唔係對軸對齊嘅矩形）可以擊碎一組5個點。聲明2｜當k=1嘅k-最近鄰分類器嘅VC-維度係無限。,真係呀，真係。,唔係，唔係。,"真, 唔係.",錯，啱。,A
_ 係指一個模型唔能夠準確地拟合訓練數據，同埋唔能夠應對新數據。,好fitting。,過度擬合係指模型對訓練數據學得嘅太好，反而喺新數據上表現唔好嘅情況。,欠擬合,全部以上,C
句1| F1分數特別適合處理有高度類別不平衡的數據集。句2| ROC曲線下的面積是評估異常檢測器的主要指標之一。,真係啱嘅，真係。,唔係，唔係。,真係呀，唔係呀。,錯，啱。,A
聲明1| 反向傳播演算法係學習一個有隱藏層嘅全局最優神經網絡。聲明2| 一條線嘅VC維度應該最多係2，因為我至少可以搵到一個3個數點嘅情況，冇任何一條線可以打碎佢哋。,真係嘅，真係。,唔係，唔係。,真啱，假啱,錯，啱。,B
高熵意呢係指分類入面嘅partition好多。,呢個嗰段嘢，我哋要喺一個齋廳聚餐熱熱鬧鬧咁慶祝一下。希望大家都可以盡興啦，好好享受一個歡樂嘅夜晚。希望我哋嘅友誼可以更加緊密，一齊歡笑，一齊分享快樂。希望今晚嘅聚會可以成為一個難忘嘅回憶，永遠留在我哋嘅心中。希望大家可以放鬆心情，盡情享受呢個難得嘅時刻。,冇純正,唔洗客氣，我地試下咁樣做吓，睇下會唔會有效果。多謝你嘅支持同埋信任，我地會盡力做到最好。希望我哋嘅努力終有成果，同時亦希望可以帶黎更多嘅機會同挑戰，一齊共同成長。有咩需要同出入嘅地方，唔好猶豫同我地講，我地會盡力解決問題。希望我哋之間嘅合作可以愈嚟愈好，為將來嘅合作奠下穩固嘅基礎。,冇用。,B
聲明1 | 喺原始 ResNet 研究論文度用咗 Layer 正則化，唔係 Batch 正則化。聲明2 | DCGANs 用自我關注嚟穩定訓練。,確定，確定。,唔係，唔係。,真係，唔係。,錯，啱。,B
喺建立一個特定數據集嘅線性回歸模型時，你觀察到一個特征嘅係數有一個相對較高嘅負值。呢個暗示着,呢个特點對模型有好大嘅影響（應該保留）。,呢个特徵對模型嘅影響唔算太大（應該忽略）。,唔可以冇啲額外嘅資料就評論呢個功能嘅重要性。,冇乜可以肯定。,C
對於一個神經網絡嚟講，係咪其中一個結構性假設最影響佢嘅欠擬合（即係高偏差模型）同過度擬合（即係高方差模型）之間嘅權衡？,隱藏節點嘅數目,學習速度,初次揀錯嘅重量,用一個固定單位輸入,A
對於多項式迴歸，以下結構性假設中那一個最影響過於擬合和過於擬合之間的折衷，係咪呢個:,呢個多項式次數,不論係以矩陣反轉定係梯度下降來學習權重，都係要留意。,高斯雜訊的假設變異數,使用恒量项单位输入,A
文告1 | 到2020年為止，有啲型號係CIFAR-10上達到大過98%嘅準確率。文告2 | 原始嘅ResNets並唔係用Adam optimizer來優化。,真係嘅，真係。,"錯, 唔係嘅",真，假,錯，啱。,A
K-means演算法：,要求特徵空間嘅維度唔可以比樣本數多。,當K等於1時，目標函數的值最小。,減少咗係一定數量嘅簇度裏面嘅內部差異。,只有係揀啱一啲樣本本身作為初始平均值嘅情況下啱啱到達全局最優解。,C
聲明1| VGGNet嘅卷積核係比AlexNet嘅第一層核仔細。聲明2| Batch Normalization之前已經引入咗依賴數據嘅權重初始化過程。,真係，真係。,唔係，唔係。,真係啦，唔係啦。,唔啱，係嘅。,A
"呢個矩陣嘅秩係幾多？A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]",唔该你幫我將底下呢段嘢翻譯成正宗嘅口語粵語（對專業嘅詞彙同句子要專業嘅翻譯），格式唔變，所有一定要全，唔好落哂，直接輸出結果，唔好有多餘嘅話同多餘嘅符號（留意唔好生成粵拼同英文，直接輸出口語粵語嘅繁體字中文形式，繁體字）：0,唔該幫我將底下呢段嘢翻譯成正宗嘅口語粵語（對專業嘅詞彙同句子要專業嘅翻譯），格式唔變，全部一定要，唔好漏，直接出嚟結果，唔好有多餘嘅話同多餘嘅符號：1,請幫我將呢段嘢翻譯做正宗嘅口語粵語，對專業嘅詞彙同句子要專業嘅翻譯，格式唔變，所有一定要全，唔好漏，直接輸出結果，唔好有多餘嘅嘢同符號。,請幫我將以下呢段嘢翻譯成正宗嘅口語粵語（對專業嘅詞彙同句子要專業嘅翻譯），格式唔變，全部一定要全，唔好漏哂，直接輸出結果，唔好有多餘嘅話同多餘嘅符號：3,B
聲明1| 喺度疏密預測（比方話用核密度估計器）可以用嚟做分類。聲明2| 喺羅吉斯迴歸同高斯朴素貝葉斯（同簡單協方差類）之間有啲對應，咁就代表呢兩個分類器嘅參數係一對一對應嘅。,真係呀，真係。,錯咗，唔係啱。,真係，唔係。,錯，啱。,C
如果我哋想要對空間數據（例如屋嘅幾何位置）進行分群，我哋希望產生各種不同大小同形狀嘅簇。以下哪種方法最適合？,決策樹,基於密度的聚類,基於模型嘅分類。,K-means clustering --> K均值分類,B
聲明1| 喺AdaBoost度，啲預錯嘅樣本嘅權重會以同一個乘法因子增加。聲明2| 喺AdaBoost度，啲權重訓練錯誤率e_t隨住t個弱分類器對有權重D_t嘅訓練數據而增加。,真係嘅，真係。,"假, 唔係。",對，唔係。,錯，啱。,A
MLE 估計通常唔係咁正卓，因為,佢哋有偏見。,佢哋嘅變異性好高。,佢哋唔係一啲一致嘅估計量。,冇一樣係以上述嘅。,B
Gradient descent 嘅計算複雜度係相當高嘅，主要係因為每一個步驟都需要計算梯度同執行更新參數嘅操作，如果數據集或者模型複雜度過高，就有可能導致算法嘅執行時間過長。,直線性係D,喺N度係線性既。,多項式係D中。,視乎咗迭代嘅次數。,C
平均多棵決策樹的輸出可以幫助_。,加大偏向,減少偏見,增加變異量,減少變異,D
進行線性迴歸分析嘅模型同喺識別特徵子集結束時嘅模型有可能唔同。,最佳子集选择,前向逐步選擇,前進階段選擇,全部以上,C
神經網絡：Neural networks:,優化一個凸目標函數。,只可以用隨機梯度下降訓練。,可以使用各種不同嘅激活函數。,冇一樣係以上嗰啲。,C
假如一种疾病D嘅发生率大约係100人中有5个案例（即P(D) = 0.05）。让布尔随机变量D代表病人“有病D”，而布尔随机变量TP代表“检测呈阳性”。已知检测病D嘅检测方法非常准确，即当你有病时检测呈阳性嘅概率係0.99，而当你冇病时检测唔做反应嘅概率係0.97。问题係P(TP)，即测试呈阳性嘅先验概率係几多。,零點零三六八,0.473 - 唔啱嘅囉!,零點零七捌,冇啲以上既。,C
"報告1| 經過將其映射到功能空間 Q 後，使用無加權歐幾里得距離的 1-NN 可能比在原始空間中實現更好的分類性能（儘管我們不能保證這一點）。
報告2| 感知器的 VC 維度小於簡單線性支持向量機的 VC 維度。",真係嘅，真係。,唔係、錯、唔係、錯,真嘅，假嘅。,唔係，係啱。,B
Grid search 嘅缺點係...,冇辦法用係唔可微嘅函數上。,冇得用係非連續函數上。,好難實施。,佢做多元線性迴歸分析嘅速度係比較慢既。,D
預測一個地區嘅雨量係一個好算唔準嘅問題。,監督式學習。,自学深度学习,叢聚化係一種機器學習嘅技術，佢可以將類似嘅數據分組埋一齊，係用嚟發現數據中嘅潛在模式同埋結構。透過分析數據間嘅相似度同差異性，Clustering 可以幫助我哋了解數據中嘅群組同趨勢。,冇哂哋,A
邊一句關於回歸分析係錯誤嘅？,佢同埋入對出。,用嚟預測嘅。,可以用嚟翻譯。,佢發現因果關係。,D
邊個係剪枝決策樹嘅主要原因？,為咗節省測試時嘅運算時間,攞吓位存 Decision Tree。,為咗令訓練集嘅錯誤更細,畀书唔好过度啦，避免太过训练集。,D
聲明1｜核密度估計器等同於在原始數據集中每個點Xi上執行核回歸，值Yi = 1/n。聲明2｜學習決策樹的深度可以大於用於創建樹的訓練示例數。,真係好呀，真係。,假，假。,真，假。,唔係，係嘅。,B
假設你嘅模型係過度擬合。以下哪一個方法唔係有效嘅方法去減少過度擬合？,增加訓練數據量。,改善而家用嚟最細化錯誤減少嘅優化演算法。,簡化模型複雜度。,減少培訓數據中嘅嘈音。,B
聲明1| 軟最大化函數係常用喺多類別羅吉斯迴歸裏面。聲明2| 非均勻軟最大化分佈嘅溫度會影響佢嘅熵值。,真係呀，真係。,"唔係, 唔係。","真, 假",唔啱，啱。,A
以下關於 SVM 嘅敘述邊個係正確？,對於二維數據點嚟講，線性SVM學習嘅分隔超平面會係一條直線。,理論上，高斯核支持向量機唔可以模擬任何複雜嘅分離超平面。,喺SVM度用嘅每一個kernel function，我哋都可以搵到一個等效嘅閉合基底展開式。,過度擬合唔係取決係支援向量嘅個數。,A
"請問係咪係比你知如果已知 Bayesian Network 係 H -> U <- P <- W 嘅情況底下，H, U, P, 同埋 W 嘅聯合機率係乜嚟？【注意：係條件機率嘅乘積】。","P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)","機率（H, U, P, W）= 機率（H）* 機率（W）* 機率（P | W）* 機率（W | H, P）","機率(H, U, P, W) = 機率(H) * 機率(W) * 機率(P | W) * 機率(U | H, P)",都唔係以上嘅。,C
由於RBF核函數的SVM之VC維數為無限，因此這種SVM必定比有有限VC維數的多項式核函數的SVM表現差。一個具有線性激活函數的兩層神經網絡本質上是在給定數據集上訓練的線性分隔器的加權組合；建立在線性分隔器上的提升算法也能找出線性分隔器的組合，因此這兩種算法將得出相同的結果。,真係嘅，真係。,啲嘢，唔啱，唔係啱。,真，假,錯，啱。,B
聲明1| ID3 演算法保證會搵到最佳嘅決策樹。聲明2| 考慮一個有密度f()嘅連續概率分佈，任何地方都唔係零。一個值x嘅機率等於f(x)。,真係嘅，真係。,唔係，唔係。,真係嘅，唔係嘅。,錯，啱。,B
有一個有N個輸入節點，無中間層，一個輸出節點，使用Entropy Loss同Sigmoid Activation Functions的神經網絡，以下哪個算法（配合合適的超參數同初始設定）可以用來尋找全局最優解？,隨機梯度下降,小批量梯度下降算法,批量梯度下降,以上所有嘢。,D
加多啲基函數喺一個線性模型入面，揀最有可能嘅選擇：,減低模型偏差,減少估計偏差。,減少變異,唔影響偏差同方差,A
考慮下面所給的貝葉斯網絡。如果我們對獨立性或條件獨立性 H -> U <- P <- W 沒有做任何假設，我們需要多少個獨立參數？,我想預約一個理髮服務，請問你哋得閒嗎？,唔該幫我將底面嘅句子翻譯成正宗嘅口語粵語（對專業嘅詞彙同句子要專業嘅翻譯），格式唔好改，全部一定要翻譯，唔好漏咗，直接輸出結果，唔好有多餘嘅話同符號（留意唔好生成粵拼同英文，直接輸出口語粵語嘅繁體字中文形式，繁體字）：4,唔該你幫我將下面呢段嘢翻譯成正宗嘅口語粤語（對專業嘅詞彙同句子要專業嘅翻譯），格式唔變，所有一定要全，唔好落哂，直接輸出結果，唔好有多餘嘅話同多餘嘅符號（注意唔好生成粤拼同英文，直接輸出口語粤語嘅繁體字中文形式，繁體字）：7,請幫我將呢段嘢翻譯成正宗嘅口語粵語（對專業嘅詞彙同句子要專業嘅翻譯），格式唔變，所有一定要全，唔好失左，直接輸出結果，唔好有多餘嘅話同多餘嘅符號（留心唔好生成粵拼同英文，直接輸出口語粵語嘅繁體字中文形式，繁體字）：15,D
係咪意味呢個偵測方法係偵測冇該有嘅分佈？,異常檢測,一級檢測,訓練測試不匹配韌性,背景檢測,A
第一個敘述 | 我哋透過 boost 強學生 h 去學習一個 classifier f。f 嘅決策邊界係同 h 嘅一樣，但係參數唔同。（例如，如果 h 係一個線性 classifier，咁 f 都係一個線性 classifier）。第二個敘述 | 交叉驗證可以用嚟揀選 boosting 中迭代嘅次數；呢個過程可能有助於減少過擬合。,真係，真係。,假嘅，假嘅。,啱㗎，唔啱。,錯，啱。,D
句 1| 高速公路網絡係喺 ResNets 之後推出嘅，佢唔鍾意用最大池化，而係偏愛卷積。句 2| DenseNets 通常比 ResNets 佔用更多記憶體。,真係呀，真係。,唔係，唔係。,真係，唔係,唔係，係嘅。,D
如果N係訓練資料集中的實例數，最近鄰演算法嘅分類運行時間係,唔洗睇數據，O(1)即刻就有結果。,啲N个O.,係咁先，O(log N )。,O( N^2 ) 嘅時間複雜度係指㨭嘅執行時間會隨住輸入數據嘅規模 N 㨝乘法關係而呈現一個平方嘅關係。,B
聲明1| 原始嘅 ResNets 同 Transformers 係屬於前饋神經網絡。聲明2| 原始嘅 Transformers 使用自我關注，但原始嘅 ResNet 唔用。,真係嘅，真係。,錯，錯。,真係，唔係,唔係，係嘅,A
聲明1| RELUs唔係喺單調，但sigmoids係單調。聲明2| 用梯度下降訓練嘅神經網路好大機會收斂到全局最優。,真係呀，真係。,唔係啦，唔係啦。,"真, 唔係",錯，真,D
神經網絡入面一個Sigmoid節點嘅數字輸出：,係無界嘅， 可以包括所有嘅實數。,無窮大，包羅所有整數。,係0同1之間。,值介乎於 -1 同 1 之間。,C
邊個以下嗰啲只可以係訓練數據係線性可分嘅時候使用？,直索Hard-margin SVM。,線性邏輯回歸。,線性軟邊界支持向量機。,中心點方法。,A
以下邊個係空間聚類演算法？,分區式分類。,K-means clustering 係一種資料分類同分群嘅技術，主要用喺資訊檢索、資料探勘同埋機器學習嘅領域。該方法主要係將資料分組，同時使得每個組內嘅資料點相似度盡可能高，而不同組之間嘅資料點相似度盡可能低。,格子分類算法,以上所有嘢。,D
聲明1|支援向量機所建構嘅最大邊緣決策界限，係所有線性分類器中擁有最低概括錯誤率。聲明2|從生成式模型獲得嘅任何決策界限，只要係由具備班操條件高斯分佈嘅模型得出，原則上都可以用SVM同多項式核函數（次數小於或等於三次）複製出嚟。,"真係呀, 真係。",唔係，唔係。,真係呀，唔係呀,錯，對。,D
聲明1| L2 regularization 對線性模型嘅影響通常比 L1 regularization 更加稀疏。聲明2| Residual connections 可以喺 ResNets 同埋 Transformers度搵到。,真係呀，真係。,錯，錯。,真係，唔係,唔啱，真係。,D
"假設我哋想要計算P(H|E, F)，但係冇任何條件獨立嘅信息。以下嘅哪幾組數字係足夠用嚟做計算呢？","P(E, F) = P(E|H) * P(F|H) * P(H), P(H), P(E|H), P(F|H)","機率(E, F)，機率(H)，機率(E, F|H)",P(H)、P(E|H)、P(F|H),機會擲出E同埋F嘅機會，條件係有H嘅情況下，E出現嘅機會同埋F出現嘅機會。,B
點解我哋做 bagging 時，下列邊個先可以預防 overfitting？,用上替換抽樣作為抽樣技術,弱分類器的使用,唔使擔心過度擬合嘅分類演算法,每個訓練好嘅分類器都要進行驗證嘅操做。,B
聲明1 | 主成分分析同譜聚類（例如吳恩達嘅方法）喺兩個唔同嘅矩陣上進行特徵分解。不過，呢兩個矩陣嘅尺寸係一樣嘅。聲明2 | 因為分類係迴歸嘅一個特例，邏輯回歸係線性回歸嘅一個特例。,真係，真係。,錯，錯。,真係，假的。,錯，啱,B
聲明 1| Stanford 情感樹庫有電影評論，冇書評。聲明 2| Penn 樹庫用作語言模型。,真係啱，真係啱。,唔係，唔係。,真、假,錯，啱。,A
"呢個矩陣嘅零空間嘅維度係幾多呢？ A = [[3, 2, −9], [−6, −4, 18], [12, 8, −36]]",唔該幫我將底下呢段嘢翻譯做正宗嘅口語粤語，條文要唔要落，直接輸出成果，唔好有多餘嘅嘢同符號：0,唔好意思，我唔識講廣東話，唔可以幫你翻譯。,呢段時間好忙，我真係冇時間搞其他嘢喇。希望快啲過去，可以放鬆下。,唔好意思，我遲咗返工。係呀，我知道唔夠尊重，唔好意思。唔好再嬲啦，我冇乜野意思。来，我请客！,C
支援向量係咩嚟㗎？,決策邊界最遠嘅例子。,計算 SVM 中 f(x) 所需的唯一例子。,數據中心。,所有係SVM度有non-zero weight αk嘅例子。,B
聲明1 | Word2Vec 參數係冇用 Restricted Boltzman Machine 嚟初始化。聲明2 | tanh 函數係一個非線性嘅激活函數。,真係嘅，真係嘅。,唔係，唔係。,真係，假的,"錯, 對",A
如果你嘅訓練損失喺訓練嘅回合數增加嗰陣都會升嗰話，可能係以下哪個問題係學習過程嘅潛在問題？,正規化嘅參數太低，模型過度配合。,正規化得太高，模型未能充分擬合。,步長太大啦。,步幅太細啦。,C
假設一種疾病D的發生率約為100人中5人有（即P(D) = 0.05）。讓布林隨機變量D表示一個病人「患疾病D」，讓布林隨機變量TP代表「測試呈陽性」。患疾病D的測試被證明非常準確，即當你患有該疾病時測試呈陽性的概率為0.99，當你沒有患病時測試呈陰性的概率為0.97。當測試呈陽性時，P(D | TP)是多少，即後驗概率是多少？,雖然我哋可以做啲好嘅工作，但係好遺憾，我哋嘅產品無法合適您嘅需求。希望日後有機會再合作。感謝支持。,唔該幫我將底下呢段嘢翻譯做正宗嘅口語粤語（對專業嘅詞彙同句子要專業嘅翻譯），格式唔變，全部一定要，唔好漏咗，直接輸出結果，唔好有多餘嘅話同符號（留意唔好生成粤拼同英文，直接輸出口語粤語嘅繁體字中文形式，繁體字）：0.078,"0.635
六百三十五厘。",0.九七,C
聲明1| 傳統機器學習結果假設訓練集和測試集是獨立且同分佈的。聲明2| 2017年，COCO模型通常是在ImageNet上預先訓練的。,真係嘅，真係。,假嘢，假嘢。,"真, 唔係","錯, 啱。",A
"句子1 | 兩個唔同核心函數 K1(x, x0) 同埋 K2(x, x0) 喺同一個訓練集上獲取嘅邊際值唔可以話俾我哋知邊個分類器係測試集上表現更好。句子2 | BERT嘅啓動函數係GELU。",真係，真係。,錯，錯,真，假,錯，啱。,A
邊一個係機器學習嘅分群演算法？,期望最大化,喂，唔該，我要一個PUSH CART，多謝。,高斯朴素贝叶斯,Apriori係一個常用嘅關聯規則學習算法，主要用嚟挖掘數據中頻繁出現嘅項目集。該算法係基於先驗知識定義一個最小支持度閾值，通過遞迴法尋找滿足最小支持度閾值嘅頻繁項目集。該算法係數據挖掘中常用嘅工具之一，係俾演算法工程師同數據科學家嘅關注。,A
你剛剛訓練完分類垃圾郵件的決策樹，但係成績好差，唔單止係訓練集，測試集都係。你知道自己嘅實作冇bug，咁係咪有其他原因導致問題呢？,你啲決策樹太淺。,你要加快啲學習進度。,你係過度擬合。,冇一個係啦。,A
K-fold 交叉驗證係一種模型評估嘅方法，將數據集分為K份，進行K次訓練同測試，每次訓練K-1份數據，測試1份數據，最後取平均值衡量模型嘅性能。,喺K度係線性嘅。,二次函數係K中。,cubic 唔係咩好嘅？係别嘅喎，呢度嘅呀。,指数级別喺K。,A
聲明1| 工業規模嘅神經網絡一般都係用CPU嚟訓練，唔係GPU。聲明2| ResNet-50模型有超過10億個參數。,真係，真係。,錯，錯。,真係，唔係。,唔係，係。,B
有兩個布爾隨機變量 A 和 B，其中 P(A) = 1/2，P(B) = 1/3，以及 P(A | ¬B) = 1/4，那麼 P(A | B) 是多少？,第一樣內容係一個係人為智能範疇中嘅一個議題。係指呢個範疇，系統或者機器被賦予某種類似人類智力嘅能力，咁樣就可以進行某啲通常需要人類智力嚟處理嘅任務。,今個星期五，我哋會聚埋一齊食飯，所以希望你都可以參加，食完飯之後，我哋會一齊去飲酒，慶祝我哋嘅友誼。希望你唔好缺席啦，我哋等緊你嚟。見到你就喺啱啦！,我係一個學生，平時就讀大學。每日學多啲野，為將來嘅工作做好準備！,唔該幫我將下面呢段嘢翻譯做啱啱嘅口語粵語（對專業嘅詞彙同句子要專業嘅翻譯），格式唔好變，所有一定要全，唔好漏咗，直接輸出結果，唔好有多餘嘅嘢同符號。,D
人工智能帶黎嘅存亡風險，最常同以下邊位教授有關？,急需要人哋幫手我翻譯一句嘢，字句唔好漏晒，全部都要準確。多謝！,唔好睇到呢個人名就反應佢同LeBron James一樣，佢其實係一位著名嘅人工智能科學家，係Facebook人工智能研究中心嘅主任。,史都華羅素,翻譯如下：佢嘅名係積天華,C
聲明1 | 最大化 logistic 迴歸模型的機率，可能產生多個局部最優解。聲明2 | 如果數據的分佈已知，則沒有分類器比一個 naive Bayes 分類器做得更好。,真係，真係。,錯，唔係。,係嘞，唔係。,錯，對。,B
對於核迴歸（Kernel Regression），以下哪一種結構性假設最影響了欠擬合和過擬合之間的權衡：,係邊個核函數好，高斯定三角形定盒形？,無論我哋用歐氏距離、L1距離定係L∞距離,核寬,核函數嘅最大高度,C
聲明1| SVM 學習演算法有保證係對佢嘅目標函數，搵到全局最佳嘅假設。聲明2| 經過射影咗入特徵空間Q嘅径向基核函數，感知機或許可以比原來空間有更好嘅分類表現（雖然我哋唔能夠保證）。,係呀，係呀。,唔係，冇錯。,啱，唔啱。,錯，啱。,A
對於高斯貝氏分類器黎講，邊個結構假設係最影響返擺下fit同過fit之間嘅權衡？,無論我地用最大概似定梯度下降黎學習class centers，都係得架。,不論我哋假設滿標準協方差矩陣定係對角線協方差矩陣,不論係我哋有一樣嘅班級先驗定係由數據估算而得嘅先驗。,緊係我哋准俾班有唔同嘅平均向量 定係迫佢哋共用同一個平均向量,B
過擬合容易發生喺訓練數據集係小嘅時候。過擬合容易發生喺假設空間細嘅時候。,真係呀，真係。,錯、唔係。,"真, 唔係",錯，啱。,D
除咗EM之外，梯度下降可以用嚟喺高斯混合模型上進行推論或學習。假設固定嘅特徵數量，基於高斯嘅貝葉斯最優分類器可以喺數據集中嘅記錄數目線性時間內學習。,真係呀，真係。,唔係，唔係。,係，唔係。,錯，啱。,A
喺Bayesian network度，junction tree algorithm嘅推論結果同variable elimination嘅推論結果係一樣嘅。如果有兩個隨機變量X同Y，係喺另一個隨機變量Z嘅條件下獨立嘅嘅話，咁喺對應嘅Bayesian network度，X同Y嘅節點係喺Z嘅條件下係d-separated嘅。,真係嘅，真係。,唔係，唔係。,係嘅，唔係。,錯，啱。,C
畀一個大量嘅心臟病患者嘅醫療記錄數據，睇下有冇可能係其中發現唔同嘅病人群體，我哋要唔要度身訂造唔同嘅治療方法。呢個係咩學習問題呢？,監督式學習,無導師學習,兩個都係。,冇(a) 亦冇(b)。,B
喺PCA度點樣樣先可以同SVD一樣嘅投影？,將數據轉換為零均值,將數據轉換為零中位數,唔得啦。,冇一個,A
話1| 1最近鄰分類器嘅訓練錯誤係0。話2| 當數據點數目無限增長嘅時候，MAP估計逐漸接近所有可能先驗下嘅MLE估計。換句話講，有足夠數據嘅情況下，先驗嘅選擇係無關緊要嘅。,真係，真係。,唔係，唔係。,啱，唔啱。,錯，啱。,C
喺進行最小二乘回歸同埋正規化嗰陣時（假設最佳化可以準確進行），將正規化參數λ嘅值增加會增加測試誤差。,永遠唔會減少訓練誤差。,永遠唔會增加訓練錯誤。,永遠唔會減少測試錯誤。,永遠唔會增加,A
點解以下哪一個描述最能夠概括歧視性方法嘅模型目標？（模型中嘅參數係咩）,梗係等於 y 係比 x 同埋 w 噉條件下嘅機率啦。,P呢，係y對x嘅條件概率。,"p(w|x, w) 嘅意思係係已知情況 x 同埋 w 嘅情況下，出現喺 w 嘅機率。",冇一樣係以上述提嘢。,A
聲明1| 用卷積神經網絡進行 CIFAR-10 圖像分類的表現可以超過 95%。聲明2| 神經網絡的集成不會提高分類準確度，因為它們學習的表示高度相關。,真係嘅，真係。,唔啱，唔係啱,啱，錯。,"錯, 啱。",C
呢度Bayes論和頻率論會有唔同嘅觀點？,用唔係高斯噪音模型嘅機率回歸。,用機率模型做返歸。,喺一個機率模型度用唔同參數嘅先驗分佈。,喺高斯判别分析中使用班别先验。,C
BLEU量度用precision，而ROGUE量度用recall。Hidden markov模型經常用嚟模擬英文句子。,真係呀，真係。,唔係，唔係。,真，假,錯，對。,A
"聲明1 | ImageNet有好多唔同解像度嘅圖像。
聲明2 | Caltech-101嘅圖像比ImageNet多。",真係呀，真係。,唔係，唔係。,真的，假的,錯，啱。,C
邊個方法係更啱進行特徵選擇？,"翻到到岭,我地就快到目的地喇。越岭行到尽头,我们就可以睇到全景喇。",拉索,兩樣都係（a）同埋（b）。,冇乜 (a) 都冇乜 (b),B
假設你有一個EM算法，可以為具有潛在變量的模型找到最大可能性估計值。你被要求修改該算法，使其找到MAP估計值。你需要修改哪一個步驟或步驟？,期望係一種強大嘅力量，可以幫助我哋達到自己嘅目標同夢想。當我哋設定一個明確嘅目標，我哋就可以更加專注同堅定嘅追求佢，唔怕遇到困難同挑戰。喺追逐夢想嘅過程中，要保持對自己嘅期望同信心，相信自己一定可以做到，唔好輕易放棄。每一次克服困難同挑戰，都會令自己變得更加強大同成熟，同時令夢想更加接近現實。睇開自己嘅期望，踏實跟隨自己嘅內心，向著夢想不斷前進，必定可以實現心中嘅期望。,最大化,唔使修改。,兩個都係我嘅至愛，我唔可以揀擇。,B
喺一個高斯貝葉斯分類器度，係咪呢啲結構性假設當中，邊一個影響最大係欠擬合同過擬合之間嘅折衷：,不論我哋係咪透過最大概似法定梯度下降法嚟學習班中心,不論我哋假設全部班共變矩陣定係對角班共變矩陣,不論係我哋有一樣嘅班級先驗分佈定係由數據估計嘅先驗分佈。,不論係唔係容許各個類別有唔同嘅平均向量，定係迫使佢哋共用同一個平均向量。,B
"聲明1｜對於任何兩個變數x同y喺有聯合分佈p(x, y)嘅情況下，我哋總係擁有熵函數H[x, y] ≥ H[x] + H[y]。聲明2｜對於啲有啲有向圖，道德化會減少圖中存在嘅邊嘅數目。",真係呀，真係。,假嘢，假嘢。,係，唔係。,錯，啱。,B
邊個係唔係監督學習？,基本上係使用特徵值同特徵向量嚟將一個啲唔相關嘅數據轉換成一個相關嘅數據。PCA可以減少數據集嘅維度同埋保留大部分原本數據嘅重要信息。,決策樹係一種監督式機器學習演算法，主要用嚟解決分類同埋返開題。喺決策樹模型入面，數據會根據屬性值同埋條件去進行分類，最終產生一個可以做決策嘅樹狀結構。訓練決策樹模型嘅目的係為咗將數據輸出分類得更加準確同埋有效。,線性回歸,天真貝葉斯,A
聲明1| 神經網絡嘅收斂取決於嗰個learning rate。聲明2| Dropout會將隨機選擇嘅激活值乘以零。,係呀，係呀。,唔係呀，唔係呀。,真係呀，唔係呀,錯，啱。,A
"點我地嘅P(A, B, C)等於邊個？假設有Boolean隨機變量A、B同埋C，冇任何相互獨立或有條件獨立嘅假設？",條件機率係指 P(A | B) * P(B | C) * P(C | A)。,"P(C | A, B) * P(A) * P(B) --> P(C | A, B) * P(A) * P(B)","P(A, B | C) * P(C) -> P(A, B | C) * P(C)","P( A | B, C) * P(B | A, C) * P(C | A, B) -> 條件機率 (A | B, C) * 條件機率 (B | A, C) * 條件機率 (C | A, B)",C
邊項任務係最啱用Cluster分類解決。,預測雨量基於唔同嘅提示。,偵測詐騙信用卡交易,訓練隻機械人解謎,全部都係咁啦。,B
喺線性迴歸度入一個正則化懲罰後，你發現有啲w嘅係數被歸零咗。以下邊種懲罰可能係用咗？,L0 範數,L1範數，係表示一個向量內各個元素絕對值嘅總和。,L2 標準差,不是（a）就係（b）,D
"A同B係兩個事件。如果P(A, B)下降而P(A)上升，下列邊個講法係啱嘅？",條件B發生嘅情況下，A發生嘅機率就會減少。,條件機率為P(B|A)減少。,機率 B 減少。,全部以上,B
話說練習一個固定嘅觀測集合嘅時候，假設我哋唔知道真正嘅隱藏狀態嘅數量（呢個情況好常見），我哋可以透過增加隱藏狀態嘅數量嚟提高訓練數據嘅可能性。合作篩選常用嚟模擬用戶對電影嘅喜好。,真係嘅，真係。,唔係，唔係。,真係，唔係。,唔啱，係嘅。,A
你訓練一個線性迴歸模型嚟做一個簡單嘅估計任務，發現到模型過度擬合咗數據。你決定加入$\ell_2$正則化去懲罰啲權重。當你增加$\ell_2$正則化係數嘅時候，模型嘅偏差同變異會發生乜嘢變化？,偏差增加；方差增加,偏差增加；方差减少,偏差減少；變異增加,偏差減少; 方差減少,B
"邊個 PyTorch 1.8 command(s) 產生 $10\times 5$ 個高斯矩陣，每個入都係從 $\mathcal{N}(\mu=5,\sigma^2=16)$ 獨立同分佈樣本，同埋一個 $10\times 10$ 個均勻矩陣，每個入都係從 $U[-1,1)$ 獨立同分佈樣本？","5 加 torch.randn(10,5) 乘 16 ; torch.rand(10,10,low=-1,high=1)","5 加 torch.randn(10,5) 乘以 16 ; (torch.rand(10,10) 減 0.5) 除以 0.5","5 加 torch.randn(10,5) 乘 4 ；2 乘 torch.rand(10,10) 減 1","torch 正慘正常（torch.normal）（torch.ones(10,5)*5，torch.ones(5,5)*16）；2 * torch 條條（torch.rand(10,10)） - 1",C
文1 | ReLU 嘅梯度係 $x<0$ 嗰陣度係零，而 sigmoid 嘅梯度 $\sigma(x)(1-\sigma(x))\le \frac{1}{4}$，喺所有 $x$ 既情況下。文2 | Sigmoid 有一個連續嘅梯度，而 ReLU 有一個唔連續嘅梯度。,真係架，真係。,唔係，唔係。,真係呀，唔係呀,"錯, 對。",A
Batch Normalization嘅求就係為咗解決深度學習入嚟嘅梯度消失同梯度爆炸問題。,做完批量標準化之後，呢層嘅激活值就會跟住標準高斯分佈。,affine層嘅偏差參數，如果之後緊接緊一個batch normalization層嘅話，就會變得多餘。,用 Batch Normalization 時，標準的權重初始化方法必須更改。,批量规范化同卷積神經網路嘅層規範化一樣。,B
假設我哋有以下嘅目標函數: $\argmin_{w} \frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\gamma \norm{w}^2_2$ 問嘅係 $\frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\lambda \norm{w}^2_2$對$w$嘅梯度係點？,梯度返轉 w 的 f(w) =（X的轉置X + λI）w - X的轉置y + λw,梯度搖 f(w) = X轉置 X w - X轉置 y + λ,梯度係數 f(w) = X嘅轉置乘以X同埋w，再減去X嘅轉置乘以y，再加上λ同埋w。,梯度 $f(w)$ 等於 $X$ 轉置乘以 $X$ 乘以 $w$ 再減去 $X$ 轉置乘以 $y$ 加上（λ加1）乘以 $w$。,C
邊一個係真係一個卷積核？,用$\begin{bmatrix}1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$嚟convolve个圖唔會改變個圖。,將一張圖像同 $\begin{bmatrix}0 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ 做 convolution，係唔會改變圖像。,用$\begin{bmatrix}1 & 1 & 1\\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}$呢个矩阵去卷积图像，唔会改变图像。,用$\begin{bmatrix}0 & 0 & 0\\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$嘅圖片卷積唔會改變呢張圖片。,B
邊一個係假嘅？,語義分割模型預測每個像素點的類別，而多類圖像分類器預測整張圖像的類別。,一個 IoU (intersection over union) 等於96% 嘅邊界框大概會被認為係真正嘅正例。,當一個預測的邊界框同場景中的任何物體都不對應時，就被視為偽陽性。,一個交集聯集比（IoU）為3%嘅包圍框可能會被認為係假反例。,D
邊一個係唔真係？,以下冇啟用函數嘅全連接網絡係線性嘅：$g_3(g_2(g_1(x)))$，其中$g_i(x) = W_i x$，而$W_i$係矩陣。,"漏電 ReLU $\max\{0.01x,x\}$ 係凸。",一啲ReLUs嘅組合，例如$ReLU(x) - ReLU(x-1)$係凸嘅。,失去 $\log \sigma(x)= -\log(1+e^{-x})$ 係凹的,C
我哋係訓練一個有兩個隱藏層嘅全連接網絡，用嚟預測樓價。輸入係$100$維，有幾個特徵，例如呢個方呎數、家庭收入中位數等等。第一個隱藏層有$1000$個啓動點。第二個隱藏層有$10$個啓動點。輸出係一個樓價嘅標量。假設一個原味網絡，有仿射變換，冇任何批量標準化同埋啓動函數冇可學參數，呢個網絡有幾個參數？,唔該幫我將底下呢段嘢翻譯做正宗嘅口語粵語（對專業嘅詞彙同句子要專業嘅翻譯），格式唔變，全部一定要，唔好漏咗，直接輸出結果，唔好有多餘嘅話同多餘嘅符號。,一一零零一〇,唔知你識唔識講廣東話呀？如果唔識既話，要唔要我教你啊？係嘛，唔會？冇問題，慢慢來嘛，有咩問題可以隨時問我。希望我可以幫到你啦。,唔該幫我搵返呢段嘢嘅粵語口語翻譯，唔好漏咗任何一句，必須全文翻譯，唔想聽多餘嘅話同符號，直接俾我成果，唔好有多餘嘅字句。多謝。,A
第一句| sigmoid 函數嘅導數係 $\sigma(x)=(1+e^{-x})^{-1}$ 喺 $x$ 嘅倒數等於 $\text{Var}(B)$，其中 $B\sim \text{Bern}(\sigma(x))$ 係一個伯努利隨機變量。第二句| 將神經網絡每層嘅偏差參數設置為0會改變偏差-變異權衡，使得模型嘅變異增加同埋模型嘅偏差減少。,真係呀，真係。,唔係，唔係。,"真係, 唔係.",錯，對。,C
