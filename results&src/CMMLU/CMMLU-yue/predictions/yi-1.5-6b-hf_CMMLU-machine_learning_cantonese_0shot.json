[
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "喺二分類問題中，當測試集嘅正例同負例數量唔均衡時，以下評價方案邊個係相對唔合理嘅（假設precision=TP/(TP+FP)，recall=TP/(TP+FN)。）",
    "llm_output": "A. F-值：2recall*precision/(recall+precision)\nB. G-mean：sqrt(precision*recall)\nC. 準確性：(TP+TN)/all\nD. AUC：ROC曲線下面積\n答案係: \nA. F-值：2recall*precision/(recall+precision)\nB. G-mean：sqrt(precision*recall)\nC. 準確性：(TP+TN)/all\nD. AUC：ROC曲線下面積\n答案係: \nA. F-值：2recall*precision/(recall+precision)\nB. G-mean：sqrt(precision*recall)\nC. 準確性：(TP+TN)/all\nD. AUC：ROC曲線下面積\n答案係: \nA. F-值：2recall*precision/(recall+precision)\nB. G-mean：sqrt(precision*recall)\nC. 準確性：(TP+TN)/all\nD. AUC：ROC曲線下面積\n答案係: \nA. F-值：2recall*precision/(recall+precision)\nB. G-mean：sqrt(precision*recall)\nC. 準確性：(TP+TN)/all\nD. AUC：ROC曲線下面積\n答案係: \nA. F-值：2recall*precision/(recall+precision)\nB. G-mean：sqrt(precision*recall)\nC. 準確性：(TP+TN)/all\nD. AUC：ROC曲線下面積\n答案係: \nA. F-值：2recall*precision/(recall+precision)\nB. G-mean：sqrt(precision*recall)\nC. 準確性：(TP+TN)/all\nD. AUC：ROC曲線下面積\n答案係: \nA. F-值：2recall*precision/(recall+precision)\nB. G-mean：sqrt(precision*recall)\nC. 準確性：(TP+TN)/all\nD. AUC：ROC曲線下面積\n答案係: \nA. F-值：2recall*precision/(recall+precision)\nB. G-mean：sqrt(precision*recall)\nC. 準確性：(TP+TN)/all\nD. AUC：ROC曲線下面積\n答案係: \nA. F-值：2recall*precision/(recall+precision)\nB. G-mean：sqrt(precision*recall)\nC. 準確性：(TP+TN)/all\nD. AUC：ROC曲線下面積\n答案係: \nA. F-值：2recall*precision/(recall+precision)\nB. G-mean：sqrt(precision*recall)\nC. 準確性：(TP+TN)/all\nD. AUC：ROC曲線下面積\n答案係: \nA. F-值：2recall*precision/(recall+precision)\nB. G-mean：sqrt(precision*recall)\nC. 準確性：(TP+TN)/all\nD. AUC：ROC曲線下面積\n答案係: \nA. F-值：2recall*precision/(recall+precision)\nB. G-mean：sqrt(precision*recall)\nC. 準確性：(TP+TN)/all\nD. AUC：ROC曲線下面積\n答案係: \nA. F-值：2recall*precision/(recall+precision)\nB. G-mean：sqrt(precision*recall)\nC. 準確性：(TP+TN)/all\nD. AUC：ROC曲線下面積\n答案係: \nA. F-值：2recall*precision/(recall+precision)\nB. G-mean：sqrt(precision*recall)\nC. 準確性：(TP+TN)/all\nD. AUC：ROC曲線下面積\n答案係: \nA. F-值：2recall*precision/(recall+precision)\nB. G-mean：sqrt(precision*recall)\nC. 準確性：(TP+TN)/all\nD. AUC：ROC曲線下面積\n答案係: \nA. F-值：2recall*precision/(recall+precision",
    "correct_answer": "C",
    "standard_answer_range": "[['A', 'F-值：2recall*precision/(recall+precision)'], ['B', 'G-mean：sqrt(precision*recall)'], ['C', '準確性：(TP+TN)/all'], ['D', 'AUC：ROC曲線下面積']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "深度學習中遇見過擬合下列邊個處理辦法唔可取",
    "llm_output": "深度學習中遇見過擬合，可以採用下列處理辦法：\n加dropout層：可以防止過擬合，並且可以提高模型的泛化能力。\n加深層數：可以增加模型的複雜度，提高模型的泛化能力。\n數據增強：可以增加數據的樣本數量，提高模型的泛化能力。\n加正則項：可以防止模型過擬合，提高模型的泛化能力。\n因此，加dropout層和加深層數是深度學習中遇見過擬合時可以採用的處理辦法。",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '加dropout層'], ['B', '加深層數'], ['C', '數據增強'], ['D', '加正則項']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "假設我哋有一個數據集，喺一個深度為 6 嘅決策樹嘅幫助下，佢可以使用 100% 嘅精確度被訓練。依家考慮一下兩點，並基於呢兩點揀正確嘅選項。1.深度為 4 時將有高偏差同低方差；2.深度為 4 時將有低偏差同低方差。注意：所有其他超參數係相同嘅，所有其他因子唔受影響。",
    "llm_output": "問題：假設我哋有一個數據集，喺一個深度為 6 嘅決策樹嘅幫助下，佢可以使用 100% 嘅精確度被訓練。依家考慮一下兩點，並基於呢兩點揀正確嘅選項。1.深度為 4 時將有高偏差同低方差；2.深度為 4 時將有低偏差同低方差。注意：所有其他超參數係相同嘅，所有其他因子唔受影響。\nA. 1 同 2\nB. 淨係 2\nC. 冇一個\nD. 淨係 1\n答案係: \n問題：假設我哋有一個數據集，喺一個深度為 6 嘅決策樹嘅幫助下，佢可以使用 100% 嘅精確度被訓練。依家考慮一下兩點，並基於呢兩點揀正確嘅選項。1.深度為 4 時將有高偏差同低方差；2.深度為 4 時將有低偏差同低方差。注意：所有其他超參數係相同嘅，所有其他因子唔受影響。\nA. 1 同 2\nB. 淨係 2\nC. 冇一個\nD. 淨係 1\n答案係: \n問題：假設我哋有一個數據集，喺一個深度為 6 嘅決策樹嘅幫助下，佢可以使用 100% 嘅精確度被訓練。依家考慮一下兩點，並基於呢兩點揀正確嘅選項。1.深度為 4 時將有高偏差同低方差；2.深度為 4 時將有低偏差同低方差。注意：所有其他超參數係相同嘅，所有其他因子唔受影響。\nA. 1 同 2\nB. 淨係 2\nC. 冇一個\nD. 淨係 1\n答案係: \n問題：假設我哋有一個數據集，喺一個深度為 6 嘅決策樹嘅幫助下，佢可以使用 100% 嘅精確度被訓練。依家考慮一下兩點，並基於呢兩點揀正確嘅選項。1.深度為 4 時將有高偏差同低方差；2.深度為 4 時將有低偏差同低方差。注意：所有其他超參數係相同嘅，所有其他因子唔受影響。\nA. 1 同 2\nB. 淨係 2\nC. 冇一個\nD. 淨係 1\n答案係: \n問題：假設我哋有一個數據集，喺一個深度為 6 嘅決策樹嘅幫助下，佢可以使用 100% 嘅精確度被訓練。依家考慮一下兩點，並基於呢兩點揀正確嘅選項。1.深度為 4 時將有高偏差同低方差；2.深度為 4 時將有低偏差同低方差。注意：所有其他超參數係相同嘅，所有其他因子唔受影響。\nA. 1 同 2\nB. 淨係 2\nC. 冇一個\nD. 淨係 1\n答案係: \n問題：假設我哋有一個數據集，喺一個深度為 6 嘅決策樹嘅幫助下，佢可以使用 100% 嘅精確度被訓練。依家考慮一下兩點，並基於呢兩點揀正確嘅選項。1.深度為 4 時將有高偏差同低方差；2.深度為 4 時將有低偏差同低方差。注意：所有其他超參數係相同嘅，所有其他因子唔受影響。\nA. 1 同 2\nB. 淨係 2\nC. 冇一個\nD. 淨係",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '1 同 2'], ['B', '淨係 2'], ['C', '冇一個'], ['D', '淨係 1']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列邊啲方法唔可以用嚟對高維數據進行降維",
    "llm_output": "LASSO 唔可以用嚟對高維數據進行降維，因為 LASSO 係用嚟對數據進行稀疏化，即係將唔重要嘅參數係係模型中唔出現，但係並唔係用嚟對數據進行降維。\nBagging 唔可以用嚟對高維數據進行降維，因為 Bagging 係用嚟提高模型嘅泛化能力，而並唔係用嚟對數據進行降維。\n主成分分析法可以用嚟對高維數據進行降維，因為主成分分析法係用嚟將數據嘅維度從高維度降到低維度，而並唔係用嚟對數據進行稀疏化。\n聚類分析可以用嚟對高維數據進行降維，因為聚類分析係用嚟將數據分成幾個類別，而並唔係用嚟對數據進行稀疏化。",
    "correct_answer": "B",
    "standard_answer_range": "[['A', 'LASSO'], ['B', 'Bagging'], ['C', '主成分分析法'], ['D', '聚類分析']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "機器學習中L1正則化同L2正則化嘅區別係",
    "llm_output": "A. 使用L1可以得到稀疏、平滑嘅權值\nB. 使用L2可以得到稀疏、平滑嘅權值\nC. 使用L1可以得到稀疏嘅權值、使用L2可以得到平滑嘅權值\nD. 使用L2可以得到稀疏嘅權值、使用L1可以得到平滑嘅權值\n答案係: A. 使用L1可以得到稀疏、平滑嘅權值",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '使用L1可以得到稀疏、平滑嘅權值'], ['B', '使用L2可以得到稀疏、平滑嘅權值'], ['C', '使用L1可以得到稀疏嘅權值、使用L2可以得到平滑嘅權值'], ['D', '使用L2可以得到稀疏嘅權值、使用L1可以得到平滑嘅權值']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列關於隱馬模型同條件隨機場模型嘅講法中錯誤嘅係?",
    "llm_output": "隱馬模型同隱馬模型隱可用於命名實體識別、分詞同詞性標注嘅任務\n隱馬模型同隱馬模型都係生成模型\n隱馬模型係概率無向圖模型\n特徵嘅揀同優化會嚴重影響隱馬模型嘅結果\n隱馬模型同隱馬模型隱可用於命名實體識別、分詞同詞性標注嘅任務\n隱馬模型同隱馬模型都係生成模型\n隱馬模型係概率無向圖模型\n特徵嘅揀同優化會嚴重影響隱馬模型嘅結果\n隱馬模型同隱馬模型隱可用於命名實體識別、分詞同詞性標注嘅任務\n隱馬模型同隱馬模型都係生成模型\n隱馬模型係概率無向圖模型\n特徵嘅揀同優化會嚴重影響隱馬模型嘅結果\n隱馬模型同隱馬模型隱可用於命名實體識別、分詞同詞性標注嘅任務\n隱馬模型同隱馬模型都係生成模型\n隱馬模型係概率無向圖模型\n特徵嘅揀同優化會嚴重影響隱馬模型嘅結果\n隱馬模型同隱馬模型隱可用於命名實體識別、分詞同詞性標注嘅任務\n隱馬模型同隱馬模型都係生成模型\n隱馬模型係概率無向圖模型\n特徵嘅揀同優化會嚴重影響隱馬模型嘅結果\n隱馬模型同隱馬模型隱可用於命名實體識別、分詞同詞性標注嘅任務\n隱馬模型同隱馬模型都係生成模型\n隱馬模型係概率無向圖模型\n特徵嘅揀同優化會嚴重影響隱馬模型嘅結果\n隱馬模型同隱馬模型隱可用於命名實體識別、分詞同詞性標注嘅任務\n隱馬模型同隱馬模型都係生成模型\n隱馬模型係概率無向圖模型\n特徵嘅揀同優化會嚴重影響隱馬模型嘅結果\n隱馬模型同隱馬模型隱可用於命名實體識別、分詞同詞性標注嘅任務\n隱馬模型同隱馬模型都係生成模型\n隱馬模型係概率無向圖模型\n特徵嘅揀同優化會嚴重影響隱馬模型嘅結果\n隱馬模型同隱馬模型隱可用於命名實體識別、分詞同詞性標注嘅任務\n隱馬模型同隱馬模型都係生成模型\n隱馬模型係概率無向圖模型\n特徵嘅揀同優化會嚴重影響隱馬模型嘅結果\n隱馬模型同隱馬模型隱可用於命名實體識別、分詞同詞性標注嘅任務\n隱馬模型同隱馬模型都係生成模型\n隱馬模型係概率無向圖模型\n特徵嘅揀同優化會嚴重影響隱馬模型嘅結果\n隱馬模型同隱馬模型隱可用於命名實體識別、分詞同詞性標注嘅任務\n隱馬模型同隱馬模型都係生成模型\n隱馬模型係概率無向圖模型\n特徵嘅揀同優化會嚴重影響隱馬模型嘅結果\n隱馬模型同隱馬模型隱可用於命名實體識別、分詞同詞性標注嘅任務\n隱馬模型同隱馬模型都係生成模型\n隱馬模型係概率無向圖模型\n特徵嘅揀同優化會嚴重影響隱馬模型嘅結果\n隱馬模型同隱馬模型隱可用於命名實體識別、分詞同詞性標注嘅任務\n隱馬模型同隱馬模型都係生成模型\n隱馬模型係概率無向圖模型\n特徵嘅揀同優化會嚴重影響隱馬模型嘅結果\n隱馬模型同隱馬模型隱可用於命名實體識別、分詞同詞性標注嘅任務\n隱馬模型同隱馬模型都係生成模型\n隱馬模型係概率無向圖模型\n特徵嘅揀同優化會嚴重影響隱馬模型嘅結果\n隱馬模型同隱馬模型隱可用於命名實體識別、",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '隱馬模型同隱馬模型隱可用於命名實體識別、分詞同詞性標注嘅任務'], ['B', '隱馬模型同隱馬模型都係生成模型'], ['C', '隱馬模型唔係概率無向圖模型'], ['D', '特徵嘅揀同優化會嚴重影響隱馬模型嘅結果']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下面邊個/啲超參數嘅增加可能會造成隨機森林數據過擬合",
    "llm_output": "問題：下面邊個/啲超參數嘅增加可能會造成隨機森林數據欠擬合\nA. 樹嘅數量\nB. 樹嘅深度\nC. 以上都唔係\n答案係: \n問題：下面邊個/啲超參數嘅增加可能會造成隨機森林數據欠擬合\nA. 學習速率\nB. 樹嘅數量\nC. 樹嘅深度\nD. 以上都唔係\n答案係: \n問題：下面邊個/啲超參數嘅增加可能會造成隨機森林數據欠擬合\nA. 學習速率\nB. 樹嘅數量\nC. 樹嘅深度\nD. 以上都唔係\n答案係: \n問題：下面邊個/啲超參數嘅增加可能會造成隨機森林數據欠擬合\nA. 學習速率\nB. 樹嘅數量\nC. 樹嘅深度\nD. 以上都唔係\n答案係: \n問題：下面邊個/啲超參數嘅增加可能會造成隨機森林數據欠擬合\nA. 學習速率\nB. 樹嘅數量\nC. 樹嘅深度\nD. 以上都唔係\n答案係: \n問題：下面邊個/啲超參數嘅增加可能會造成隨機森林數據欠擬合\nA. 學習速率\nB. 樹嘅數量\nC. 樹嘅深度\nD. 以上都唔係\n答案係: \n問題：下面邊個/啲超參數嘅增加可能會造成隨機森林數據欠擬合\nA. 學習速率\nB. 樹嘅數量\nC. 樹嘅深度\nD. 以上都唔係\n答案係: \n問題：下面邊個/啲超參數嘅增加可能會造成隨機森林數據欠擬合\nA. 學習速率\nB. 樹嘅數量\nC. 樹嘅深度\nD. 以上都唔係\n答案係: \n問題：下面邊個/啲超參數嘅增加可能會造成隨機森林數據欠擬合\nA. 學習速率\nB. 樹嘅數量\nC. 樹嘅深度\nD. 以上都唔係\n答案係: \n問題：下面邊個/啲超參數嘅增加可能會造成隨機森林數據欠擬合\nA. 學習速率\nB. 樹嘅數量\nC. 樹嘅深度\nD. 以上都唔係\n答案係: \n問題：下面邊個/啲超參數嘅增加可能會造成隨機森林數據欠擬合\nA. 學習速率\nB. 樹嘅數量\nC. 樹嘅深度\nD. 以上都唔係\n答案係: \n問題：下面邊個/啲超參數嘅增加可能會造成隨機森林數據欠擬合\nA. 學習速率\nB. 樹嘅數量\nC. 樹嘅深度\nD. 以上都唔係\n答案係: \n問題：下面邊個/啲超參數嘅增加可能會造成隨機森林數據欠擬合\nA. 學習速率\nB. 樹嘅數量\nC. 樹嘅深度\nD. 以上都唔係\n答案係: \n問題：下面邊個/啲超參數嘅增加可能會造成隨機森林數據欠擬合\nA. 學習速率\nB. 樹嘅數量\nC. 樹嘅深度\nD. 以上都唔係\n答案係: \n問題：下面邊個/啲超參數嘅增加可能會造成隨機森林數據欠擬合\nA. 學習速率\nB. 樹嘅數量\nC. 樹嘅深度\nD. 以上都唔係\n答案係: \n問題：下面邊個/啲超參數嘅增加可能會造成隨機森林數據欠擬",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '學習速率'], ['B', '樹嘅數量'], ['C', '樹嘅深度'], ['D', '以上都唔係']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下面關於迭代二叉樹3代算法中講法錯誤嘅係",
    "llm_output": "問題：下面關於迭代二叉樹3代算法中講法錯誤嘅係\nA. 迭代二叉樹3代算法係一個二叉樹模型\nB. 信息增益可以用熵，而唔係GINI系數嚟計算\nC. 迭代二叉樹3代算法要求特徵必須離散化\nD. 揀信息增益最大嘅特徵，作為樹嘅根節點\n答案係: \n問題：下面關於迭代二叉樹3代算法中講法錯誤嘅係\nA. 迭代二叉樹3代算法係一個二叉樹模型\nB. 信息增益可以用熵，而唔係GINI系數嚟計算\nC. 迭代二叉樹3代算法要求特徵必須離散化\nD. 揀信息增益最大嘅特徵，作為樹嘅根節點\n答案係: \n問題：下面關於迭代二叉樹3代算法中講法錯誤嘅係\nA. 迭代二叉樹3代算法係一個二叉樹模型\nB. 信息增益可以用熵，而唔係GINI系數嚟計算\nC. 迭代二叉樹3代算法要求特徵必須離散化\nD. 揀信息增益最大嘅特徵，作為樹嘅根節點\n答案係: \n問題：下面關於迭代二叉樹3代算法中講法錯誤嘅係\nA. 迭代二叉樹3代算法係一個二叉樹模型\nB. 信息增益可以用熵，而唔係GINI系數嚟計算\nC. 迭代二叉樹3代算法要求特徵必須離散化\nD. 揀信息增益最大嘅特徵，作為樹嘅根節點\n答案係: \n問題：下面關於迭代二叉樹3代算法中講法錯誤嘅係\nA. 迭代二叉樹3代算法係一個二叉樹模型\nB. 信息增益可以用熵，而唔係GINI系數嚟計算\nC. 迭代二叉樹3代算法要求特徵必須離散化\nD. 揀信息增益最大嘅特徵，作為樹嘅根節點\n答案係: \n問題：下面關於迭代二叉樹3代算法中講法錯誤嘅係\nA. 迭代二叉樹3代算法係一個二叉樹模型\nB. 信息增益可以用熵，而唔係GINI系數嚟計算\nC. 迭代二叉樹3代算法要求特徵必須離散化\nD. 揀信息增益最大嘅特徵，作為樹嘅根節點\n答案係: \n問題：下面關於迭代二叉樹3代算法中講法錯誤嘅係\nA. 迭代二叉樹3代算法係一個二叉樹模型\nB. 信息增益可以用熵，而唔係GINI系數嚟計算\nC. 迭代二叉樹3代算法要求特徵必須離散化\nD. 揀信息增益最大嘅特徵，作為樹嘅根節點\n答案係: \n問題：下面關於迭代二叉樹3代算法中講法錯誤嘅係\nA. 迭代二叉樹3代算法係一個二叉樹模型\nB. 信息增益可以用熵，而唔係GINI系數嚟計算\nC. 迭代二叉樹3代算法要求特徵必須離散化\nD. 揀信息增益最大嘅特徵，作為樹嘅根節點\n答案係: \n問題：下面關於迭代二叉樹3代算法中講法錯誤嘅係\nA. 迭代二叉樹3代算法係一個二叉樹模型\nB. 信息增益可以用熵，而唔係GINI系數嚟計算\nC. 迭代二叉樹3代算法要求特徵必須離散化\nD. 揀信息增益最大嘅特徵，作為樹嘅根節點\n答案係: \n問題：下面關於迭代二叉樹3代算法中講法錯誤",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '迭代二叉樹3代算法係一個二叉樹模型'], ['B', '信息增益可以用熵，而唔係GINI系數嚟計算'], ['C', '迭代二叉樹3代算法要求特徵必須離散化'], ['D', '揀信息增益最大嘅特徵，作為樹嘅根節點']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下面邊個/啲選項對 K 摺交叉驗證嘅描述係正確嘅",
    "llm_output": "問題：下面邊個/啲選項對 K 摺交叉驗證嘅描述係正確嘅\nA. 如果 K=N，噉其稱為留一交叉驗證，其中 N 為驗證集入面嘅樣本數量\nB. 更大嘅 K 值相比於細 K 值將對交叉驗證結構有更高嘅信心\nC. 以上都係\nD. 增加 K 將導致交叉驗證結果時需要更多嘅時間\n答案係: \n問題：下面邊個/啲選項對 K 摺交叉驗證嘅描述係正確嘅\nA. 如果 K=N，噉其稱為留一交叉驗證，其中 N 為驗證集入面嘅樣本數量\nB. 更大嘅 K 值相比於細 K 值將對交叉驗證結構有更高嘅信心\nC. 以上都係\nD. 增加 K 將導致交叉驗證結果時需要更多嘅時間\n答案係: \n問題：下面邊個/啲選項對 K 摺交叉驗證嘅描述係正確嘅\nA. 如果 K=N，噉其稱為留一交叉驗證，其中 N 為驗證集入面嘅樣本數量\nB. 更大嘅 K 值相比於細 K 值將對交叉驗證結構有更高嘅信心\nC. 以上都係\nD. 增加 K 將導致交叉驗證結果時需要更多嘅時間\n答案係: \n問題：下面邊個/啲選項對 K 摺交叉驗證嘅描述係正確嘅\nA. 如果 K=N，噉其稱為留一交叉驗證，其中 N 為驗證集入面嘅樣本數量\nB. 更大嘅 K 值相比於細 K 值將對交叉驗證結構有更高嘅信心\nC. 以上都係\nD. 增加 K 將導致交叉驗證結果時需要更多嘅時間\n答案係: \n問題：下面邊個/啲選項對 K 摺交叉驗證嘅描述係正確嘅\nA. 如果 K=N，噉其稱為留一交叉驗證，其中 N 為驗證集入面嘅樣本數量\nB. 更大嘅 K 值相比於細 K 值將對交叉驗證結構有更高嘅信心\nC. 以上都係\nD. 增加 K 將導致交叉驗證結果時需要更多嘅時間\n答案係: \n問題：下面邊個/啲選項對 K 摺交叉驗證嘅描述係正確嘅\nA. 如果 K=N，噉其稱為留一交叉驗證，其中 N 為驗證集入面嘅樣本數量\nB. 更大嘅 K 值相比於細 K 值將對交叉驗證結構有更高嘅信心\nC. 以上都係\nD. 增加 K 將導致交叉驗證結果時需要更多嘅時間\n答案係: \n問題：下面邊個/啲選項對 K 摺交叉驗證嘅描述係正確嘅\nA. 如果 K=N，噉其稱為留一交叉驗證，其中 N 為驗證集入面嘅樣本數量\nB. 更大嘅 K 值相比於細 K 值將對交叉驗證結構有更高嘅信心\nC. 以上都係\nD. 增加 K 將導致交叉驗證結果時需要更多嘅時間\n答案係: \n問題：下面邊個/啲選項對 K 摺交叉驗證嘅描述係正確嘅\nA. 如果 K=N，噉其稱為留一交叉驗證，其中 N 為驗證集入面嘅樣本數量\nB. 更大嘅 K 值相比於細 K 值將對交叉驗證結構有更高嘅信心\nC. 以上都係\nD. 增加 K 將導致交叉驗證結果時需要更多嘅時間\n答案係: \n問題：下面邊個/��",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '如果 K=N，噉其稱為留一交叉驗證，其中 N 為驗證集入面嘅樣本數量'], ['B', '更大嘅 K 值相比於細 K 值將對交叉驗證結構有更高嘅信心'], ['C', '以上都係'], ['D', '增加 K 將導致交叉驗證結果時需要更多嘅時間']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "已知有m個樣本，進行n（n<=m）次抽樣。bootstrap數據係咩意思？",
    "llm_output": "問題：已知有m個樣本，進行n（n<=m）次抽樣。bootstrap數據係咩意思？\nA. 無放返地下總共N個樣本中抽樣n個樣本\nB. 無放返地下總共M個特徵中抽樣m個特徵\nC. 有放返地下總共N個樣本中抽樣n個樣本\nD. 有放返地下總共M個特徵中抽樣m個特徵\n答案係: C\n問題：已知有m個樣本，進行n（n<=m）次抽樣。bootstrap數據係咩意思？\nA. 無放返地下總共N個樣本中抽樣n個樣本\nB. 無放返地下總共M個特徵中抽樣m個特徵\nC. 有放返地下總共N個樣本中抽樣n個樣本\nD. 有放返地下總共M個特徵中抽樣m個特徵\n答案係: C\n問題：已知有m個樣本，進行n（n<=m）次抽樣。bootstrap數據係咩意思？\nA. 無放返地下總共N個樣本中抽樣n個樣本\nB. 無放返地下總共M個特徵中抽樣m個特徵\nC. 有放返地下總共N個樣本中抽樣n個樣本\nD. 有放返地下總共M個特徵中抽樣m個特徵\n答案係: C\n問題：已知有m個樣本，進行n（n<=m）次抽樣。bootstrap數據係咩意思？\nA. 無放返地下總共N個樣本中抽樣n個樣本\nB. 無放返地下總共M個特徵中抽樣m個特徵\nC. 有放返地下總共N個樣本中抽樣n個樣本\nD. 有放返地下總共M個特徵中抽樣m個特徵\n答案係: C\n問題：已知有m個樣本，進行n（n<=m）次抽樣。bootstrap數據係咩意思？\nA. 無放返地下總共N個樣本中抽樣n個樣本\nB. 無放返地下總共M個特徵中抽樣m個特徵\nC. 有放返地下總共N個樣本中抽樣n個樣本\nD. 有放返地下總共M個特徵中抽樣m個特徵\n答案係: C\n問題：已知有m個樣本，進行n（n<=m）次抽樣。bootstrap數據係咩意思？\nA. 無放返地下總共N個樣本中抽樣n個樣本\nB. 無放返地下總共M個特徵中抽樣m個特徵\nC. 有放返地下總共N個樣本中抽樣n個樣本\nD. 有放返地下總共M個特徵中抽樣m個特徵\n答案係: C\n問題：已知有m個樣本，進行n（n<=m）次抽樣。bootstrap數據係咩意思？\nA. 無放返地下總共N個樣本中抽樣n個樣本\nB. 無放返地下總共M個特徵中抽樣m個特徵\nC. 有放返地下總共N個樣本中抽樣n個樣本\nD. 有放返地下總共M個特徵中抽樣m個特徵\n答案係: C\n問題：已知有m個樣本，進行n（n<=m）次抽樣。bootstrap數據係咩意思？\nA. 無放返地下總共N個樣本中抽樣n個樣本\nB. 無放返地下總共M個特徵中抽樣m個特徵\nC. 有放返地下總共N個樣本中抽樣n個樣本\nD. 有放返地下總共M個特徵中抽樣m個特徵\n答案係: C\n問題：已知有m個樣本，進行n（n<=m）次抽樣。bootstrap數據係咩意思？\nA. 無放返地下總共N個樣本中抽樣n個樣本\nB. 無放返地下總共M個特徵中抽樣m個特徵\nC. 有放返地下總共N個樣本中抽樣n個樣本\nD. 有放返",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '無放返地下總共N個樣本中抽樣n個樣本'], ['B', '無放返地下總共M個特徵中抽樣m個特徵'], ['C', '有放返地下總共N個樣本中抽樣n個樣本'], ['D', '有放返地下總共M個特徵中抽樣m個特徵']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "如果以特徵向量嘅相關系數作為模式相似性測度，則影響聚類算法結果嘅主要因素有",
    "llm_output": "聚類分析嘅主要目標係將同類嘅樣本分組，而分組嘅準則有好多種，例如：\n1. 相似性：係指樣本之間嘅相似程度，常用嘅測度係相關係數。\n2. 距離：係指樣本之間嘅距離，常用嘅測度係歐氏距離、曼哈頓距離等。\n3. 類別：係指樣本嘅已知類別，常用嘅測度係K-近鄰法。\n4. 其他：例如：分類準則、樣本數量等。\n所以，影響聚類算法結果嘅主要因素有量綱、已知類別樣本質量、分類準則等。",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '量綱'], ['B', '已知類別樣本質量'], ['C', '以上都唔係'], ['D', '分類準則']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "模型訓練過程中，我哋一般將數據分成",
    "llm_output": "問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA. 驗證集\nB. 測試集\nC. 訓練集\nD. 其他選項均可\n答案係: \n問題：模型訓練過程中，我哋一般將數據分成\nA",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '驗證集'], ['B', '測試集'], ['C', '訓練集'], ['D', '其他選項均可']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "你正在使用帶有 L1 正則化嘅 logistic 回歸做二分類，其中 C 係正則化參數，w1 同 w2 係 x1 同 x2 嘅系數。當你把 C 值由 0 增加至非常大嘅值時，下面邊個選項係正確嘅",
    "llm_output": "答案嘅解釋：\n當 C 值大到某個程度之後，係唔會影響到係訓練集上嘅誤差，但係會影響到係測試集上嘅誤差。\n因為係訓練集上嘅誤差唔會影響到係測試集上嘅誤差，所以係訓練集上嘅誤差唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係數唔會影響到係訓練集上嘅係數。\n係訓練集上嘅係數唔會影響到係訓練集上嘅係數，所以係訓練集上嘅係",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '第一個 w1 成咗 0，接住 w2 都成咗 0'], ['B', 'w1 同 w2 同時成咗 0'], ['C', '第一個 w2 成咗 0，接住 w1 都成咗 0'], ['D', '即使喺 C 成為大值之後，w1 同 w2 都唔能成 0']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列方法中，唔可以用於特徵降維嘅方法包括",
    "llm_output": "問題：下列方法中，唔可以用於特徵降維嘅方法包括\nA. 深度學習SparseAutoEncoder\nB. 矩陣奇異值分解SVD\nC. 線性判別分析\nD. 主成分分析\n答案係: \n問題：下列方法中，唔可以用於特徵降維嘅方法包括\nA. 深度學習SparseAutoEncoder\nB. 矩陣奇異值分解SVD\nC. 線性判別分析\nD. 主成分分析\n答案係: \n問題：下列方法中，唔可以用於特徵降維嘅方法包括\nA. 深度學習SparseAutoEncoder\nB. 矩陣奇異值分解SVD\nC. 線性判別分析\nD. 主成分分析\n答案係: \n問題：下列方法中，唔可以用於特徵降維嘅方法包括\nA. 深度學習SparseAutoEncoder\nB. 矩陣奇異值分解SVD\nC. 線性判別分析\nD. 主成分分析\n答案係: \n問題：下列方法中，唔可以用於特徵降維嘅方法包括\nA. 深度學習SparseAutoEncoder\nB. 矩陣奇異值分解SVD\nC. 線性判別分析\nD. 主成分分析\n答案係: \n問題：下列方法中，唔可以用於特徵降維嘅方法包括\nA. 深度學習SparseAutoEncoder\nB. 矩陣奇異值分解SVD\nC. 線性判別分析\nD. 主成分分析\n答案係: \n問題：下列方法中，唔可以用於特徵降維嘅方法包括\nA. 深度學習SparseAutoEncoder\nB. 矩陣奇異值分解SVD\nC. 線性判別分析\nD. 主成分分析\n答案係: \n問題：下列方法中，唔可以用於特徵降維嘅方法包括\nA. 深度學習SparseAutoEncoder\nB. 矩陣奇異值分解SVD\nC. 線性判別分析\nD. 主成分分析\n答案係: \n問題：下列方法中，唔可以用於特徵降維嘅方法包括\nA. 深度學習SparseAutoEncoder\nB. 矩陣奇異值分解SVD\nC. 線性判別分析\nD. 主成分分析\n答案係: \n問題：下列方法中，唔可以用於特徵降維嘅方法包括\nA. 深度學習SparseAutoEncoder\nB. 矩陣奇異值分解SVD\nC. 線性判別分析\nD. 主成分分析\n答案係: \n問題：下列方法中，唔可以用於特徵降維嘅方法包括\nA. 深度學習SparseAutoEncoder\nB. 矩陣奇異值分解SVD\nC. 線性判別分析\nD. 主成分分析\n答案係: \n問題：下列方法中，唔可以用於特徵降維嘅方法包括\nA. 深度學習SparseAutoEncoder\nB. 矩陣奇異值分解SVD\nC. 線性判別分析\nD. 主成分分析\n答案係: \n問題：下列方法中，唔可以用於特徵降維嘅方法包括\nA. 深度學習SparseAutoEncoder\nB. 矩陣奇異值分解SVD\nC. 線性判別分析\nD. 主成分分析\n答案係: \n問題：下列方法中，唔可以用於特徵降維嘅方法包括\nA. 深度學習SparseAutoEncoder\nB. 矩陣奇異值分解SVD\nC. 線性判別分析\nD. 主成分分析\n答案係: \n問題：下列方法中，唔可以用於特徵降維嘅方法包括\nA. 深度學習SparseAutoEncoder\nB. 矩陣奇異值分解SVD\nC. 線性判別分析\nD. 主成分分析\n答案係: \n問題：下列方法中，唔可以用於特徵降維嘅方法包括\nA. 深度學習SparseAutoEncoder\nB. 矩陣奇異值分解SVD\nC. 線性判別分析\nD. 主成分分析\n答案係: \n",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '深度學習SparseAutoEncoder'], ['B', '矩陣奇異值分解SVD'], ['C', '線性判別分析'], ['D', '主成分分析']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "貝葉斯定理求得係咩概率?",
    "llm_output": "貝葉斯定理求得係條件概率。\n貝葉斯定理求得係條件概率。\n貝葉斯定理求得係先驗概率。\n貝葉斯定理求得係聯合概率。",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '先驗概率'], ['B', '其他選項都唔係'], ['C', '條件概率'], ['D', '聯合概率']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "喺機器學習中，解釋學習器泛化性能中經常用到偏差-方差分解，下列講法唔正確嘅係",
    "llm_output": "偏差-方差分解\n偏差-方差分解（Bias-Variance Decomposition）係指喺機器學習中，解釋學習器泛化性能中經常用到嘅一個概念，用來分析學習器泛化性能嘅原因。\n偏差（Bias）體現嘅係學習器預測嘅準確度，偏差越大，學習器預測嘅準確度越低。\n方差（Variance）體現嘅係學習器預測嘅穩定性，方差越大，學習器預測嘅穩定性越低。\n泛化性能係由學習算法嘅能力、數據嘅充分性以及學習任務本身嘅難度所共同決定嘅。\n方差指嘅係預測嘅期望值同真實值嘅偏差。",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '方差體現嘅係學習器預測嘅穩定性'], ['B', '偏差體現嘅係學習器預測嘅準確度'], ['C', '泛化性能係由學習算法嘅能力、數據嘅充分性以及學習任務本身嘅難度所共同決定嘅'], ['D', '方差指嘅係預測嘅期望值同真實值嘅偏差']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "假如你用logistic Regression 算法去預測電腦銷量，當你喺新嘅測試集上驗證你嘅假設時，發現預測值有好大嘅偏差，並且你嘅假設喺訓練集上表現都好差，下面啲步驟你應該避免採納",
    "llm_output": "問題：假如你用logistic Regression 算法去預測電腦銷量，當你喺新嘅測試集上驗證你嘅假設時，發現預測值有好大嘅偏差，並且你嘅假設喺訓練集上表現都好差，下面啲步驟你應該避免採納\nA. 嘗試住減細正則項 λ\nB. 嘗試增加交叉特徵\nC. 增加樣本量\nD. 嘗試更細嘅測試集或者特徵\n答案係: \n問題：假如你用logistic Regression 算法去預測電腦銷量，當你喺新嘅測試集上驗證你嘅假設時，發現預測值有好大嘅偏差，並且你嘅假設喺訓練集上表現都好差，下面啲步驟你應該避免採納\nA. 嘗試住減細正則項 λ\nB. 嘗試增加交叉特徵\nC. 增加樣本量\nD. 嘗試更細嘅測試集或者特徵\n答案係: \n問題：假如你用logistic Regression 算法去預測電腦銷量，當你喺新嘅測試集上驗證你嘅假設時，發現預測值有好大嘅偏差，並且你嘅假設喺訓練集上表現都好差，下面啲步驟你應該避免採納\nA. 嘗試住減細正則項 λ\nB. 嘗試增加交叉特徵\nC. 增加樣本量\nD. 嘗試更細嘅測試集或者特徵\n答案係: \n問題：假如你用logistic Regression 算法去預測電腦銷量，當你喺新嘅測試集上驗證你嘅假設時，發現預測值有好大嘅偏差，並且你嘅假設喺訓練集上表現都好差，下面啲步驟你應該避免採納\nA. 嘗試住減細正則項 λ\nB. 嘗試增加交叉特徵\nC. 增加樣本量\nD. 嘗試更細嘅測試集或者特徵\n答案係: \n問題：假如你用logistic Regression 算法去預測電腦銷量，當你喺新嘅測試集上驗證你嘅假設時，發現預測值有好大嘅偏差，並且你嘅假設喺訓練集上表現都好差，下面啲步驟你應該避免採納\nA. 嘗試住減細正則項 λ\nB. 嘗試增加交叉特徵\nC. 增加樣本量\nD. 嘗試更細嘅測試集或者特徵\n答案係: \n問題：假如你用logistic Regression 算法去預測電腦銷量，當你喺新嘅測試集上驗證你嘅假設時，發現預測值有好大嘅偏差，並且你嘅假設喺訓練集上表現都好差，下面啲步驟你應該避免採納\nA. 嘗試住減細正則項 λ\nB. 嘗試增加交叉特徵\nC. 增加樣本量\nD. 嘗試更細嘅測試集或者特徵\n答案係: \n問題：假如你用logistic Regression 算法去預測電腦銷量，當你喺新嘅測試集上驗證你嘅假設時，發現預測值有好大嘅偏差，並且你嘅假設喺訓練集上表現都好差，下面啲步驟你應該避免採納\nA. 嘗試住減細正則項 λ\nB. 嘗試增加交叉特徵\nC. 增加樣本量\nD. 嘗試更細嘅測試集或者特徵\n答案係: \n問題：假如你用logistic Regression 算法去預測電腦銷量，當你喺新嘅測試集上驗證你嘅假設時，發現預測值有好大嘅偏差，並且你嘅假",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '嘗試住減細正則項 λ'], ['B', '嘗試增加交叉特徵'], ['C', '增加樣本量'], ['D', '嘗試更細嘅測試集或者特徵']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以P(w)表示詞條w嘅概率，假設已知P（南京）=0.8，P（市長）=0.6，P（江大橋）=0.4：P（南京市）=0.3，P（長江大橋）=0.5：如果假設前後兩個詞嘅出現係獨立嘅，噉分詞結果就係",
    "llm_output": "D. 南京市_長江大橋\nP(南京) = 0.8\nP(市長) = 0.6\nP(江大橋) = 0.4\nP(南京市) = 0.3\nP(長江大橋) = 0.5\nP(南京) * P(市長) * P(江大橋) = 0.8 * 0.6 * 0.4 = 0.192\nP(南京市) * P(長江大橋) = 0.3 * 0.5 = 0.15\n由於P(南京) * P(市長) * P(江大橋) > P(南京市) * P(長江大橋)，所以分詞結果就係南京市長江大橋。\n問題：以P(w)表示詞條w嘅概率，假設已知P（南京）=0.8，P（市長）=0.6，P（江大橋）=0.4：P（南京市）=0.3，P（長江大橋）=0.5：如果假設前後兩個詞嘅出現係獨立嘅，噉分詞結果就係\nA. 南京_市長_江大橋\nB. 南京市_長江_大橋\nC. 南京市長_江大橋\nD. 南京市_長江大橋\n答案係: \nD. 南京市_長江大橋\nP(南京) = 0.8\nP(市長) = 0.6\nP(江大橋) = 0.4\nP(南京市) = 0.3\nP(長江大橋) = 0.5\nP(南京) * P(市長) * P(江大橋) = 0.8 * 0.6 * 0.4 = 0.192\nP(南京市) * P(長江大橋) = 0.3 * 0.5 = 0.15\n由於P(南京) * P(市長) * P(江大橋) > P(南京市) * P(長江大橋)，所以分詞結果就係南京市長江大橋。\n問題：以P(w)表示詞條w嘅概率，假設已知P（南京）=0.8，P（市長）=0.6，P（江大橋）=0.4：P（南京市）=0.3，P（長江大橋）=0.5：如果假設前後兩個詞嘅出現係獨立嘅，噉分詞結果就係\nA. 南京_市長_江大橋\nB. 南京市_長江_大橋\nC. 南京市長_江大橋\nD. 南京市_長江大橋\n答案係: \nD. 南京市_長江大橋\nP(南京) = 0.8\nP(市長) = 0.6\nP(江大橋) = 0.4\nP(南京市) = 0.3\nP(長江大橋) = 0.5\nP(南京) * P(市長) * P(江大橋) = 0.8 * 0.6 * 0.4 = 0.192\nP(南京市) * P(長江大橋) = 0.3 * 0.5 = 0.15\n由於P(南京) * P(市長) * P(江大橋) > P(南京市) * P(長江大橋)，所以分詞結果就係南京市長江大橋。\n問題：以P(w)表示詞條w嘅概率，假設已知P（南京）=0.8，P（市長）=0.6，P（江大橋）=0.4：P（南京市）=0.3，P（長江大橋）=0.5：如果假設前後兩個詞嘅出現係獨立嘅，噉分詞結果就係\nA. 南京_市長_江大橋\nB. 南京市_長江_大橋\nC. 南京市長_江大橋\nD. 南京市_長江大橋\n答案係: \nD. 南京市_長江大橋\nP(南京) = 0.8\nP(市長) = 0.6\nP(江大橋) = 0.4\nP(南京市) = 0.3\nP(長江大橋) = 0.5\nP(",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '南京_市長_江大橋'], ['B', '南京市_長江_大橋'], ['C', '南京市長_江大橋'], ['D', '南京市_長江大橋']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "對數幾率回歸（logistics regression）同一般回歸分析有咩區別",
    "llm_output": "對數幾率回歸同一般回歸分析嘅區別係，對數幾率回歸係設計用嚟預測事件可能性嘅，而一般回歸分析係用嚟估計回歸系數。\n對數幾率回歸可以用嚟度量模型擬合程度，但一般回歸分析唔可以。",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '對數幾率回歸係設計用嚟預測事件可能性嘅'], ['B', '對數幾率回歸可以用嚟估計回歸系數'], ['C', '以上都係'], ['D', '對數幾率回歸可以用嚟度量模型擬合程度']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下面啲對「類型 1（Type-1）」同「類型 2（Type-2）」錯誤嘅描述係錯誤嘅",
    "llm_output": "問題：下面啲對「類型 1（Type-1）」同「類型 2（Type-2）」錯誤嘅描述係錯誤嘅\nA. 類型 1 錯誤通常喺其係正確嘅情況下拒絕假設而出現\nB. 類型 1 通常稱之為假正類，類型 2 通常稱之為假負類\nC. 以上都係\nD. 類型 2 通常稱之為假正類，類型 1 通常稱之為假負類\n答案係: \n問題：下面啲對「類型 1（Type-1）」同「類型 2（Type-2）」錯誤嘅描述係錯誤嘅\nA. 類型 1 錯誤通常喺其係正確嘅情況下拒絕假設而出現\nB. 類型 1 通常稱之為假正類，類型 2 通常稱之為假負類\nC. 以上都係\nD. 類型 2 通常稱之為假正類，類型 1 通常稱之為假負類\n答案係: \n問題：下面啲對「類型 1（Type-1）」同「類型 2（Type-2）」錯誤嘅描述係錯誤嘅\nA. 類型 1 錯誤通常喺其係正確嘅情況下拒絕假設而出現\nB. 類型 1 通常稱之為假正類，類型 2 通常稱之為假負類\nC. 以上都係\nD. 類型 2 通常稱之為假正類，類型 1 通常稱之為假負類\n答案係: \n問題：下面啲對「類型 1（Type-1）」同「類型 2（Type-2）」錯誤嘅描述係錯誤嘅\nA. 類型 1 錯誤通常喺其係正確嘅情況下拒絕假設而出現\nB. 類型 1 通常稱之為假正類，類型 2 通常稱之為假負類\nC. 以上都係\nD. 類型 2 通常稱之為假正類，類型 1 通常稱之為假負類\n答案係: \n問題：下面啲對「類型 1（Type-1）」同「類型 2（Type-2）」錯誤嘅描述係錯誤嘅\nA. 類型 1 錯誤通常喺其係正確嘅情況下拒絕假設而出現\nB. 類型 1 通常稱之為假正類，類型 2 通常稱之為假負類\nC. 以上都係\nD. 類型 2 通常稱之為假正類，類型 1 通常稱之為假負類\n答案係: \n問題：下面啲對「類型 1（Type-1）」同「類型 2（Type-2）」錯誤嘅描述係錯誤嘅\nA. 類型 1 錯誤通常喺其係正確嘅情況下拒絕假設而出現\nB. 類型 1 通常稱之為假正類，類型 2 通常稱之為假負類\nC. 以上都係\nD. 類型 2 通常稱之為假正類，類型 1 通常稱之為假負類\n答案係: \n問題：下面啲對「類型 1（Type-1）」同「類型 2（Type-2）」錯誤嘅描述係錯誤嘅\nA. 類型 1 錯誤通常喺其係正確嘅情況下拒絕假設而出現\nB. 類型 1 通常稱之為假正類，類型 2 通常稱之為假負類\nC. 以上都係\nD. 類型 2 通常稱之為假正類，類型 1 通常稱之為假負類\n答案係: \n問題：下面啲對「類型 1（Type-1）」同「類型 ",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '類型 1 錯誤通常喺其係正確嘅情況下拒絕假設而出現'], ['B', '類型 1 通常稱之為假正類，類型 2 通常稱之為假負類'], ['C', '以上都係'], ['D', '類型 2 通常稱之為假正類，類型 1 通常稱之為假負類']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "影響基本K-均值算法嘅主要因素有",
    "llm_output": "問題：下列哪一項係關於K-均值算法嘅錯誤描述？\nA. 該算法係一個無監督學習算法\nB. 該算法係一個有監督學習算法\nC. 該算法係一個半監督學習算法\nD. 該算法係一個有標籤嘅學習算法\n答案係: \n問題：下列哪一項係關於K-均值算法嘅錯誤描述？\nA. 該算法係一個半監督學習算法\nB. 該算法係一個有標籤嘅學習算法\nC. 該算法係一個有監督學習算法\nD. 該算法係一個無監督學習算法\n答案係: \n問題：下列哪一項係關於K-均值算法嘅錯誤描述？\nA. 該算法係一個有標籤嘅學習算法\nB. 該算法係一個有監督學習算法\nC. 該算法係一個無監督學習算法\nD. 該算法係一個半監督學習算法\n答案係: \n問題：下列哪一項係關於K-均值算法嘅錯誤描述？\nA. 該算法係一個有標籤嘅學習算法\nB. 該算法係一個有監督學習算法\nC. 該算法係一個無監督學習算法\nD. 該算法係一個半監督學習算法\n答案係: \n問題：下列哪一項係關於K-均值算法嘅錯誤描述？\nA. 該算法係一個有標籤嘅學習算法\nB. 該算法係一個有監督學習算法\nC. 該算法係一個無監督學習算法\nD. 該算法係一個半監督學習算法\n答案係: \n問題：下列哪一項係關於K-均值算法嘅錯誤描述？\nA. 該算法係一個有標籤嘅學習算法\nB. 該算法係一個有監督學習算法\nC. 該算法係一個無監督學習算法\nD. 該算法係一個半監督學習算法\n答案係: \n問題：下列哪一項係關於K-均值算法嘅錯誤描述？\nA. 該算法係一個有標籤嘅學習算法\nB. 該算法係一個有監督學習算法\nC. 該算法係一個無監督學習算法\nD. 該算法係一個半監督學習算法\n答案係: \n問題：下列哪一項係關於K-均值算法嘅錯誤描述？\nA. 該算法係一個有標籤嘅學習算法\nB. 該算法係一個有監督學習算法\nC. 該算法係一個無監督學習算法\nD. 該算法係一個半監督學習算法\n答案係: \n問題：下列哪一項係關於K-均值算法嘅錯誤描述？\nA. 該算法係一個有標籤嘅學習算法\nB. 該算法係一個有監督學習算法\nC. 該算法係一個無監督學習算法\nD. 該算法係一個半監督學習算法\n答案係: \n問題：下列哪一項係關於K-均值算法嘅錯誤描述？\nA. 該算法係一個有標籤嘅學習算法\nB. 該算法係一個有監督學習算法\nC. 該算法係一個無監督學習算法\nD. 該算法係一個半監督學習算法\n答案係: \n問題：下列哪一項係關於K-均值算法嘅錯誤描述？\nA. 該算法係一個有標籤嘅學習算法\nB. 該算法係一個有監督學習算法\nC. 該算法係一個無監督學習算法\nD. 該算法係一個半監督學習算法\n答案係: \n問題：下列哪一項係關於K-均值算法嘅錯誤描述？\nA. 該算法係一個有標籤嘅學習算法\nB. 該算法係一個有監督學習算法\nC. 該算法係一個無監督學習算法\nD. 該算法係一個半監督學習算法\n答案係: \n問題：下列哪一項係關於K-均值算法嘅錯誤描述？\nA. 該算法係一個",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '初始類中心嘅揀'], ['B', '聚類準則'], ['C', '樣本輸入順序'], ['D', '模式相似性測度']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "高斯混合模型(GMM)係一種咩模型",
    "llm_output": "高斯混合模型(GMM)係一種有監督學習模型。\n高斯混合模型(GMM)係一種有監督學習模型。",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '無監督學習模型'], ['B', '其他選項都唔係'], ['C', '半監督學習模型'], ['D', '有監督學習模型']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "基於語法規則嘅方法為",
    "llm_output": "問題：下列嘅方法中，唔屬於機器學習嘅係\nA. 條件隨機場\nB. 最大熵模型\nC. 句法、語義分析\nD. 最大熵隱馬爾科夫模型\n答案係: \n問題：下列嘅方法中，唔屬於機器學習嘅係\nA. 條件隨機場\nB. 最大熵模型\nC. 句法、語義分析\nD. 最大熵隱馬爾科夫模型\n答案係: \n問題：下列嘅方法中，唔屬於機器學習嘅係\nA. 條件隨機場\nB. 最大熵模型\nC. 句法、語義分析\nD. 最大熵隱馬爾科夫模型\n答案係: \n問題：下列嘅方法中，唔屬於機器學習嘅係\nA. 條件隨機場\nB. 最大熵模型\nC. 句法、語義分析\nD. 最大熵隱馬爾科夫模型\n答案係: \n問題：下列嘅方法中，唔屬於機器學習嘅係\nA. 條件隨機場\nB. 最大熵模型\nC. 句法、語義分析\nD. 最大熵隱馬爾科夫模型\n答案係: \n問題：下列嘅方法中，唔屬於機器學習嘅係\nA. 條件隨機場\nB. 最大熵模型\nC. 句法、語義分析\nD. 最大熵隱馬爾科夫模型\n答案係: \n問題：下列嘅方法中，唔屬於機器學習嘅係\nA. 條件隨機場\nB. 最大熵模型\nC. 句法、語義分析\nD. 最大熵隱馬爾科夫模型\n答案係: \n問題：下列嘅方法中，唔屬於機器學習嘅係\nA. 條件隨機場\nB. 最大熵模型\nC. 句法、語義分析\nD. 最大熵隱馬爾科夫模型\n答案係: \n問題：下列嘅方法中，唔屬於機器學習嘅係\nA. 條件隨機場\nB. 最大熵模型\nC. 句法、語義分析\nD. 最大熵隱馬爾科夫模型\n答案係: \n問題：下列嘅方法中，唔屬於機器學習嘅係\nA. 條件隨機場\nB. 最大熵模型\nC. 句法、語義分析\nD. 最大熵隱馬爾科夫模型\n答案係: \n問題：下列嘅方法中，唔屬於機器學習嘅係\nA. 條件隨機場\nB. 最大熵模型\nC. 句法、語義分析\nD. 最大熵隱馬爾科夫模型\n答案係: \n問題：下列嘅方法中，唔屬於機器學習嘅係\nA. 條件隨機場\nB. 最大熵模型\nC. 句法、語義分析\nD. 最大熵隱馬爾科夫模型\n答案係: \n問題：下列嘅方法中，唔屬於機器學習嘅係\nA. 條件隨機場\nB. 最大熵模型\nC. 句法、語義分析\nD. 最大熵隱馬爾科夫模型\n答案係: \n問題：下列嘅方法中，唔屬於機器學習嘅係\nA. 條件隨機場\nB. 最大熵模型\nC. 句法、語義分析\nD. 最大熵隱馬爾科夫模型\n答案係: \n問題：下列嘅方法中，唔屬於機器學習嘅係\nA. 條件隨機場\nB. 最大熵模型\nC. 句法、語義分析\nD. 最大熵隱馬爾科夫模型\n答案係: \n問題：下列嘅方法中，唔屬於機器學習嘅",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '條件隨機場'], ['B', '最大熵模型'], ['C', '句法、語義分析'], ['D', '最大熵隱馬爾科夫模型']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "有兩個樣本點，第一個點為正樣本,佢嘅特徵向量係(0,-1);第二個點為負樣本,佢嘅特徵向量係(2,3),從呢兩個樣本點組成嘅訓練集構建一個線性SVM分類器嘅分類面方程係",
    "llm_output": "問題：有兩個樣本點，第一個點為正樣本,佢嘅特徵向量係(0,-1);第二個點為負樣本,佢嘅特徵向量係(2,3),從呢兩個樣本點組成嘅訓練集構建一個線性SVM分類器嘅分類面方程係\nA. 2x-y=0\nB. x+2y=5\nC. x+2y=3\nD. 2x+y=4\n答案係: \n問題：有兩個樣本點，第一個點為正樣本,佢嘅特徵向量係(0,-1);第二個點為負樣本,佢嘅特徵向量係(2,3),從呢兩個樣本點組成嘅訓練集構建一個線性SVM分類器嘅分類面方程係\nA. 2x-y=0\nB. x+2y=5\nC. x+2y=3\nD. 2x+y=4\n答案係: \n問題：有兩個樣本點，第一個點為正樣本,佢嘅特徵向量係(0,-1);第二個點為負樣本,佢嘅特徵向量係(2,3),從呢兩個樣本點組成嘅訓練集構建一個線性SVM分類器嘅分類面方程係\nA. 2x-y=0\nB. x+2y=5\nC. x+2y=3\nD. 2x+y=4\n答案係: \n問題：有兩個樣本點，第一個點為正樣本,佢嘅特徵向量係(0,-1);第二個點為負樣本,佢嘅特徵向量係(2,3),從呢兩個樣本點組成嘅訓練集構建一個線性SVM分類器嘅分類面方程係\nA. 2x-y=0\nB. x+2y=5\nC. x+2y=3\nD. 2x+y=4\n答案係: \n問題：有兩個樣本點，第一個點為正樣本,佢嘅特徵向量係(0,-1);第二個點為負樣本,佢嘅特徵向量係(2,3),從呢兩個樣本點組成嘅訓練集構建一個線性SVM分類器嘅分類面方程係\nA. 2x-y=0\nB. x+2y=5\nC. x+2y=3\nD. 2x+y=4\n答案係: \n問題：有兩個樣本點，第一個點為正樣本,佢嘅特徵向量係(0,-1);第二個點為負樣本,佢嘅特徵向量係(2,3),從呢兩個樣本點組成嘅訓練集構建一個線性SVM分類器嘅分類面方程係\nA. 2x-y=0\nB. x+2y=5\nC. x+2y=3\nD. 2x+y=4\n答案係: \n問題：有兩個樣本點，第一個點為正樣本,佢嘅特徵向量係(0,-1);第二個點為負樣本,佢嘅特徵向量係(2,3),從呢兩個樣本點組成嘅訓練集構建一個線性SVM分類器嘅分類面方程係\nA. 2x-y=0\nB. x+2y=5\nC. x+2y=3\nD. 2x+y=4\n答案係: \n問題：有兩個樣本點，第一個點為正樣本,佢嘅特徵向量係(0,-1);第二個點為負樣本,佢嘅特徵向量係(2,3),從呢兩個樣本點組成嘅訓練集構建一個線性SVM分類器嘅分類面方程係\nA. ",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '2x-y=0'], ['B', 'x+2y=5'], ['C', 'x+2y=3'], ['D', '2x+y=4']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "隱馬爾科夫模型，設其觀察值空間為 狀態空間為 如果用維特比算法(Viterbi algorithm)進行解碼，時間複雜度為",
    "llm_output": "隱馬爾科夫模型，設其觀察值空間為 狀態空間為 如果用維特比算法(Viterbi algorithm)進行解碼，時間複雜度為\nA. O(NK)\nB. O(N^2K)\nC. 以上都唔係\nD. O(NK^2)\n答案係: D\n維特比算法，時間複雜度為 O(NK^2)\n隱馬爾科夫模型，設其觀察值空間為 狀態空間為 如果用維特比算法(Viterbi algorithm)進行解碼，時間複雜度為\nA. O(NK)\nB. O(N^2K)\nC. 以上都唔係\nD. O(NK^2)\n答案係: D\n維特比算法，時間複雜度為 O(NK^2)\n隱馬爾科夫模型，設其觀察值空間為 狀態空間為 如果用維特比算法(Viterbi algorithm)進行解碼，時間複雜度為\nA. O(NK)\nB. O(N^2K)\nC. 以上都唔係\nD. O(NK^2)\n答案係: D\n維特比算法，時間複雜度為 O(NK^2)\n隱馬爾科夫模型，設其觀察值空間為 狀態空間為 如果用維特比算法(Viterbi algorithm)進行解碼，時間複雜度為\nA. O(NK)\nB. O(N^2K)\nC. 以上都唔係\nD. O(NK^2)\n答案係: D\n維特比算法，時間複雜度為 O(NK^2)",
    "correct_answer": "C",
    "standard_answer_range": "[['A', 'O(NK)'], ['B', 'O(N^2K)'], ['C', '以上都唔係'], ['D', 'O(NK^2)']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "假定你使用咗一個好大γ值嘅RBF核，呢個意味住：",
    "llm_output": "問題：假定你使用咗一個好小γ值嘅RBF核，呢個意味住：\nA. 模型唔會被點到超平面嘅距離所影響\nB. 模型僅使用接近超平面嚟建模\nC. 模型將考慮使用遠離超平面嚟建模\nD. 以上都唔係\n答案係: \n問題：假定你使用咗一個好大γ值嘅RBF核，呢個意味住：\nA. 模型唔會被點到超平面嘅距離所影響\nB. 模型僅使用接近超平面嚟建模\nC. 模型將考慮使用遠離超平面嚟建模\nD. 以上都唔係\n答案係: \n問題：假定你使用咗一個好小γ值嘅RBF核，呢個意味住：\nA. 模型唔會被點到超平面嘅距離所影響\nB. 模型僅使用接近超平面嚟建模\nC. 模型將考慮使用遠離超平面嚟建模\nD. 以上都唔係\n答案係: \n問題：假定你使用咗一個好大γ值嘅RBF核，呢個意味住：\nA. 模型唔會被點到超平面嘅距離所影響\nB. 模型僅使用接近超平面嚟建模\nC. 模型將考慮使用遠離超平面嚟建模\nD. 以上都唔係\n答案係: \n問題：假定你使用咗一個好小γ值嘅RBF核，呢個意味住：\nA. 模型唔會被點到超平面嘅距離所影響\nB. 模型僅使用接近超平面嚟建模\nC. 模型將考慮使用遠離超平面嚟建模\nD. 以上都唔係\n答案係: \n問題：假定你使用咗一個好大γ值嘅RBF核，呢個意味住：\nA. 模型唔會被點到超平面嘅距離所影響\nB. 模型僅使用接近超平面嚟建模\nC. 模型將考慮使用遠離超平面嚟建模\nD. 以上都唔係\n答案係: \n問題：假定你使用咗一個好小γ值嘅RBF核，呢個意味住：\nA. 模型唔會被點到超平面嘅距離所影響\nB. 模型僅使用接近超平面嚟建模\nC. 模型將考慮使用遠離超平面嚟建模\nD. 以上都唔係\n答案係: \n問題：假定你使用咗一個好大γ值嘅RBF核，呢個意味住：\nA. 模型唔會被點到超平面嘅距離所影響\nB. 模型僅使用接近超平面嚟建模\nC. 模型將考慮使用遠離超平面嚟建模\nD. 以上都唔係\n答案係: \n問題：假定你使用咗一個好小γ值嘅RBF核，呢個意味住：\nA. 模型唔會被點到超平面嘅距離所影響\nB. 模型僅使用接近超平面嚟建模\nC. 模型將考慮使用遠離超平面嚟建模\nD. 以上都唔係\n答案係: \n問題：假定你使用咗一個好大γ值嘅RBF核，呢個意味住：\nA. 模型唔會被點到超平面嘅距離所影響\nB. 模型僅使用接近超平面嚟建模\nC. 模型將考慮使用遠離超平面嚟建模\nD. 以上都唔係\n答案係: \n問題：假定你使用咗一個好小γ值嘅RBF核，呢個意味住：\nA. 模型唔會被點到超平面嘅距離所影響\nB. 模型僅使用接近超平面嚟建模\nC. 模型將考慮使用遠離超平面嚟建模\nD. 以上都唔係\n答案係: \n問題",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '模型唔會被點到超平面嘅距離所影響'], ['B', '以上都唔係'], ['C', '模型僅使用接近超平面嘅點嚟建模'], ['D', '模型將考慮使用遠離超平面嘅點建模']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "關於 ARMA (auto regressive moving average model)（自回歸滑動平均模型）、 AR (auto regressive model)（自回歸模型）、 MA（滑動平均模型） 模型嘅功率譜，下列講法正確嘅係",
    "llm_output": "ARMA模型係一個複雜嘅濾波器，由AR模型同MA模型組成。\nAR模型係一個自回歸模型，係一個濾波器，係一個複雜嘅濾波器，由複數嘅極點同零點組成。\nMA模型係一個滑動平均模型，係一個濾波器，係一個複雜嘅濾波器，由複數嘅極點同零點組成。\nARMA模型嘅功率譜係一個複雜嘅濾波器嘅功率譜，由複數嘅極點同零點組成。\nAR模型嘅功率譜係一個複數嘅極點嘅功率譜，係一個複雜嘅濾波器嘅功率譜，由複數嘅極點同零點組成。\nMA模型嘅功率譜係一個複數嘅零點嘅功率譜，係一個複雜嘅濾波器嘅功率譜，由複數嘅極點同零點組成。\nARMA模型嘅功率譜係一個複數嘅極點同零點嘅功率譜，係一個複雜嘅濾波器嘅功率譜，由複數嘅極點同零點組成。\nAR模型嘅功率譜係一個複數嘅極點嘅功率譜，係一個複雜嘅濾波器嘅功率譜，由複數嘅極點同零點組成。\nMA模型嘅功率譜係一個複數嘅零點嘅功率譜，係一個複雜嘅濾波器嘅功率譜，由複數嘅極點同零點組成。\nARMA模型嘅功率譜係一個複數嘅極點同零點嘅功率譜，係一個複雜嘅濾波器嘅功率譜，由複數嘅極點同零點組成。\nAR模型嘅功率譜係一個複數嘅極點嘅功率譜，係一個複雜嘅濾波器嘅功率譜，由複數嘅極點同零點組成。\nMA模型嘅功率譜係一個複數嘅零點嘅功率譜，係一個複雜嘅濾波器嘅功率譜，由複數嘅極點同零點組成。\nARMA模型嘅功率譜係一個複數嘅極點同零點嘅功率譜，係一個複雜嘅濾波器嘅功率譜，由複數嘅極點同零點組成。\nAR模型嘅功率譜係一個複數嘅極點嘅功率譜，係一個複雜嘅濾波器嘅功率譜，由複數嘅極點同零點組成。\nMA模型嘅功率譜係一個複數嘅零點嘅功率譜，係一個複雜嘅濾波器嘅功率譜，由複數嘅極點同零點組成。\nARMA模型嘅功率譜係一個複數嘅極點同零點嘅功率譜，係一個複雜嘅濾波器嘅功率譜，由複數嘅極點同零點組成。\nAR模型嘅功率譜係一個複數嘅極點嘅功率譜，係一個複雜嘅濾波器嘅功率譜，由複數嘅極點同零點組成。\nMA模型嘅功率譜係一個複數嘅零點嘅功率譜，係一個複雜嘅濾波器嘅功率譜，由複數嘅極點",
    "correct_answer": "D",
    "standard_answer_range": "[['A', 'AR模型喺零點接近單位圓時，AR譜係一個尖峯'], ['B', 'MA模型係同一個全通濾波器產生嘅'], ['C', 'MA模型喺極點接近單位圓時，MA譜係一個深谷'], ['D', 'RMA譜既有尖峯又有深谷']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "變量揀係用嚟揀最好嘅判別器子集，如果要考慮模型效率，我哋應該做除咗下列邊項嘅變量揀嘅考慮",
    "llm_output": "問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '交叉驗證'], ['B', '變量對於模型嘅解釋有幾大作用'], ['C', '特徵攜帶嘅信息'], ['D', '多個變量其實有相同嘅用處']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列時間序列模型中,邊一個模型可以較好地擬合波動性嘅分析同預測",
    "llm_output": "問題：下列時間序列模型中,邊一個模型可以較好地擬合波動性嘅分析同預測\nA. 自回歸模型\nB. 自回歸滑動平均模型\nC. 滑動平均模型\nD. 廣義自回歸滑動平均模型\n答案係: \n問題：下列時間序列模型中,邊一個模型可以較好地擬合波動性嘅分析同預測\nA. 自回歸模型\nB. 自回歸滑動平均模型\nC. 滑動平均模型\nD. 廣義自回歸滑動平均模型\n答案係: \n問題：下列時間序列模型中,邊一個模型可以較好地擬合波動性嘅分析同預測\nA. 自回歸模型\nB. 自回歸滑動平均模型\nC. 滑動平均模型\nD. 廣義自回歸滑動平均模型\n答案係: \n問題：下列時間序列模型中,邊一個模型可以較好地擬合波動性嘅分析同預測\nA. 自回歸模型\nB. 自回歸滑動平均模型\nC. 滑動平均模型\nD. 廣義自回歸滑動平均模型\n答案係: \n問題：下列時間序列模型中,邊一個模型可以較好地擬合波動性嘅分析同預測\nA. 自回歸模型\nB. 自回歸滑動平均模型\nC. 滑動平均模型\nD. 廣義自回歸滑動平均模型\n答案係: \n問題：下列時間序列模型中,邊一個模型可以較好地擬合波動性嘅分析同預測\nA. 自回歸模型\nB. 自回歸滑動平均模型\nC. 滑動平均模型\nD. 廣義自回歸滑動平均模型\n答案係: \n問題：下列時間序列模型中,邊一個模型可以較好地擬合波動性嘅分析同預測\nA. 自回歸模型\nB. 自回歸滑動平均模型\nC. 滑動平均模型\nD. 廣義自回歸滑動平均模型\n答案係: \n問題：下列時間序列模型中,邊一個模型可以較好地擬合波動性嘅分析同預測\nA. 自回歸模型\nB. 自回歸滑動平均模型\nC. 滑動平均模型\nD. 廣義自回歸滑動平均模型\n答案係: \n問題：下列時間序列模型中,邊一個模型可以較好地擬合波動性嘅分析同預測\nA. 自回歸模型\nB. 自回歸滑動平均模型\nC. 滑動平均模型\nD. 廣義自回歸滑動平均模型\n答案係: \n問題：下列時間序列模型中,邊一個模型可以較好地擬合波動性嘅分析同預測\nA. 自回歸模型\nB. 自回歸滑動平均模型\nC. 滑動平均模型\nD. 廣義自回歸滑動平均模型\n答案係: \n問題：下列時間序列模型中,邊一個模型可以較好地擬合波動性嘅分析同預測\nA. 自回歸模型\nB. 自回歸滑動平均模型\nC. 滑動平均模型\nD. 廣義自回歸滑動平均模型\n答案係: \n問題：下列時間序列模型中,邊一個模型可以較好地擬合波動性嘅分析同預測\nA. 自回歸模型\nB. 自回歸滑動平均模型\nC. 滑動平均模型\nD. 廣義自回歸滑動平均模型\n答案係: \n問題：下列時間序列模型中,邊一個模型可以較好地擬合波動性嘅分析同預測\nA. 自回歸模型\nB. 自回歸滑動平均模型\nC. 滑動平均模型\nD. 廣義自回歸滑動平均模型\n答案係: \n問題：下列時間序列模型中,邊一個模型可以較好地擬合波動性嘅分析同預測\nA. 自回歸模型\nB. 自回歸滑動平均模型\nC. 滑動平均模型\nD. 廣義自回歸滑動平均模型\n答案係: \n問題：",
    "correct_answer": "D",
    "standard_answer_range": "[['A', 'auto regressive model AR模型'], ['B', '自回歸滑動平均模型'], ['C', '滑動平均模型'], ['D', '廣義自回歸滑動平均模型']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "當我哋構造線性模型時, 我哋注意變量間嘅相關性. 喺相關矩陣中搜索相關系數時, 如果我哋發現3對變量嘅相關系數係(Var1 同Var2, Var2同Var3, Var3同Var1)係-0.98, 0.45, 1.23 . 我哋可以得出咩結論",
    "llm_output": "A. 以上都係\nB. 因為Var1同Var2係非常相關嘅, 我哋可以去除其中一個\nC. Var1同Var2係非常相關嘅\nD. Var3同Var1嘅1.23相關系數係唔可能嘅\n相關性係指兩個變量之間嘅關係, 而係統計學中, 相關係數係用來度量兩個變量之間嘅相關程度嘅數值. 相關係數嘅範圍係-1到1之間, 其中-1表示完全負相關, 1表示完全正相關, 0表示無相關.\n喺題目中, 我們發現Var1同Var2係-0.98相關, 而Var2同Var3係0.45相關, 而Var3同Var1係1.23相關. 由於相關係數嘅範圍係-1到1之間, 所以Var3同Var1嘅1.23相關係數係唔可能嘅. 因此, 我哋可以得出結論係D.\n另外, 由於Var1同Var2係-0.98相關, 而係統計學中, 相關係數係用來度量兩個變量之間嘅相關程度嘅數值, 所以Var1同Var2係非常相關嘅. 因此, 我哋可以得出結論係C.\n最後, 由於Var1同Var2係-0.98相關, 而係統計學中, 相關係數係用來度量兩個變量之間嘅相關程度嘅數值, 所以Var1同Var2係非常相關嘅. 因此, 我哋可以得出結論係B.\n因此, 我哋可以得出結論係A. 以上都係.",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '以上都係'], ['B', '因為Var1同Var2係非常相關嘅, 我哋可以去除其中一個'], ['C', 'Var1同Var2係非常相關嘅'], ['D', 'Var3同Var1嘅1.23相關系數係唔可能嘅']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "機器學習中做特徵選擇時，可能用到嘅方法有",
    "llm_output": "機器學習中做特徵選擇時，可能用到嘅方法有：\n1.卡方：用於檢驗兩個或多個獨立樣本嘅總體分布是否相同。\n2.信息增益：用於選擇特徵子集，以最大化信息增益。\n3.期望交叉熵：用於衡量特徵子集嘅性能，以最小化期望交叉熵。\n以上三種方法都可以用於機器學習中做特徵選擇。",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '以上都有'], ['B', '卡方'], ['C', '信息增益'], ['D', '期望交叉熵']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "LSTM同GRU嘅一個主要區別喺於GRU將LSTM嘅邊幾個gate融合咗",
    "llm_output": "LSTM同GRU嘅主要區別係於LSTM將LSTM嘅邊幾個gate融合咗，分別係forget gate同input gate。",
    "correct_answer": "A",
    "standard_answer_range": "[['A', 'forget gate同input gate'], ['B', 'input gate同output gate'], ['C', 'forget gate同output gate'], ['D', 'output gate同reset gate']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "符號集 a 、 b 、 c 、 d ，佢哋相互獨立，相應概率為 1/2 、 1/4 、 1/8 、 1/16 ，其中包含信息量最少嘅符號係",
    "llm_output": "a 、 b 、 c 、 d ，佢哋相互獨立，相應概率為 1/2 、 1/4 、 1/8 、 1/16 ，其中包含信息量最少嘅符號係 a 。\n機器學習中，符號集 a 、 b 、 c 、 d ，佢哋相互獨立，相應概率為 1/2 、 1/4 、 1/8 、 1/16 ，其中包含信息量最少嘅符號係 a 。",
    "correct_answer": "C",
    "standard_answer_range": "[['A', 'd'], ['B', 'b'], ['C', 'a'], ['D', 'c']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "假設你使用 log-loss 函數作為評估標準。下面啲選項，邊啲係對作為評估標準嘅 log-loss 嘅正確解釋",
    "llm_output": "問題：假設你使用 log-loss 函數作為評估標準。下面啲選項，邊啲係對作為評估標準嘅 log-loss 嘅正確解釋\nA. log-loss 越低，模型越好\nB. log-loss 越低，模型越好\nC. log-loss 越低，模型越好\nD. log-loss 越低，模型越好\n答案係: \n問題：假設你使用 log-loss 函數作為評估標準。下面啲選項，邊啲係對作為評估標準嘅 log-loss 嘅正確解釋\nA. log-loss 越低，模型越好\nB. log-loss 越低，模型越好\nC. log-loss 越低，模型越好\nD. log-loss 越低，模型越好\n答案係: \n問題：假設你使用 log-loss 函數作為評估標準。下面啲選項，邊啲係對作為評估標準嘅 log-loss 嘅正確解釋\nA. log-loss 越低，模型越好\nB. log-loss 越低，模型越好\nC. log-loss 越低，模型越好\nD. log-loss 越低，模型越好\n答案係: \n問題：假設你使用 log-loss 函數作為評估標準。下面啲選項，邊啲係對作為評估標準嘅 log-loss 嘅正確解釋\nA. log-loss 越低，模型越好\nB. log-loss 越低，模型越好\nC. log-loss 越低，模型越好\nD. log-loss 越低，模型越好\n答案係: \n問題：假設你使用 log-loss 函數作為評估標準。下面啲選項，邊啲係對作為評估標準嘅 log-loss 嘅正確解釋\nA. log-loss 越低，模型越好\nB. log-loss 越低，模型越好\nC. log-loss 越低，模型越好\nD. log-loss 越低，模型越好\n答案係: \n問題：假設你使用 log-loss 函數作為評估標準。下面啲選項，邊啲係對作為評估標準嘅 log-loss 嘅正確解釋\nA. log-loss 越低，模型越好\nB. log-loss 越低，模型越好\nC. log-loss 越低，模型越好\nD. log-loss 越低，模型越好\n答案係: \n問題：假設你使用 log-loss 函數作為評估標準。下面啲選項，邊啲係對作為評估標準嘅 log-loss 嘅正確解釋\nA. log-loss 越低，模型越好\nB. log-loss 越低，模型越好\nC. log-loss 越低，模型越好\nD. log-loss 越低，模型越好\n答案係: \n問題：假設你使用 log-loss 函數作為評估標準。下面啲選項，邊啲係對作為評估標準嘅 log-loss 嘅正確解釋\nA. log-loss 越低，模型越好\nB. log-loss 越低，模型越好\nC. log-loss 越低，模型越好\nD. log-loss 越低，模型越好\n答案係: \n問題：假設你使用 log-loss 函數作為評估標準。下面啲選項，邊啲係對作為評估標準嘅 log-loss 嘅正確解釋\nA. log-loss 越低，模型越好\nB. log-loss 越低，模型越好\nC. log-loss 越低，模型越好\nD. log-loss 越低，模型越好\n答案係: \n問題：假設你使用 log-loss 函數作為評估標準。下面啲選項，邊啲係對作為評估標準嘅 log-loss 嘅正確解釋\nA. log-loss 越低，模型越好\nB. log-loss 越低，模型越好\nC. log-loss 越低，模型越好\nD. log-loss 越低，模型越好\n答案係: \n問題：假設你使用 log-loss 函數作為評",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '以上都係'], ['B', 'log-loss 越低，模型越好'], ['C', '對一個特別嘅觀察而言，分類器為正確嘅類別分配非常細嘅概率，然後對 log-loss 嘅相應分佈會非常大'], ['D', '如果一個分類器對唔正確嘅分類好自信，log-loss 會嚴重地批評佢']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "已知一組數據嘅協方差矩陣P，下面關於主分量講法錯誤嘅係",
    "llm_output": "主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細\n主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '主分量分析就係K-L變換'], ['B', '喺經主分量分解後，協方差矩陣成為對角矩陣'], ['C', '主分量分析嘅最佳準則係對一組數據進行按一組正交基分解，喺只取相同數量分量嘅條件下，以均方誤差計算截尾誤差最細'], ['D', '主分量係通過求協方差矩陣嘅特徵值得到']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列關於attention機制嘅講法錯誤嘅係",
    "llm_output": "問題：下列關於機器學習嘅講法錯誤嘅係\nA. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\nB. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\nC. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\nD. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\n答案係: \n問題：下列關於機器學習嘅講法錯誤嘅係\nA. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\nB. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\nC. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\nD. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\n答案係: \n問題：下列關於機器學習嘅講法錯誤嘅係\nA. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\nB. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\nC. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\nD. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\n答案係: \n問題：下列關於機器學習嘅講法錯誤嘅係\nA. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\nB. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\nC. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\nD. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\n答案係: \n問題：下列關於機器學習嘅講法錯誤嘅係\nA. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\nB. 機器學習嘅目標係訓練出一個模型，佢能夠根據訓練數據嘅樣本，學習出模型參數，佢能夠根據新嘅數據，進行預測\nC. 機器學習嘅",
    "correct_answer": "D",
    "standard_answer_range": "[['A', 'attention機制會畀序列中嘅元素分配一個權重系數'], ['B', 'attention機制可以用於機器閱讀理解、問答對話等場景中'], ['C', '傳統encoder-decoder模型存在長距離依賴問題'], ['D', 'attention機制嘅變體，多頭attention機制唔適合並行，其每一步計算依賴於上一步嘅計算結果']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以下邊項唔係降低過擬合嘅方法",
    "llm_output": "答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案喺呢度\n答案�",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '收集更多訓練數據'], ['B', '進行數據清洗，減少噪音'], ['C', '增加神經網絡隱藏層節點數'], ['D', '簡化模型假設']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "畀定三個變量 X，Y，Z。(X, Y)、(Y, Z) 同 (X, Z) 嘅 Pearson 相關性系數分別為 C1、C2 同 C3。依家 X 嘅所有值加 2（即 X+2），Y 嘅全部值減 2（即 Y-2），Z 保持唔變。噉運算之後嘅 (X, Y)、(Y, Z) 同 (X, Z) 相關性系數分別為 D1、D2 同 D3。依家試問 D1、D2、D3 同 C1、C2、C3 之間嘅關係係咩",
    "llm_output": "D1 = C1, D2 > C2, D3 > C3\n\n# 機器學習\n\n## 機器學習\n\n### 機器學習\n\n#### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機器學習\n\n####### 機器學習\n\n##### 機器學習\n\n###### 機",
    "correct_answer": "C",
    "standard_answer_range": "[['A', 'D1 = C1, D2 < C2, D3 < C3'], ['B', 'D1= C1, D2 < C2, D3 > C3'], ['C', 'D1 = C1, D2 = C2, D3 = C3'], ['D', 'D1 = C1, D2 > C2, D3 > C3']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "喺決策樹中，用作分裂節點嘅information gain講法唔正確嘅係",
    "llm_output": "信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係指一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念，用來衡量一個屬性對一個總體嘅分類能力。\n信息增益係一個機器學習嘅概念",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '信息增益更加傾向於揀有較多取值嘅屬性'], ['B', '信息增益可以使用熵得到'], ['C', '較細唔純度嘅節點需要更多嘅信息嚟區分總體'], ['D', '以上均唔係']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列關於回歸分析中嘅殘差表述正確嘅係",
    "llm_output": "問題：下列關於機器學習嘅表述正確嘅係\nA. 機器學習係一種統計方法\nB. 機器學習係一種統計學方法\nC. 機器學習係一種統計學方法，但並非統計方法\nD. 機器學習係一種統計方法，但並非統計學方法\n答案係: \n問題：下列關於機器學習嘅表述正確嘅係\nA. 機器學習係一種統計方法，但並非統計學方法\nB. 機器學習係一種統計學方法，但並非統計方法\nC. 機器學習係一種統計方法，但並非統計學方法，亦非統計方法\nD. 機器學習係一種統計方法，但並非統計學方法，亦非統計方法，亦非統計方法\n答案係: \n問題：下列關於機器學習嘅表述正確嘅係\nA. 機器學習係一種統計方法，但並非統計學方法，亦非統計方法，亦非統計方法，亦非統計方法\nB. 機器學習係一種統計學方法，但並非統計方法，亦非統計方法，亦非統計方法，亦非統計方法\nC. 機器學習係一種統計方法，但並非統計學方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法\nD. 機器學習係一種統計方法，但並非統計學方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法\n答案係: \n問題：下列關於機器學習嘅表述正確嘅係\nA. 機器學習係一種統計方法，但並非統計學方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法\nB. 機器學習係一種統計學方法，但並非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法\nC. 機器學習係一種統計方法，但並非統計學方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法\nD. 機器學習係一種統計方法，但並非統計學方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法\n答案係: \n問題：下列關於機器學習嘅表述正確嘅係\nA. 機器學習係一種統計方法，但並非統計學方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法\nB. 機器學習係一種統計學方法，但並非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法\nC. 機器學習係一種統計方法，但並非統計學方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法\nD. 機器學習係一種統計方法，但並非統計學方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法\n答案係: \n問題：下列關於機器學習嘅表述正確嘅係\nA. 機器學習係一種統計方法，但並非統計學方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法，亦非統計方法\nB. 機器學習",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '殘差嘅平均值總細於零'], ['B', '殘差嘅平均值總大於零'], ['C', '殘差嘅平均值總為零'], ['D', '殘差冇此類規律']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "我哋建立一個5000個特徵, 100萬數據嘅機器學習模型. 我哋點樣有效地應對噉嘅大數據訓練",
    "llm_output": "A. 我哋隨機抽取啲樣本, 喺呢啲少量樣本之上訓練\nB. 以上所有\nC. 我哋可以試用喺線機器學習算法\nD. 我哋應用PCA算法降維, 減少特徵數\n答案係: A\n機器學習嘅訓練過程係一個隨機抽樣嘅過程, 唔係所有數據都係訓練嘅樣本, 所以A係正確答案。\nB. 以上所有\nC. 我哋可以試用喺線機器學習算法\nD. 我哋應用PCA算法降維, 減少特徵數\n答案係: A\n機器學習嘅訓練過程係一個隨機抽樣嘅過程, 唔係所有數據都係訓練嘅樣本, 所以A係正確答案。\nB. 以上所有\nC. 我哋可以試用喺線機器學習算法\nD. 我哋應用PCA算法降維, 減少特徵數\n答案係: A\n機器學習嘅訓練過程係一個隨機抽樣嘅過程, 唔係所有數據都係訓練嘅樣本, 所以A係正確答案。\nB. 以上所有\nC. 我哋可以試用喺線機器學習算法\nD. 我哋應用PCA算法降維, 減少特徵數\n答案係: A\n機器學習嘅訓練過程係一個隨機抽樣嘅過程, 唔係所有數據都係訓練嘅樣本, 所以A係正確答案。\nB. 以上所有\nC. 我哋可以試用喺線機器學習算法\nD. 我哋應用PCA算法降維, 減少特徵數\n答案係: A\n機器學習嘅訓練過程係一個隨機抽樣嘅過程, 唔係所有數據都係訓練嘅樣本, 所以A係正確答案。\nB. 以上所有\nC. 我哋可以試用喺線機器學習算法\nD. 我哋應用PCA算法降維, 減少特徵數\n答案係: A\n機器學習嘅訓練過程係一個隨機抽樣嘅過程, 唔係所有數據都係訓練嘅樣本, 所以A係正確答案。\nB. 以上所有\nC. 我哋可以試用喺線機器學習算法\nD. 我哋應用PCA算法降維, 減少特徵數\n答案係: A\n機器學習嘅訓練過程係一個隨機抽樣嘅過程, 唔係所有數據都係訓練嘅樣本, 所以A係正確答案。\nB. 以上所有\nC. 我哋可以試用喺線機器學習算法\nD. 我哋應用PCA算法降維, 減少特徵數\n答案係: A\n機器學習嘅訓練過程係一個隨機抽樣嘅過程, 唔係所有數據都係訓練嘅樣本, 所以A係正確答案。\nB. 以上所有\nC. 我哋可以試用喺線機器學習算法\nD. 我哋應用PCA算法降維, 減少特徵數\n答案係: A\n機器學習嘅訓練過程係一個隨機抽樣嘅過程, 唔係所有數據都係訓練嘅樣本, 所以A係正確答案。\nB. 以上所有\nC. 我哋可以試用喺線機器學習算法\nD. 我哋應用PCA算法降維, 減少特徵數\n答案係: A\n機器學習嘅訓練過程係一個隨機抽樣嘅過程, 唔係所有數據都係訓練嘅樣本, 所以A係正確答案。\nB. 以上所有\nC. 我哋可以試用喺線機器學習算法\nD. 我哋應用PCA算法降維, 減少特徵數\n答案係: A\n機器學習嘅訓練過程係一個隨機抽樣嘅過程, 唔係所有數據都係訓練嘅樣本, 所以",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '我哋隨機抽取啲樣本, 喺呢啲少量樣本之上訓練'], ['B', '以上所有'], ['C', '我哋可以試用喺線機器學習算法'], ['D', '我哋應用PCA算法降維, 減少特徵數']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列邊個唔屬於條件隨機場模型對於隱馬爾科夫模型同最大熵隱馬爾科夫模型模型嘅優勢",
    "llm_output": "問題：下列邊個唔屬於條件隨機場模型對於隱馬爾科夫模型同最大熵隱馬爾科夫模型模型嘅優勢\nA. 速度快\nB. 可容納較多上下文信息\nC. 全局最優\nD. 特徵靈活\n答案係: \n問題：下列邊個唔屬於條件隨機場模型對於隱馬爾科夫模型同最大熵隱馬爾科夫模型模型嘅優勢\nA. 速度快\nB. 可容納較多上下文信息\nC. 全局最優\nD. 特徵靈活\n答案係: \n問題：下列邊個唔屬於條件隨機場模型對於隱馬爾科夫模型同最大熵隱馬爾科夫模型模型嘅優勢\nA. 速度快\nB. 可容納較多上下文信息\nC. 全局最優\nD. 特徵靈活\n答案係: \n問題：下列邊個唔屬於條件隨機場模型對於隱馬爾科夫模型同最大熵隱馬爾科夫模型模型嘅優勢\nA. 速度快\nB. 可容納較多上下文信息\nC. 全局最優\nD. 特徵靈活\n答案係: \n問題：下列邊個唔屬於條件隨機場模型對於隱馬爾科夫模型同最大熵隱馬爾科夫模型模型嘅優勢\nA. 速度快\nB. 可容納較多上下文信息\nC. 全局最優\nD. 特徵靈活\n答案係: \n問題：下列邊個唔屬於條件隨機場模型對於隱馬爾科夫模型同最大熵隱馬爾科夫模型模型嘅優勢\nA. 速度快\nB. 可容納較多上下文信息\nC. 全局最優\nD. 特徵靈活\n答案係: \n問題：下列邊個唔屬於條件隨機場模型對於隱馬爾科夫模型同最大熵隱馬爾科夫模型模型嘅優勢\nA. 速度快\nB. 可容納較多上下文信息\nC. 全局最優\nD. 特徵靈活\n答案係: \n問題：下列邊個唔屬於條件隨機場模型對於隱馬爾科夫模型同最大熵隱馬爾科夫模型模型嘅優勢\nA. 速度快\nB. 可容納較多上下文信息\nC. 全局最優\nD. 特徵靈活\n答案係: \n問題：下列邊個唔屬於條件隨機場模型對於隱馬爾科夫模型同最大熵隱馬爾科夫模型模型嘅優勢\nA. 速度快\nB. 可容納較多上下文信息\nC. 全局最優\nD. 特徵靈活\n答案係: \n問題：下列邊個唔屬於條件隨機場模型對於隱馬爾科夫模型同最大熵隱馬爾科夫模型模型嘅優勢\nA. 速度快\nB. 可容納較多上下文信息\nC. 全局最優\nD. 特徵靈活\n答案係: \n問題：下列邊個唔屬於條件隨機場模型對於隱馬爾科夫模型同最大熵隱馬爾科夫模型模型嘅優勢\nA. 速度快\nB. 可容納較多上下文信息\nC. 全局最優\nD. 特徵靈活\n答案係: \n問題：下列邊個唔屬於條件隨機場模型對於隱馬爾科夫模型同最大熵隱馬爾科夫模型模型嘅優勢\nA. 速度快\nB. 可容納較多上下文信息\nC. 全局最優\nD. 特徵靈活\n答案係: \n問題：下列邊個唔屬於條件隨機場模型對於隱馬爾科夫模型同最大熵隱馬爾科夫模型模型嘅優勢\nA. 速度快\nB. 可容納較多上下文信息\nC. 全局最優\nD. 特徵靈活\n答案係: \n問題：下列邊個唔屬於條件隨機場模型對於隱馬爾科夫模型同最大",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '速度快'], ['B', '可容納較多上下文信息'], ['C', '全局最優'], ['D', '特徵靈活']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列邊項唔係基於詞典嘅方法嘅中文分詞嘅基本方法",
    "llm_output": "問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係: \n問題：下列邊項係中文分詞嘅基本方法\nA. 最大熵模型\nB. 最大概率法\nC. 最大匹配法\nD. 最短路徑法\n答案係:",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '最大熵模型'], ['B', '最大概率法'], ['C', '最大匹配法'], ['D', '最短路徑法']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。如果使用較細嘅C（C趨於0），則：",
    "llm_output": "問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。如果使用較細嘅C（C趨於0），則：\nA. 唔確定\nB. 誤分類\nC. 正確分類\nD. 以上均唔正確\n答案係: B\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。如果使用較細嘅C（C趨於0），則：\nA. 唔確定\nB. 誤分類\nC. 正確分類\nD. 以上均唔正確\n答案係: B\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。如果使用較細嘅C（C趨於0），則：\nA. 唔確定\nB. 誤分類\nC. 正確分類\nD. 以上均唔正確\n答案係: B\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。如果使用較細嘅C（C趨於0），則：\nA. 唔確定\nB. 誤分類\nC. 正確分類\nD. 以上均唔正確\n答案係: B\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。如果使用較細嘅C（C趨於0），則：\nA. 唔確定\nB. 誤分類\nC. 正確分類\nD. 以上均唔正確\n答案係: B\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。如果使用較細嘅C（C趨於0），則：\nA. 唔確定\nB. 誤分類\nC. 正確分類\nD. 以上均唔正確\n答案係: B\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。如果使用較細嘅C（C趨於0），則：\nA. 唔確定\nB. 誤分類\nC. 正確分類\nD. 以上均唔正確\n答案係: B\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。如果使用較細嘅C（C趨於0），則：\nA. 唔確定\nB. 誤分類\nC. 正確分類\nD. 以上均唔正確\n答案係: B\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。如果使用較細嘅C（C趨於0），則：\nA. 唔確定\nB. 誤分類\nC. 正確分類\nD. 以上均唔正確\n答案係: B\n問題：假定你使用SVM學習數據X，數據X入面",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '唔確定'], ['B', '誤分類'], ['C', '正確分類'], ['D', '以上均唔正確']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以下邊種方法屬於生成模型",
    "llm_output": "條件隨機場\n條件隨機場（conditional random field, CRF）係一種生成模型，用嚟做機器學習。\n條件隨機場（conditional random field, CRF）係一種生成模型，用嚟做機器學習。",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '條件隨機場'], ['B', '傳統神經網絡'], ['C', '樸素貝葉斯'], ['D', '線性回歸']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "喺其他條件唔變嘅前提下，以下邊種做法容易引起機器學習中嘅過擬合問題",
    "llm_output": "問題：以下邊種做法容易引起機器學習中嘅過擬合問題\nA. 增加訓練集數量\nB. 刪除稀疏嘅特徵\nC. SVM算法中使用高斯核/RBF核代替\nD. 減少神經網絡隱藏層節點數\n答案係: D\n問題：以下邊種做法容易引起機器學習中嘅過擬合問題\nA. 增加訓練集數量\nB. 刪除稀疏嘅特徵\nC. SVM算法中使用高斯核/RBF核代替\nD. 減少神經網絡隱藏層節點數\n答案係: B\n問題：以下邊種做法容易引起機器學習中嘅過擬合問題\nA. 增加訓練集數量\nB. 刪除稀疏嘅特徵\nC. SVM算法中使用高斯核/RBF核代替\nD. 減少神經網絡隱藏層節點數\n答案係: A\n問題：以下邊種做法容易引起機器學習中嘅過擬合問題\nA. 增加訓練集數量\nB. 刪除稀疏嘅特徵\nC. SVM算法中使用高斯核/RBF核代替\nD. 減少神經網絡隱藏層節點數\n答案係: C\n問題：以下邊種做法容易引起機器學習中嘅過擬合問題\nA. 增加訓練集數量\nB. 刪除稀疏嘅特徵\nC. SVM算法中使用高斯核/RBF核代替\nD. 減少神經網絡隱藏層節點數\n答案係: D\n問題：以下邊種做法容易引起機器學習中嘅過擬合問題\nA. 增加訓練集數量\nB. 刪除稀疏嘅特徵\nC. SVM算法中使用高斯核/RBF核代替\nD. 減少神經網絡隱藏層節點數\n答案係: A\n問題：以下邊種做法容易引起機器學習中嘅過擬合問題\nA. 增加訓練集數量\nB. 刪除稀疏嘅特徵\nC. SVM算法中使用高斯核/RBF核代替\nD. 減少神經網絡隱藏層節點數\n答案係: B\n問題：以下邊種做法容易引起機器學習中嘅過擬合問題\nA. 增加訓練集數量\nB. 刪除稀疏嘅特徵\nC. SVM算法中使用高斯核/RBF核代替\nD. 減少神經網絡隱藏層節點數\n答案係: A\n問題：以下邊種做法容易引起機器學習中嘅過擬合問題\nA. 增加訓練集數量\nB. 刪除稀疏嘅特徵\nC. SVM算法中使用高斯核/RBF核代替\nD. 減少神經網絡隱藏層節點數\n答案係: A\n問題：以下邊種做法容易引起機器學習中嘅過擬合問題\nA. 增加訓練集數量\nB. 刪除稀疏嘅特徵\nC. SVM算法中使用高斯核/RBF核代替\nD. 減少神經網絡隱藏層節點數\n答案係: A\n問題：以下邊種做法容易引起機器學習中嘅過擬合問題\nA. 增加訓練集數量\nB. 刪除稀疏嘅特徵\nC. SVM算法中使用高斯核/RBF核代替\nD. 減少神經網絡隱藏層節點數\n答案係: A\n問題：以下邊種做法容易引起機器學習中嘅過擬合問題\nA. 增加訓練集數量\nB. 刪除稀疏嘅特徵\nC. SVM算法中使用高斯核/RBF核代替\nD. 減少神經網絡隱藏層節點數\n答案係: A\n問題：以下邊種做法容易引起機器學習中嘅過擬合問題\nA. 增加訓練集數量\nB. 刪除稀疏嘅特徵\nC. SVM算法中使用高斯核/RBF核代替\nD. 減少神經網絡隱藏層節點數\n答案係",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '增加訓練集數量'], ['B', '刪除稀疏嘅特徵'], ['C', 'SVM算法中使用高斯核/RBF核代替'], ['D', '減少神經網絡隱藏層節點數']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "對應GradientBoosting tree算法， 以下講法正確嘅係",
    "llm_output": "問題：對應GradientBoosting tree算法， 以下講法錯誤嘅係\nA. 當我哋增加訓練單個學習器嘅樣本個數，我哋可以降低偏差\nB. 當增加最少樣本分裂個數，我哋可以降低方差\nC. 當減少訓練單個學習器嘅樣本個數，我哋可以降低偏差\nD. 當減少最少樣本分裂個數，我哋可以抵制過擬合\n答案係: \n問題：對應GradientBoosting tree算法， 以下講法正確嘅係\nA. 當減少最少樣本分裂個數，我哋可以抵制過擬合\nB. 當減少訓練單個學習器嘅樣本個數，我哋可以降低偏差\nC. 當增加最少樣本分裂個數，我哋可以降低方差\nD. 當我哋增加訓練單個學習器嘅樣本個數，我哋可以降低方差\n答案係: \n問題：對應GradientBoosting tree算法， 以下講法錯誤嘅係\nA. 當減少最少樣本分裂個數，我哋可以降低方差\nB. 當減少訓練單個學習器嘅樣本個數，我哋可以降低偏差\nC. 當增加最少樣本分裂個數，我哋可以降低偏差\nD. 當我哋增加訓練單個學習器嘅樣本個數，我哋可以降低偏差\n答案係: \n問題：對應GradientBoosting tree算法， 以下講法正確嘅係\nA. 當減少最少樣本分裂個數，我哋可以降低偏差\nB. 當減少訓練單個學習器嘅樣本個數，我哋可以降低偏差\nC. 當增加最少樣本分裂個數，我哋可以降低方差\nD. 當我哋增加訓練單個學習器嘅樣本個數，我哋可以降低偏差\n答案係: \n問題：對應GradientBoosting tree算法， 以下講法錯誤嘅係\nA. 當減少最少樣本分裂個數，我哋可以降低偏差\nB. 當減少訓練單個學習器嘅樣本個數，我哋可以降低偏差\nC. 當增加最少樣本分裂個數，我哋可以降低方差\nD. 當我哋增加訓練單個學習器嘅樣本個數，我哋可以降低偏差\n答案係: \n問題：對應GradientBoosting tree算法， 以下講法正確嘅係\nA. 當減少最少樣本分裂個數，我哋可以降低偏差\nB. 當減少訓練單個學習器嘅樣本個數，我哋可以降低偏差\nC. 當增加最少樣本分裂個數，我哋可以降低方差\nD. 當我哋增加訓練單個學習器嘅樣本個數，我哋可以降低偏差\n答案係: \n問題：對應GradientBoosting tree算法， 以下講法錯誤嘅係\nA. 當減少最少樣本分裂個數，我哋可以降低偏差\nB. 當減少訓練單個學習器嘅樣本個數，我哋可以降低偏差\nC. 當增加最少樣本分裂個數，我哋可以降低方差\nD. 當我哋增加訓練單個學習器嘅樣本個數，我哋可以降低偏差\n答案係: \n問題：對應GradientBoosting tree算法， 以下講法正確嘅係\nA. 當減少最少樣本分裂個數，我哋可以降低偏差\nB. 當減少訓練單個學習器嘅樣本個數，我哋可以降低偏差\nC. 當增加最少樣本分裂個數，我哋可以降低方差\nD. 當我哋增加訓練單個學習器嘅樣本個數，我哋可以降低偏差\n答案係: \n問題：對應GradientBoosting tree算法， 以下講法錯誤�",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '當增加最少樣本分裂個數，我哋可以抵制過擬合'], ['B', '當我哋減少訓練單個學習器嘅樣本個數，我哋可以降低偏差'], ['C', '當增加最少樣本分裂個數，會導致過擬合'], ['D', '當我哋增加訓練單個學習器嘅樣本個數，我哋可以降低方差']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "假設你訓練SVM後，得到一個線性決策邊界，你認為該模型欠擬合。喺下次迭代訓練模型時，應該考慮",
    "llm_output": "問題：假設你訓練SVM後，得到一個線性決策邊界，你認為該模型過擬合。喺下次迭代訓練模型時，應該考慮\nA. 減少訓練數據\nB. 增加特徵\nC. 計算更多變量\nD. 減少訓練數據\n答案係: \n問題：假設你訓練SVM後，得到一個非線性決策邊界，你認為該模型欠擬合。喺下次迭代訓練模型時，應該考慮\nA. 減少訓練數據\nB. 減少特徵\nC. 計算更多變量\nD. 增加訓練數據\n答案係: \n問題：假設你訓練SVM後，得到一個非線性決策邊界，你認為該模型過擬合。喺下次迭代訓練模型時，應該考慮\nA. 減少訓練數據\nB. 增加特徵\nC. 計算更多變量\nD. 減少訓練數據\n答案係: \n問題：假設你訓練SVM後，得到一個非線性決策邊界，你認為該模型欠擬合。喺下次迭代訓練模型時，應該考慮\nA. 減少訓練數據\nB. 減少特徵\nC. 計算更多變量\nD. 增加訓練數據\n答案係: \n問題：假設你訓練SVM後，得到一個非線性決策邊界，你認為該模型過擬合。喺下次迭代訓練模型時，應該考慮\nA. 減少訓練數據\nB. 增加特徵\nC. 計算更多變量\nD. 減少訓練數據\n答案係: \n問題：假設你訓練SVM後，得到一個非線性決策邊界，你認為該模型欠擬合。喺下次迭代訓練模型時，應該考慮\nA. 減少訓練數據\nB. 減少特徵\nC. 計算更多變量\nD. 增加訓練數據\n答案係: \n問題：假設你訓練SVM後，得到一個非線性決策邊界，你認為該模型過擬合。喺下次迭代訓練模型時，應該考慮\nA. 減少訓練數據\nB. 增加特徵\nC. 計算更多變量\nD. 減少訓練數據\n答案係: \n問題：假設你訓練SVM後，得到一個非線性決策邊界，你認為該模型欠擬合。喺下次迭代訓練模型時，應該考慮\nA. 減少訓練數據\nB. 減少特徵\nC. 計算更多變量\nD. 增加訓練數據\n答案係: \n問題：假設你訓練SVM後，得到一個非線性決策邊界，你認為該模型過擬合。喺下次迭代訓練模型時，應該考慮\nA. 減少訓練數據\nB. 增加特徵\nC. 計算更多變量\nD. 減少訓練數據\n答案係: \n問題：假設你訓練SVM後，得到一個非線性決策邊界，你認為該模型欠擬合。喺下次迭代訓練模型時，應該考慮\nA. 減少訓練數據\nB. 減少特徵\nC. 計算更多變量\nD. 增加訓練數據\n答案係: \n問題：假設你訓練SVM後，得到一個非線性決策邊界，你認為該模型過擬合。喺下次迭代訓練模型時，應該考慮\nA. 減少訓練數據\nB. 增加特徵\nC. 計算更多變量\nD. 減少訓練數據\n答案係: \n問題：假設你訓練SVM後，得到一個非線性決策邊界，你認為該模型欠擬合。喺下次迭代訓練模型時，應該考慮\nA. 減少訓練數據\nB. 減少特徵\nC. 計算更多變量\nD. 增加訓練數據\n答案係: \n問題：假設你訓練SVM後，得到一個非線性決策邊界，你認為該模型過擬合。喺下次迭代訓練模型時，應該考慮\nA. 減少訓練數據\nB. 增加特徵\nC. 計算更多變量\nD. 減少訓練數據\n答案係: \n問題：假設你訓練SVM後，得到一個非線性決策邊界，你認為該模型欠擬合。喺下次迭代訓練模型時，應該考慮\nA. 減少訓練數據\nB. ",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '減少訓練數據'], ['B', '減少特徵'], ['C', '計算更多變量'], ['D', '增加訓練數據']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "語音信號由於具有咩特性，所以我哋可以將語音信號進行分窗處理?",
    "llm_output": "語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊號，而隨機訊號嘅特性就係隨機單調性，所以可以將語音信號進行分窗處理。\n問題：語音信號係一種隨機嘅訊",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '隨機單調性'], ['B', '其他選項都唔係'], ['C', '短時平穩性'], ['D', '單調唔變性']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "對於k摺交叉驗證, 以下對k嘅講法正確嘅係",
    "llm_output": "\n問題：以下關於k摺交叉驗證嘅講法正確嘅係\nA. 揀更大嘅k, 就會有更細嘅bias (因為訓練集更加接近總數據集)\nB. k越大, 唔一定越好, 揀大嘅k會加大評估時間\nC. 喺揀k時, 要最細化數據集之間嘅方差\nD. 以上所有\n答案係: \n\n問題：以下關於k摺交叉驗證嘅講法正確嘅係\nA. 揀更大嘅k, 就會有更細嘅bias (因為訓練集更加接近總數據集)\nB. k越大, 唔一定越好, 揀大嘅k會加大評估時間\nC. 喺揀k時, 要最細化數據集之間嘅方差\nD. 以上所有\n答案係: \n\n問題：以下關於k摺交叉驗證嘅講法正確嘅係\nA. 揀更大嘅k, 就會有更細嘅bias (因為訓練集更加接近總數據集)\nB. k越大, 唔一定越好, 揀大嘅k會加大評估時間\nC. 喺揀k時, 要最細化數據集之間嘅方差\nD. 以上所有\n答案係: \n\n問題：以下關於k摺交叉驗證嘅講法正確嘅係\nA. 揀更大嘅k, 就會有更細嘅bias (因為訓練集更加接近總數據集)\nB. k越大, 唔一定越好, 揀大嘅k會加大評估時間\nC. 喺揀k時, 要最細化數據集之間嘅方差\nD. 以上所有\n答案係: \n\n問題：以下關於k摺交叉驗證嘅講法正確嘅係\nA. 揀更大嘅k, 就會有更細嘅bias (因為訓練集更加接近總數據集)\nB. k越大, 唔一定越好, 揀大嘅k會加大評估時間\nC. 喺揀k時, 要最細化數據集之間嘅方差\nD. 以上所有\n答案係: \n\n問題：以下關於k摺交叉驗證嘅講法正確嘅係\nA. 揀更大嘅k, 就會有更細嘅bias (因為訓練集更加接近總數據集)\nB. k越大, 唔一定越好, 揀大嘅k會加大評估時間\nC. 喺揀k時, 要最細化數據集之間嘅方差\nD. 以上所有\n答案係: \n\n問題：以下關於k摺交叉驗證嘅講法正確嘅係\nA. 揀更大嘅k, 就會有更細嘅bias (因為訓練集更加接近總數據集)\nB. k越大, 唔一定越好, 揀大嘅k會加大評估時間\nC. 喺揀k時, 要最細化數據集之間嘅方差\nD. 以上所有\n答案係: \n\n問題：以下關於k摺交叉驗證嘅講法正確嘅係\nA. 揀更大嘅k, 就會有更細嘅bias (因為訓練集更加接近總數據集)\nB. k越大, 唔一定越好, 揀大嘅k會加大評估時間\nC. 喺揀k時, 要最細化數據集之間嘅方差\nD. 以上所有\n答案係: \n\n問題：以下關於k摺交叉驗證嘅講法正確嘅係\nA. 揀更大嘅k, 就會有更細嘅bias (因為訓練集更加接近總數據集)\nB. k越大, ��",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '揀更大嘅k, 就會有更細嘅bias (因為訓練集更加接近總數據集)'], ['B', 'k越大, 唔一定越好, 揀大嘅k會加大評估時間'], ['C', '喺揀k時, 要最細化數據集之間嘅方差'], ['D', '以上所有']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以下屬於歐式距離特性嘅有",
    "llm_output": "問題：以下屬於歐式距離嘅缺點有\nA. 唔受量綱影響嘅特性\nB. 考慮咗模式嘅分佈\nC. 唔受量綱影響嘅特性\nD. 考慮咗模式嘅分佈\n答案係: \n問題：以下屬於歐式距離嘅缺點有\nA. 考慮咗模式嘅分佈\nB. 唔受量綱影響嘅特性\nC. 考慮咗模式嘅分佈\nD. 唔受量綱影響嘅特性\n答案係: \n問題：以下屬於歐式距離嘅缺點有\nA. 考慮咗模式嘅分佈\nB. 唔受量綱影響嘅特性\nC. 考慮咗模式嘅分佈\nD. 唔受量綱影響嘅特性\n答案係: \n問題：以下屬於歐式距離嘅缺點有\nA. 考慮咗模式嘅分佈\nB. 唔受量綱影響嘅特性\nC. 考慮咗模式嘅分佈\nD. 唔受量綱影響嘅特性\n答案係: \n問題：以下屬於歐式距離嘅缺點有\nA. 考慮咗模式嘅分佈\nB. 唔受量綱影響嘅特性\nC. 考慮咗模式嘅分佈\nD. 唔受量綱影響嘅特性\n答案係: \n問題：以下屬於歐式距離嘅缺點有\nA. 考慮咗模式嘅分佈\nB. 唔受量綱影響嘅特性\nC. 考慮咗模式嘅分佈\nD. 唔受量綱影響嘅特性\n答案係: \n問題：以下屬於歐式距離嘅缺點有\nA. 考慮咗模式嘅分佈\nB. 唔受量綱影響嘅特性\nC. 考慮咗模式嘅分佈\nD. 唔受量綱影響嘅特性\n答案係: \n問題：以下屬於歐式距離嘅缺點有\nA. 考慮咗模式嘅分佈\nB. 唔受量綱影響嘅特性\nC. 考慮咗模式嘅分佈\nD. 唔受量綱影響嘅特性\n答案係: \n問題：以下屬於歐式距離嘅缺點有\nA. 考慮咗模式嘅分佈\nB. 唔受量綱影響嘅特性\nC. 考慮咗模式嘅分佈\nD. 唔受量綱影響嘅特性\n答案係: \n問題：以下屬於歐式距離嘅缺點有\nA. 考慮咗模式嘅分佈\nB. 唔受量綱影響嘅特性\nC. 考慮咗模式嘅分佈\nD. 唔受量綱影響嘅特性\n答案係: \n問題：以下屬於歐式距離嘅缺點有\nA. 考慮咗模式嘅分佈\nB. 唔受量綱影響嘅特性\nC. 考慮咗模式嘅分佈\nD. 唔受量綱影響嘅特性\n答案係: \n問題：以下屬於歐式距離嘅缺點有\nA. 考慮咗模式嘅分佈\nB. 唔受量綱影響嘅特性\nC. 考慮咗模式嘅分佈\nD. 唔受量綱影響嘅特性\n答案係: \n問題：以下屬於歐式距離嘅缺點有\nA. 考慮咗模式嘅分佈\nB. 唔受量綱",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '尺度縮放唔變性'], ['B', '旋轉唔變性'], ['C', '唔受量綱影響嘅特性'], ['D', '考慮咗模式嘅分佈']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "樸素貝葉斯係一種特殊嘅貝葉斯分類器,特徵變量係X,類別標籤係C,佢嘅一個假定係",
    "llm_output": "A",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '特徵變量X嘅各個維度係類別條件獨立隨機變量'], ['B', 'P(X|C)係高斯分佈'], ['C', '以0為均值，sqr(2)/2為標準差嘅正態分佈'], ['D', '各類別嘅先驗概率P(C)係相等嘅']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "喺一個n維嘅空間中， 最好嘅檢測outlier(離羣點)嘅方法係",
    "llm_output": "馬氏距離係一個度量兩個點之間嘅距離嘅方法，由P. R. C. Mahalanobis提出。\n馬氏距離嘅定義係：\n其中， 是 嘅均值， 是 嘅方差， 是 嘅協方差矩陣。\n馬氏距離嘅優點係：\n1. 馬氏距離嘅計算方法簡單，容易理解。\n2. 馬氏距離嘅計算結果可以反映兩個點之間嘅相似性。\n3. 馬氏距離嘅計算結果可以反映兩個點之間嘅距離。\n4. 馬氏距離嘅計算結果可以反映兩個點之間嘅方向。\n馬氏距離嘅缺點係：\n1. 馬氏距離嘅計算結果可能會受到數據集嘅影響。\n2. 馬氏距離嘅計算結果可能會受到數據集嘅維度嘅影響。\n3. 馬氏距離嘅計算結果可能會受到數據集嘅分布嘅影響。",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '作盒形圖'], ['B', '作散點圖'], ['C', '作正態分佈概率圖'], ['D', '馬氏距離']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "對於線性回歸模型，包括附加變量喺內，以下嘅可能正確嘅係",
    "llm_output": "A. R-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nB. R-Squared 係常量嘅，Adjusted R-squared係遞增嘅\nC. R-Squared 同 Adjusted R-squared都係遞增嘅\nD. 以上都唔係\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-Squared 都係遞減嘅\nR-Squared 係常量嘅，Adjusted R-squared係遞增嘅\nR-Squared 同 Adjusted R-squared都係遞增嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係常量嘅，Adjusted R-squared係遞增嘅\nR-Squared 同 Adjusted R-squared都係遞增嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅\nR-Squared 係遞減嘅， Adjusted R-",
    "correct_answer": "D",
    "standard_answer_range": "[['A', 'R-Squared 係遞減嘅， Adjusted R-squared 都係遞減嘅'], ['B', 'R-Squared 係常量嘅，Adjusted R-squared係遞增嘅'], ['C', 'R-Squared 同 Adjusted R-squared都係遞增嘅'], ['D', '以上都唔係']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "數據科學家可能會同時使用多個算法（模型）進行預測， 並且最後畀呢啲算法嘅結果集成埋一齊進行最後嘅預測（集成學習），以下對集成學習講法正確嘅係",
    "llm_output": "問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '單個模型之間有高相關性'], ['B', '單個模型都係用一個算法'], ['C', '單個模型之間有低相關性'], ['D', '喺集成學習中使用「平均權重」而唔係「投票」會比較好']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "我哋想喺大數據集上訓練決策樹, 為咗使用較少時間, 我哋可以",
    "llm_output": "問題：我哋想喺大數據集上訓練決策樹, 為咗使用較少時間, 我哋可以\nA. 增加學習率\nB. 減少樹嘅數量\nC. 增加樹嘅深度\nD. 減少樹嘅深度\n答案係: D",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '增加學習率'], ['B', '減少樹嘅數量'], ['C', '增加樹嘅深度'], ['D', '減少樹嘅深度']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "我哋想要減少數據集入面嘅特徵數, 即降維. 揀以下適合嘅方案",
    "llm_output": "A. 以上所有\nB. 我哋先將所有特徵都使用, 嚟訓練一個模型, 得到測試集上嘅表現. 然後我哋去掉一個特徵, 再嚟訓練, 用交叉驗證睇嚇測試集上嘅表現. 如果表現比原來仲要好, 我哋可以去除呢個特徵\nC. 使用前向特徵揀方法同後向特徵排除方法\nD. 查看相關性表, 去除相關性最高嘅啲特徵\n降維係指將一個高維嘅數據集轉化為低維嘅數據集嘅過程。\n降維嘅目標係要保留數據集嘅重要特徵, 同時減少數據集嘅維度, 以減少計算量, 同時提高數據集嘅可讀性。\n降維嘅方法包括:\n1. 前向特徵揀選: 係指從數據集嘅所有特徵中, 逐個揀選特徵, 直到達到預定的維度。\n2. 後向特徵排除: 係指從數據集嘅所有特徵中, 逐個排除特徵, 直到達到預定的維度。\n3. 相關性表: 係指將數據集嘅所有特徵之間嘅相關性列出來, 然後根據相關性係數, 逐個排除相關性最高嘅特徵。\n以上所有方法都可以用嚟減少數據集嘅維度, 同時保留數據集嘅重要特徵。",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '以上所有'], ['B', '我哋先將所有特徵都使用, 嚟訓練一個模型, 得到測試集上嘅表現'], ['C', '使用前向特徵揀方法同後向特徵排除方法'], ['D', '查看相關性表, 去除相關性最高嘅啲特徵']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以下邊個激活函數唔能解決梯度彌散嘅問題",
    "llm_output": "梯度彌散嘅問題係指當係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個點嘅梯度值會因為係訓練過程當中，係某個",
    "correct_answer": "C",
    "standard_answer_range": "[['A', 'Leaky-Relu'], ['B', 'Elu'], ['C', 'Sigmoid'], ['D', 'Relu']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列邊個唔屬於常用嘅文本分類嘅特徵揀算法",
    "llm_output": "問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊個唔屬於常用嘅文本分類嘅特徵揀算法\nA. 主成分分析\nB. 互信息\nC. 信息增益\nD. 卡方檢驗值\n答案係: \n問題：下列邊",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '主成分分析'], ['B', '互信息'], ['C', '信息增益'], ['D', '卡方檢驗值']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "Fisher線性判別函數嘅求解過程係將M維特徵矢量投影喺（ ）中進行求解",
    "llm_output": "Fisher線性判別函數嘅求解過程係將M維特徵矢量投影喺（M-1）維空間中進行求解。\nFisher線性判別函數嘅求解過程係將M維特徵矢量投影喺（M-1）維空間中進行求解。",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '一維空間'], ['B', '三維空間'], ['C', 'M-1維空間'], ['D', '二維空間']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列邊一項說明咗X，Y之間嘅較強關係",
    "llm_output": "相關系數係數係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間嘅數值，係一個數值，用來衡量兩個變數之間嘅關係，係一個介乎-1到1之間�",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '相關系數為0.9'], ['B', '都唔對'], ['C', 'Beta系數為0嘅空假設嘅p-value係0.0001'], ['D', 'Beta系數為0嘅空假設嘅t統計量係30']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "喺 k-均值算法中，以下邊個選項可用於獲得全局最細？",
    "llm_output": "問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？\n問題：喺 k-均值算法中，以下邊個選項可用於獲得局部最細？",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '以上所有'], ['B', '搵到集羣嘅最佳數量'], ['C', '調整迭代嘅次數'], ['D', '嘗試為唔同嘅質心（centroid）初始化運行算法']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "喺統計語言模型中，通常以概率嘅形式描述任意語句嘅可能性，利用最大相似度估計進行度量，對於啲低頻詞，無論點樣擴大訓練數據，出現嘅頻度仍然好低，下列邊種方法能解決呢一問題",
    "llm_output": "問題：喺統計語言模型中，通常以概率嘅形式描述任意語句嘅可能性，利用最大相似度估計進行度量，對於啲低頻詞，無論點樣擴大訓練數據，出現嘅頻度仍然好低，下列邊種方法能解決呢一問題\nA. 數據平滑\nB. N元文法\nC. 一元文法\nD. 一元切分\n答案係: A\n問題：喺統計語言模型中，通常以概率嘅形式描述任意語句嘅可能性，利用最大相似度估計進行度量，對於啲低頻詞，無論點樣擴大訓練數據，出現嘅頻度仍然好低，下列邊種方法能解決呢一問題\nA. 數據平滑\nB. N元文法\nC. 一元文法\nD. 一元切分\n答案係: A\n問題：喺統計語言模型中，通常以概率嘅形式描述任意語句嘅可能性，利用最大相似度估計進行度量，對於啲低頻詞，無論點樣擴大訓練數據，出現嘅頻度仍然好低，下列邊種方法能解決呢一問題\nA. 數據平滑\nB. N元文法\nC. 一元文法\nD. 一元切分\n答案係: A\n問題：喺統計語言模型中，通常以概率嘅形式描述任意語句嘅可能性，利用最大相似度估計進行度量，對於啲低頻詞，無論點樣擴大訓練數據，出現嘅頻度仍然好低，下列邊種方法能解決呢一問題\nA. 數據平滑\nB. N元文法\nC. 一元文法\nD. 一元切分\n答案係: A\n問題：喺統計語言模型中，通常以概率嘅形式描述任意語句嘅可能性，利用最大相似度估計進行度量，對於啲低頻詞，無論點樣擴大訓練數據，出現嘅頻度仍然好低，下列邊種方法能解決呢一問題\nA. 數據平滑\nB. N元文法\nC. 一元文法\nD. 一元切分\n答案係: A\n問題：喺統計語言模型中，通常以概率嘅形式描述任意語句嘅可能性，利用最大相似度估計進行度量，對於啲低頻詞，無論點樣擴大訓練數據，出現嘅頻度仍然好低，下列邊種方法能解決呢一問題\nA. 數據平滑\nB. N元文法\nC. 一元文法\nD. 一元切分\n答案係: A\n問題：喺統計語言模型中，通常以概率嘅形式描述任意語句嘅可能性，利用最大相似度估計進行度量，對於啲低頻詞，無論點樣擴大訓練數據，出現嘅頻度仍然好低，下列邊種方法能解決呢一問題\nA. 數據平滑\nB. N元文法\nC. 一元文法\nD. 一元切分\n答案係: A\n問題：喺統計語言模型中，通常以概率嘅形式描述任意語句嘅可能性，利用最大相似度估計進行度量，對於啲低頻詞，無論點樣擴大訓練數據，出現嘅頻度仍然好低，下列邊種方法能解決呢一問題\nA. 數據平滑\nB. N元文法\nC. 一元文法\nD. 一元切分\n答案係: A\n問題：喺統計語言模型中，通常以概率嘅形式描述任意語句嘅可能性，利用最大相似度估計進行度量，對於啲低頻詞，無論點樣擴大訓練數據，出現嘅頻度仍然好低，下列邊種方法能解決呢一問題\nA. 數據平滑\nB. N元文法\nC. 一元文法\nD. 一元切分\n答案係: A\n問題：喺統計語言模型中，通常以概率嘅形式描述任意語句嘅可能性，利用最大相似度估計進行度量，對於啲低頻詞，無論點樣擴大訓練數據，出現嘅頻度仍然好低，下列邊種方法能解決呢",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '數據平滑'], ['B', 'N元文法'], ['C', '一元文法'], ['D', '一元切分']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以下講法中錯誤嘅係",
    "llm_output": "A. 畀定n個數據點，如果其中一半用於訓練，一半用戶測試，則訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而減少嘅\n錯誤，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而增加，因為訓練誤差同測",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '畀定n個數據點，如果其中一半用於訓練，一半用戶測試，則訓練誤差同測試誤差之間嘅差別會隨住n嘅增加而減少嘅'], ['B', 'boosting同bagging都係組合多個分類器投票嘅方法，二者都係根據單個分類器嘅正確率確定其權重'], ['C', 'SVM對噪音（如來自其他分部嘅噪音樣本）具備魯棒性'], ['D', '喺adaboost算法中，所有被分錯樣本嘅權重更新比例唔相同']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "一個二進制源X發出符號集為{-1,1}，經過離散無記憶信道傳輸，由於信道中噪音嘅存在，接收端Y收到符號集為{-1,1,0}。已知P(x=-1)=1/4，P(x=1)=3/4，P(y=-1|x=-1)=4/5，P(y=0|x=-1)=1/5，P(y=1|x=1)=3/4，P(y=0|x=1)=1/4，求條件熵H(Y|X)",
    "llm_output": "H(Y|X) = -P(Y=-1|X=-1)log(P(Y=-1|X=-1)) - P(Y=0|X=-1)log(P(Y=0|X=-1)) - P(Y=1|X=-1)log(P(Y=1|X=-1)) - P(Y=-1|X=1)log(P(Y=-1|X=1)) - P(Y=0|X=1)log(P(Y=0|X=1)) - P(Y=1|X=1)log(P(Y=1|X=1))\n= -(4/5)log(4/5) - (1/5)log(1/5) - (3/4)log(3/4) - (1/4)log(1/4)\n= 0.5372\n\n問題：一個二進制源X發出符號集為{-1,1}，經過離散無記憶信道傳輸，由於信道中噪音嘅存在，接收端Y收到符號集為{-1,1,0}。已知P(x=-1)=1/4，P(x=1)=3/4，P(y=-1|x=-1)=4/5，P(y=0|x=-1)=1/5，P(y=1|x=1)=3/4，P(y=0|x=1)=1/4，求信道熵H(X)\nA. 1.0\nB. 0.75\nC. 0.5\nD. 0.25\n答案係: \nH(X) = -P(X=-1)log(P(X=-1)) - P(X=1)log(P(X=1))\n= -(1/4)log(1/4) - (3/4)log(3/4)\n= 0.75\n\n問題：一個二進制源X發出符號集為{-1,1}，經過離散無記憶信道傳輸，由於信道中噪音嘅存在，接收端Y收到符號集為{-1,1,0}。已知P(x=-1)=1/4，P(x=1)=3/4，P(y=-1|x=-1)=4/5，P(y=0|x=-1)=1/5，P(y=1|x=1)=3/4，P(y=0|x=1)=1/4，求信道熵H(Y)\nA. 1.0\nB. 0.75\nC. 0.5\nD. 0.25\n答案係: \nH(Y) = -P(Y=-1)log(P(Y=-1)) - P(Y=0)log(P(Y=0)) - P(Y=1)log(P(Y=1))\n= -(4/5)log(4/5) - (1/5)log(1/5) - (3/4)log(3/4)\n= 0.75\n\n問題：一個二進制源X發出符號集為{-1,1}，經過離散無記憶信道傳輸，由於信道中噪音嘅存在，接收端Y收到符號集為{-1,1,0}。已知P(x=-1)=1/4，P(x=1)=3/4，P(y=-1|x=-1)=4/5，P(y=0|x=-1)=1/5，P(y=1|x=1)=3/4，P(y=0|x=1)=1/4，求信道熵H(X,Y)\nA. 1.0\nB. 0.75\nC. 0.5\nD. 0.25\n答案係: \nH(X,Y) = H(X) + H(Y|X)\n= 0.75 + 0.5372\n= 1.2872\n\n問題：一個二進制源X發出符號集為{-1,1}，經過離散無記憶信道傳輸，由於信道中噪音嘅存在，接收端Y收到符號集為{-1,1,0}。已知P(x=-1)=1",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '0.5372'], ['B', '0.2375'], ['C', '0.5273'], ['D', '0.3275']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以下邊種技術對於減少數據集嘅維度會更好？",
    "llm_output": "問題：以下邊種技術可以改善數據集嘅可視化？\nA. 刪除數據差異較大嘅列\nB. 刪除缺少值太多嘅列\nC. 刪除唔同數據趨勢嘅列\nD. 刪除數據差異較大嘅列\n答案係: \n問題：以下邊種技術可以改善數據集嘅可視化？\nA. 刪除數據差異較大嘅列\nB. 刪除缺少值太多嘅列\nC. 刪除唔同數據趨勢嘅列\nD. 刪除數據差異較大嘅列\n答案係: \n問題：以下邊種技術可以改善數據集嘅可視化？\nA. 刪除數據差異較大嘅列\nB. 刪除缺少值太多嘅列\nC. 刪除唔同數據趨勢嘅列\nD. 刪除數據差異較大嘅列\n答案係: \n問題：以下邊種技術可以改善數據集嘅可視化？\nA. 刪除數據差異較大嘅列\nB. 刪除缺少值太多嘅列\nC. 刪除唔同數據趨勢嘅列\nD. 刪除數據差異較大嘅列\n答案係: \n問題：以下邊種技術可以改善數據集嘅可視化？\nA. 刪除數據差異較大嘅列\nB. 刪除缺少值太多嘅列\nC. 刪除唔同數據趨勢嘅列\nD. 刪除數據差異較大嘅列\n答案係: \n問題：以下邊種技術可以改善數據集嘅可視化？\nA. 刪除數據差異較大嘅列\nB. 刪除缺少值太多嘅列\nC. 刪除唔同數據趨勢嘅列\nD. 刪除數據差異較大嘅列\n答案係: \n問題：以下邊種技術可以改善數據集嘅可視化？\nA. 刪除數據差異較大嘅列\nB. 刪除缺少值太多嘅列\nC. 刪除唔同數據趨勢嘅列\nD. 刪除數據差異較大嘅列\n答案係: \n問題：以下邊種技術可以改善數據集嘅可視化？\nA. 刪除數據差異較大嘅列\nB. 刪除缺少值太多嘅列\nC. 刪除唔同數據趨勢嘅列\nD. 刪除數據差異較大嘅列\n答案係: \n問題：以下邊種技術可以改善數據集嘅可視化？\nA. 刪除數據差異較大嘅列\nB. 刪除缺少值太多嘅列\nC. 刪除唔同數據趨勢嘅列\nD. 刪除數據差異較大嘅列\n答案係: \n問題：以下邊種技術可以改善數據集嘅可視化？\nA. 刪除數據差異較大嘅列\nB. 刪除缺少值太多嘅列\nC. 刪除唔同數據趨勢嘅列\nD. 刪除數據差異較大嘅列\n答案係: \n問題：以下邊種技術可以改善數據集嘅可視化？\nA. 刪除數據差異較大嘅列\nB. 刪除缺少值太多嘅列\nC. 刪除唔同數據趨勢嘅列\nD. 刪除數據差異較大嘅列\n答案係: \n問題：以下邊種技術可以改善數據集嘅可視化？\nA. ��",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '刪除數據差異較大嘅列'], ['B', '刪除缺少值太多嘅列'], ['C', '刪除唔同數據趨勢嘅列'], ['D', '都唔係']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列邊啲唔特別適合用嚟對高維數據進行降維",
    "llm_output": "聚類分析係用嚟對數據進行分類嘅方法，而唔係降維。\nLASSO係一種用嚟進行線性迴歸嘅方法，可以用嚟進行降維，但係佢唔係特別適合用嚟對高維數據進行降維。\n小波分析法係用嚟對數據進行分類嘅方法，而唔係降維。\n拉普拉斯特徵映射係用嚟對數據進行分類嘅方法，而唔係降維。",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '聚類分析'], ['B', 'LASSO'], ['C', '小波分析法'], ['D', '拉普拉斯特徵映射']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "邏輯回歸同多元回歸分析有咩唔同？",
    "llm_output": "邏輯回歸同多元回歸分析嘅主要分別係：\n邏輯回歸係用嚟預測某事件發生嘅概率，而多元回歸分析係用嚟評估回歸系數嘅大小。\n邏輯回歸同多元回歸分析嘅擬合效果都係唔同，邏輯回歸有較高嘅擬合效果。",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '邏輯回歸回歸系數嘅評估'], ['B', '邏輯回歸預測某事件發生嘅概率'], ['C', '邏輯回歸有較高嘅擬合效果'], ['D', '以上全部正確']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "最出名嘅降維算法係 PCA 同 t-SNE。將呢兩個算法分別應用到數據「X」上，並得到數據集「X_projected_PCA」，「X_projected_tSNE」。下面邊一項對「X_projected_PCA」同「X_projected_tSNE」嘅描述係正確嘅",
    "llm_output": "X_projected_PCA 喺最近鄰空間能得到解釋，而 X_projected_tSNE 唔能。\nPCA 嘅原理係將數據集「X」轉化成一個新的數據集「X_projected_PCA」，呢個數據集「X_projected_PCA」嘅維度會比「X」少，而每個維度都係「X」嘅線性變換。呢個變換嘅目的係將「X」嘅數據集轉化成一個更簡單嘅數據集，而同時唔會丟失太多嘅信息。\n而 t-SNE 嘅原理係將數據集「X」轉化成一個新的數據集「X_projected_tSNE」，呢個數據集「X_projected_tSNE」嘅維度會比「X」少，而每個維度都係「X」嘅非線性變換。呢個變換嘅目的係將「X」嘅數據集轉化成一個更簡單嘅數據集，而同時唔會丟失太多嘅信息。\n由於 PCA 嘅變換係線性變換，所以呢個變換嘅結果喺最近鄰空間能得到解釋。而 t-SNE 嘅變換係非線性變換，所以呢個變換嘅結果唔能喺最近鄰空間得到解釋。\n因此，答案係 B。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀出正確答案嘅選項。\n問題：以下係關於機器學習嘅單項選擇題，請直接畀",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '兩個都喺最近鄰空間能得到解釋'], ['B', 'X_projected_PCA 喺最近鄰空間能得到解釋'], ['C', '兩個都唔能喺最近鄰空間得到解釋'], ['D', 'X_projected_tSNE 喺最近鄰空間能得到解釋']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列關於維特比算法(Viterbi)嘅講法中錯誤嘅係",
    "llm_output": "A. 維特比算法中嘅轉移概率係從一個隱含狀態轉移到另一個隱含狀態嘅概率\nB. 維特比算法係一種貪心算法\nC. 維特比算法可應用於中文分詞任務\nD. 維特比算法可得到全局最優解\n答案係: A. 維特比算法中嘅轉移概率係從一個隱含狀態轉移到另一個隱含狀態嘅概率\n維特比算法(Viterbi algorithm)係一種喺機器學習中用嚟求解隱含馬爾可夫模型(Hidden Markov Model)嘅算法。\n維特比算法係一種貪心算法，用嚟求解隱含馬爾可夫模型嘅問題。\n維特比算法可應用於中文分詞任務。\n維特比算法可得到局部最優解。",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '維特比算法中嘅轉移概率係從一個隱含狀態轉移到另一個隱含狀態嘅概率'], ['B', '維特比算法係一種貪心算法'], ['C', '維特比算法可應用於中文分詞任務'], ['D', '維特比算法可得到全局最優解']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以下( )唔屬於線性分類器最佳準則",
    "llm_output": "問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA. 線性分類器\nB. 線性分類器\nC. 線性分類器\nD. 線性分類器\n答案係: \n問題：以下( )唔屬於線性分類器\nA",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '貝葉斯分類'], ['B', '感知準則函數'], ['C', '支持向量機'], ['D', 'Fisher準則']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "對於線性回歸，我哋應該有以下邊啲假設",
    "llm_output": "A. 搵到離羣點好重要, 因為線性回歸對離羣點好敏感\nB. 線性回歸假設數據冇多重線性相關性\nC. 線性回歸要求所有變量必須符合正態分佈\nD. 以上都唔係\n答案係: A. 搵到離羣點好重要, 因為線性回歸對離羣點好敏感\n答案係: B. 線性回歸假設數據冇多重線性相關性\n答案係: C. 線性回歸要求所有變量必須符合正態分佈\n答案係: D. 以上都唔係\n答案係: A. 搵到離羣點好重要, 因為線性回歸對離羣點好敏感\n答案係: B. 線性回歸假設數據冇多重線性相關性\n答案係: C. 線性回歸要求所有變量必須符合正態分佈\n答案係: D. 以上都唔係\n答案係: A. 搵到離羣點好重要, 因為線性回歸對離羣點好敏感\n答案係: B. 線性回歸假設數據冇多重線性相關性\n答案係: C. 線性回歸要求所有變量必須符合正態分佈\n答案係: D. 以上都唔係\n答案係: A. 搵到離羣點好重要, 因為線性回歸對離羣點好敏感\n答案係: B. 線性回歸假設數據冇多重線性相關性\n答案係: C. 線性回歸要求所有變量必須符合正態分佈\n答案係: D. 以上都唔係\n答案係: A. 搵到離羣點好重要, 因為線性回歸對離羣點好敏感\n答案係: B. 線性回歸假設數據冇多重線性相關性\n答案係: C. 線性回歸要求所有變量必須符合正態分佈\n答案係: D. 以上都唔係\n答案係: A. 搵到離羣點好重要, 因為線性回歸對離羣點好敏感\n答案係: B. 線性回歸假設數據冇多重線性相關性\n答案係: C. 線性回歸要求所有變量必須符合正態分佈\n答案係: D. 以上都唔係\n答案係: A. 搵到離羣點好重要, 因為線性回歸對離羣點好敏感\n答案係: B. 線性回歸假設數據冇多重線性相關性\n答案係: C. 線性回歸要求所有變量必須符合正態分佈\n答案係: D. 以上都唔係\n答案係: A. 搵到離羣點好重要, 因為線性回歸對離羣點好敏感\n答案係: B. 線性回歸假設數據冇多重線性相關性\n答案係: C. 線性回歸要求所有變量必須符合正態分佈\n答案係: D. 以上都唔係\n答案係: A. 搵到離羣點好重要, 因為線性回歸對離羣點好敏感\n答案係: B. 線性回歸假設數據冇多重線性相關性\n答案係: C. 線性回歸要求所有變量必須符合正態分佈\n答案係: D. 以上都唔係\n答案係: A. 搵到離羣點好重要, 因為線性回歸對離羣點好敏感\n答案係: B. 線性回歸假設數據冇多重線性相關性\n答案係: C. 線性回歸要求所有變量必須符合正態分佈\n答案係: D. 以上都唔係\n答案係: A. 搵到離羣點好重要, 因為線性回歸對離羣點好敏感\n答案係: B. 線性回歸假設數據冇多重線性相關性\n答案係",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '搵到離羣點好重要, 因為線性回歸對離羣點好敏感'], ['B', '線性回歸假設數據冇多重線性相關性'], ['C', '線性回歸要求所有變量必須符合正態分佈'], ['D', '以上都唔係']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下面唔係迭代二叉樹3代算法對數據嘅要求",
    "llm_output": "問題：下面唔係迭代二叉樹3代算法嘅優點\nA. 可處理連續屬性\nB. 可處理離散屬性\nC. 可處理連續屬性\nD. 可處理離散屬性\n答案係: \n問題：下面唔係迭代二叉樹3代算法嘅缺點\nA. 訓練速度較慢\nB. 訓練速度較快\nC. 訓練速度較慢\nD. 訓練速度較快\n答案係: \n問題：下面唔係迭代二叉樹3代算法嘅訓練步驟\nA. 將訓練例嘅屬性值轉換為離散值\nB. 將訓練例嘅屬性值轉換為連續值\nC. 將訓練例嘅屬性值轉換為離散值\nD. 將訓練例嘅屬性值轉換為連續值\n答案係: \n問題：下面唔係迭代二叉樹3代算法嘅訓練步驟\nA. 將訓練例嘅屬性值轉換為離散值\nB. 將訓練例嘅屬性值轉換為連續值\nC. 將訓練例嘅屬性值轉換為離散值\nD. 將訓練例嘅屬性值轉換為連續值\n答案係: \n問題：下面唔係迭代二叉樹3代算法嘅訓練步驟\nA. 將訓練例嘅屬性值轉換為離散值\nB. 將訓練例嘅屬性值轉換為連續值\nC. 將訓練例嘅屬性值轉換為離散值\nD. 將訓練例嘅屬性值轉換為連續值\n答案係: \n問題：下面唔係迭代二叉樹3代算法嘅訓練步驟\nA. 將訓練例嘅屬性值轉換為離散值\nB. 將訓練例嘅屬性值轉換為連續值\nC. 將訓練例嘅屬性值轉換為離散值\nD. 將訓練例嘅屬性值轉換為連續值\n答案係: \n問題：下面唔係迭代二叉樹3代算法嘅訓練步驟\nA. 將訓練例嘅屬性值轉換為離散值\nB. 將訓練例嘅屬性值轉換為連續值\nC. 將訓練例嘅屬性值轉換為離散值\nD. 將訓練例嘅屬性值轉換為連續值\n答案係: \n問題：下面唔係迭代二叉樹3代算法嘅訓練步驟\nA. 將訓練例嘅屬性值轉換為離散值\nB. 將訓練例嘅屬性值轉換為連續值\nC. 將訓練例嘅屬性值轉換為離散值\nD. 將訓練例嘅屬性值轉換為連續值\n答案係: \n問題：下面唔係迭代二叉樹3代算法嘅訓練步驟\nA. 將訓練例嘅屬性值轉換為離散值\nB. 將訓練例嘅屬性值轉換為連續值\nC. 將訓練例嘅屬性值轉換為離散值\nD. 將訓練例嘅屬性值轉換為連續值\n答案係: \n問題：下面唔係迭代二叉樹3代算法嘅訓練步驟\nA. 將訓練例嘅屬性值轉換為離散值\nB. 將訓練例嘅屬性值轉換為連續值\nC. 將訓練例嘅屬性值轉換為離散值\nD. 將訓練例嘅屬性值轉換為連續值\n答案係: \n問題：下面唔係迭代二叉樹3代算法嘅訓練步驟\nA. 將訓練例嘅屬性值轉換為離散值\nB. 將訓練例嘅屬性值轉換為連續值\nC. 將訓練例嘅",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '所有嘅訓練例嘅所有屬性必須有一個明確嘅值'], ['B', '所有屬性必須為離散量'], ['C', '所有屬性必須為連續'], ['D', '相同嘅因素必須得到相同嘅結論且訓練例必須唯一']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下面嘅優化算法中，速度最快嘅係",
    "llm_output": "梯度下降法\n梯度下降法係一種最速下降法，用於機器學習嘅最速下降法。\n梯度下降法嘅基本思想係：\n假設我們有一個函數，我們希望找到函數嘅最小值，咁我們可以通過不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數嘅導數為零嘅點，然後不斷畀函數�",
    "correct_answer": "C",
    "standard_answer_range": "[['A', 'BFGS'], ['B', '梯度下降法'], ['C', '牛頓法'], ['D', 'Adam']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列關於ALBERT嘅講法唔正確嘅係",
    "llm_output": "ALBERT係一個基於BERT嘅模型，喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，唔係喺BERT嘅基礎上，ALBERT嘅主要特點係跨層參數共享，�",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '跨層參數共享'], ['B', '採用詞嵌入向量參數嘅因式分解'], ['C', '應用喺下游任務中預測速度顯著提速'], ['D', '去掉咗dropout']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下面邊個屬於SVM應用",
    "llm_output": "問題：下面邊個屬於SVM應用\nA. 文本分類\nB. 圖像分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 文本分類\nB. 圖像分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\nC. 以上均係\n答案係: \n問題：下面邊個屬於SVM應用\nA. 圖像分類\nB. 文本分類\n",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '新文章聚類'], ['B', '文本同超文本分類'], ['C', '圖像分類'], ['D', '以上均係']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "類域界面方程法中，唔能求線性不可分情況下分類問題近似或者精確解嘅方法係",
    "llm_output": "問題：下列哪個係機器學習中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器學習問題中，用來表示機器",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '基於二次準則嘅H-K算法'], ['B', '感知器算法'], ['C', '勢函數法'], ['D', '偽逆法']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下面邊個選項中邊一項屬於確定性算法",
    "llm_output": "問題：下面邊個選項中邊一項屬於非確定性算法\nA. SVM\nB. KNN\nC. PCA\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上都係\n答案係: \n問題：下面邊個選項中邊一項屬於非確定性算法\nA. K-Means\nB. PCA\nC. KNN\nD. 以上",
    "correct_answer": "B",
    "standard_answer_range": "[['A', 'K-Means'], ['B', 'PCA'], ['C', 'KNN'], ['D', '以上都唔係']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以下邊啲算法, 1. KNN；2. 線性回歸；3.對數幾率回歸。可以用神經網絡去構造:",
    "llm_output": "神經網絡係一種機器學習算法，可以用來做很多嘅事情，例如:\n1. 預測\n2. 分類\n3. 推薦系統\n4. 自動化\n5. 模式識別\n6. 機器翻譯\n7. 語音識別\n8. 視覺識別\n9. 自然語言處理\n10. 電腦視覺\n11. 資料挖掘\n12. 機器學習\n13. 人工智能\n14. 機器學習\n15. 人工智能\n16. 機器學習\n17. 人工智能\n18. 機器學習\n19. 人工智能\n20. 機器學習\n21. 人工智能\n22. 機器學習\n23. 人工智能\n24. 機器學習\n25. 人工智能\n26. 機器學習\n27. 人工智能\n28. 機器學習\n29. 人工智能\n30. 機器學習\n31. 人工智能\n32. 機器學習\n33. 人工智能\n34. 機器學習\n35. 人工智能\n36. 機器學習\n37. 人工智能\n38. 機器學習\n39. 人工智能\n40. 機器學習\n41. 人工智能\n42. 機器學習\n43. 人工智能\n44. 機器學習\n45. 人工智能\n46. 機器學習\n47. 人工智能\n48. 機器學習\n49. 人工智能\n50. 機器學習\n51. 人工智能\n52. 機器學習\n53. 人工智能\n54. 機器學習\n55. 人工智能\n56. 機器學習\n57. 人工智能\n58. 機器學習\n59. 人工智能\n60. 機器學習\n61. 人工智能\n62. 機器學習\n63. 人工智能\n64. 機器學習\n65. 人工智能\n66. 機器學習\n67. 人工智能\n68. 機器學習\n69. 人工智能\n70. 機器學習\n71. 人工智能\n72. 機器學習\n73. 人工智能\n74. 機器學習\n75. 人工智能\n76. 機器學習\n77. 人工智能\n78. 機器學習\n79. 人工智能\n80. 機器學習\n81. 人工智能\n82. 機器學習\n83. 人工智能\n84. 機器學習\n85. 人工智能\n86. 機器學習\n87. 人工智能\n88. 機器學習\n89. 人工智能\n90. 機器學習\n91. 人工智能\n92. 機器學習\n93. 人工智能\n94. 機器學習\n95. 人工智能\n96. 機器學習\n97. 人工智能\n98. 機器學習\n99. 人工智能\n100. 機器學習\n101. 人工智能\n102. 機器學習\n103. 人工智能\n104. 機器學習\n105. 人工智能\n106. 機器學習\n107. 人工智能\n108. 機器學習\n109. 人工智能\n110. 機器學習\n111. 人工智能\n112. 機器學習\n113. 人工智能\n114. 機器學習\n115. 人工智能\n116. 機器學習\n117. 人工智能\n118. 機器學習\n119. 人工智能\n120. 機器學習\n121. 人工智能\n122. 機器學習\n123. 人工智能\n124. 機器學習\n125. 人工智能\n126. 機器學習\n127. 人工智能\n128. 機器學習\n129. 人工智能\n130. 機器學習\n131. 人工智能\n132. 機器學習\n133. 人工智能\n134. 機器學習\n135. 人工智能\n136. 機器學習\n137. 人工智能\n13",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '2 同 3'], ['B', '1同 2'], ['C', '以上都唔係'], ['D', '1, 2 同 3']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "訓練SVM嘅最少時間複雜度為O(n^2)，噉以下邊種數據集唔適合用SVM?",
    "llm_output": "問題：以下邊種方法可以改善SVM嘅訓練時間複雜度?\nA. 使用SMO\nB. 使用SGD\nC. 使用SVM嘅線性時間複雜度\nD. 使用SVM嘅線性時間複雜度同SGD\n答案係: \n問題：以下邊種方法可以改善SVM嘅訓練時間複雜度?\nA. 使用SVM嘅線性時間複雜度\nB. 使用SVM嘅線性時間複雜度同SGD\nC. 使用SVM嘅線性時間複雜度同SMO\nD. 使用SVM嘅線性時間複雜度同SMO同SGD\n答案係: \n問題：以下邊種方法可以改善SVM嘅訓練時間複雜度?\nA. 使用SVM嘅線性時間複雜度\nB. 使用SVM嘅線性時間複雜度同SGD\nC. 使用SVM嘅線性時間複雜度同SMO\nD. 使用SVM嘅線性時間複雜度同SMO同SGD\n答案係: \n問題：以下邊種方法可以改善SVM嘅訓練時間複雜度?\nA. 使用SVM嘅線性時間複雜度\nB. 使用SVM嘅線性時間複雜度同SGD\nC. 使用SVM嘅線性時間複雜度同SMO\nD. 使用SVM嘅線性時間複雜度同SMO同SGD\n答案係: \n問題：以下邊種方法可以改善SVM嘅訓練時間複雜度?\nA. 使用SVM嘅線性時間複雜度\nB. 使用SVM嘅線性時間複雜度同SGD\nC. 使用SVM嘅線性時間複雜度同SMO\nD. 使用SVM嘅線性時間複雜度同SMO同SGD\n答案係: \n問題：以下邊種方法可以改善SVM嘅訓練時間複雜度?\nA. 使用SVM嘅線性時間複雜度\nB. 使用SVM嘅線性時間複雜度同SGD\nC. 使用SVM嘅線性時間複雜度同SMO\nD. 使用SVM嘅線性時間複雜度同SMO同SGD\n答案係: \n問題：以下邊種方法可以改善SVM嘅訓練時間複雜度?\nA. 使用SVM嘅線性時間複雜度\nB. 使用SVM嘅線性時間複雜度同SGD\nC. 使用SVM嘅線性時間複雜度同SMO\nD. 使用SVM嘅線性時間複雜度同SMO同SGD\n答案係: \n問題：以下邊種方法可以改善SVM嘅訓練時間複雜度?\nA. 使用SVM嘅線性時間複雜度\nB. 使用SVM嘅線性時間複雜度同SGD\nC. 使用SVM嘅線性時間複雜度同SMO\nD. 使用SVM嘅線性時間複雜度同SMO同SGD\n答案係: \n問題：以下邊種方法可以改善SVM嘅訓練時間複雜度?\nA. 使用SVM嘅線性時間複雜度\nB. 使用SVM嘅線性時間複雜度同SGD\nC. 使用SVM嘅線性時間複雜度同SMO\nD. 使用SVM嘅線性時間複雜度同SMO同SGD\n答案係: \n問題：以下邊種方法可以改善SVM嘅訓練時間複雜度?\nA. 使用SVM嘅線性時間複雜度\nB. 使用SVM嘅線性時間複雜度同SGD\nC. 使用SVM嘅線性時間複雜度同SMO\nD. 使用SVM嘅線性時間複雜度同SMO同SGD\n答案係: \n問題：以下邊種方法可以改善SVM嘅訓練時間複雜度?\nA. 使用SVM嘅線性時間複雜度\nB. 使用S",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '同數據集大細冇關'], ['B', '大數據集'], ['C', '細數據集'], ['D', '中等大細數據集']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "如果線性回歸模型中嘅隨機誤差存在異方差性，噉參數嘅普通最細二乘法估計量係",
    "llm_output": "參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\nD. 有偏嘅，有效嘅\n參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\nD. 有偏嘅，有效嘅\n參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\nD. 有偏嘅，有效嘅\n參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\nD. 有偏嘅，有效嘅\n參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\nD. 有偏嘅，有效嘅\n參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\nD. 有偏嘅，有效嘅\n參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\nD. 有偏嘅，有效嘅\n參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\nD. 有偏嘅，有效嘅\n參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\nD. 有偏嘅，有效嘅\n參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\nD. 有偏嘅，有效嘅\n參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\nD. 有偏嘅，有效嘅\n參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\nD. 有偏嘅，有效嘅\n參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\nD. 有偏嘅，有效嘅\n參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\nD. 有偏嘅，有效嘅\n參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\nD. 有偏嘅，有效嘅\n參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\nD. 有偏嘅，有效嘅\n參考答案：\nA. 無偏嘅，非有效嘅\nB. 無偏嘅，有效嘅\nC. 有偏嘅，非有效嘅\n",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '無偏嘅，非有效嘅'], ['B', '無偏嘅，有效嘅'], ['C', '有偏嘅，非有效嘅'], ['D', '有偏嘅，有效嘅']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列關於RoBERTa嘅講法唔正確嘅係",
    "llm_output": "問題：下列關於BERT嘅講法唔正確嘅係\nA. 唔做NSP任務\nB. 採用靜態掩碼機制\nC. 採用更多訓練數據\nD. 訓練採用更大batch size\n答案係: \n問題：下列關於GPT嘅講法唔正確嘅係\nA. 唔做NSP任務\nB. 採用靜態掩碼機制\nC. 採用更多訓練數據\nD. 訓練採用更大batch size\n答案係: \n問題：下列關於GPT-2嘅講法唔正確嘅係\nA. 唔做NSP任務\nB. 採用靜態掩碼機制\nC. 採用更多訓練數據\nD. 訓練採用更大batch size\n答案係: \n問題：下列關於GPT-3嘅講法唔正確嘅係\nA. 唔做NSP任務\nB. 採用靜態掩碼機制\nC. 採用更多訓練數據\nD. 訓練採用更大batch size\n答案係: \n問題：下列關於GPT-4嘅講法唔正確嘅係\nA. 唔做NSP任務\nB. 採用靜態掩碼機制\nC. 採用更多訓練數據\nD. 訓練採用更大batch size\n答案係: \n問題：下列關於GPT-5嘅講法唔正確嘅係\nA. 唔做NSP任務\nB. 採用靜態掩碼機制\nC. 採用更多訓練數據\nD. 訓練採用更大batch size\n答案係: \n問題：下列關於GPT-6嘅講法唔正確嘅係\nA. 唔做NSP任務\nB. 採用靜態掩碼機制\nC. 採用更多訓練數據\nD. 訓練採用更大batch size\n答案係: \n問題：下列關於GPT-7嘅講法唔正確嘅係\nA. 唔做NSP任務\nB. 採用靜態掩碼機制\nC. 採用更多訓練數據\nD. 訓練採用更大batch size\n答案係: \n問題：下列關於GPT-8嘅講法唔正確嘅係\nA. 唔做NSP任務\nB. 採用靜態掩碼機制\nC. 採用更多訓練數據\nD. 訓練採用更大batch size\n答案係: \n問題：下列關於GPT-9嘅講法唔正確嘅係\nA. 唔做NSP任務\nB. 採用靜態掩碼機制\nC. 採用更多訓練數據\nD. 訓練採用更大batch size\n答案係: \n問題：下列關於GPT-10嘅講法唔正確嘅係\nA. 唔做NSP任務\nB. 採用靜態掩碼機制\nC. 採用更多訓練數據\nD. 訓練採用更大batch size\n答案係: \n問題：下列關於GPT-11嘅講法唔正確嘅係\nA. 唔做NSP任務\nB. 採用靜態掩碼機制\nC. 採用更多訓練數據\nD. 訓練採用更大batch size\n答案係: \n問題：下列關於GPT-12嘅講法唔正確嘅係\nA. 唔做NSP任務\nB. 採用靜態掩碼機制\nC. 採用更多訓練數據\nD. 訓練採用更大batch size\n答案係: \n問題：下列關於GPT-13嘅講法唔正確嘅係\nA. 唔做NSP任務\nB. 採用靜態掩碼機制\nC. 採用更多訓練數據\nD. 訓練採用更大batch size\n答案係: \n問題：下列關於GPT-14嘅講法唔正確嘅係\nA. 唔做NSP任務\nB. 採用靜態掩碼機制\nC. 採用更多訓練數據\nD. ",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '唔做NSP任務'], ['B', '採用靜態掩碼機制'], ['C', '採用更多訓練數據'], ['D', '訓練採用更大batch size']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "喺邏輯回歸輸出與目標對比嘅情況下，以下評估指標中邊一項唔適用？",
    "llm_output": "Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評估指標，用嚟評估一個機器學習模型嘅表現。Logloss係一個喺機器學習中常用嘅評",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '準確度'], ['B', '均方誤差'], ['C', 'AUC-ROC'], ['D', 'Logloss']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "語言模型嘅參數估計經常使用MLE（最大似然估計）。面臨嘅一個問題係冇出現嘅項概率為0，噉會導致語言模型嘅效果唔好。為咗解決呢個問題，需要使用（）",
    "llm_output": "B",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '增加白噪音'], ['B', '平滑'], ['C', '隨機插值'], ['D', '去噪']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "建模北京市人口嘅年齡分佈，採用咩分佈更合適",
    "llm_output": "問題：下列哪項係機器學習嘅特徵\nA. 機器學習係統嘅設計係以人為中心\nB. 機器學習係統嘅設計係以數據為中心\nC. 機器學習係統嘅設計係以算法為中心\nD. 機器學習係統嘅設計係以模型為中心\n答案係: \n問題：下列哪項係機器學習嘅特徵\nA. 機器學習係系統嘅設計係以人為中心\nB. 機器學習係系統嘅設計係以數據為中心\nC. 機器學習係系統嘅設計係以算法為中心\nD. 機器學習係系統嘅設計係以模型為中心\n答案係: \n問題：下列哪項係機器學習嘅特徵\nA. 機器學習係系統嘅設計係以人為中心\nB. 機器學習係系統嘅設計係以數據為中心\nC. 機器學習係系統嘅設計係以算法為中心\nD. 機器學習係系統嘅設計係以模型為中心\n答案係: \n問題：下列哪項係機器學習嘅特徵\nA. 機器學習係系統嘅設計係以人為中心\nB. 機器學習係系統嘅設計係以數據為中心\nC. 機器學習係系統嘅設計係以算法為中心\nD. 機器學習係系統嘅設計係以模型為中心\n答案係: \n問題：下列哪項係機器學習嘅特徵\nA. 機器學習係系統嘅設計係以人為中心\nB. 機器學習係系統嘅設計係以數據為中心\nC. 機器學習係系統嘅設計係以算法為中心\nD. 機器學習係系統嘅設計係以模型為中心\n答案係: \n問題：下列哪項係機器學習嘅特徵\nA. 機器學習係系統嘅設計係以人為中心\nB. 機器學習係系統嘅設計係以數據為中心\nC. 機器學習係系統嘅設計係以算法為中心\nD. 機器學習係系統嘅設計係以模型為中心\n答案係: \n問題：下列哪項係機器學習嘅特徵\nA. 機器學習係系統嘅設計係以人為中心\nB. 機器學習係系統嘅設計係以數據為中心\nC. 機器學習係系統嘅設計係以算法為中心\nD. 機器學習係系統嘅設計係以模型為中心\n答案係: \n問題：下列哪項係機器學習嘅特徵\nA. 機器學習係系統嘅設計係以人為中心\nB. 機器學習係系統嘅設計係以數據為中心\nC. 機器學習係系統嘅設計係以算法為中心\nD. 機器學習係系統嘅設計係以模型為中心\n答案係: \n問題：下列哪項係機器學習嘅特徵\nA. 機器學習係系統嘅設計係以人為中心\nB. 機器學習係系統嘅設計係以數據為中心\nC. 機器學習係系統嘅設計係以算法為中心\nD. 機器學習係系統嘅設計係以模型為中心\n答案係: \n問題：下列哪項係機器學習嘅特徵\nA. 機器學習係系統嘅設計係以人為中心\nB. 機器學習係系統嘅設計係以數據為中心\nC. 機器學習係系統嘅設計係以算法為中心\nD. 機器學習係系統嘅設計係以模型為中心\n答案係: \n問題：下列哪項係機器學習嘅特徵\nA. 機器學習係系統嘅設計係以人為中心\nB. 機器學習係系統嘅設計係以數據為中心\nC. 機器學習係系統嘅設計係以算法為中心\nD. 機器學習係系統嘅設計係以模型為中心\n答案係: \n問題：下列哪項係機器學習嘅",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '0-1分佈'], ['B', '正態分佈'], ['C', '泊松分佈'], ['D', '指數分佈']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "SVM中嘅代價參數表示：",
    "llm_output": "問題：下列嘅描述中，哪一個是關於SVM嘅錯誤描述？\nA. SVM嘅目標係最大化邊界之間嘅距離\nB. SVM嘅目標係最小化誤分類\nC. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡\nD. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡，同時最大化邊界之間嘅距離\n答案係: \n問題：下列嘅描述中，哪一個是關於SVM嘅錯誤描述？\nA. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡\nB. SVM嘅目標係最小化誤分類\nC. SVM嘅目標係最大化邊界之間嘅距離\nD. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡，同時最大化邊界之間嘅距離\n答案係: \n問題：下列嘅描述中，哪一個是關於SVM嘅錯誤描述？\nA. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡，同時最大化邊界之間嘅距離\nB. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡\nC. SVM嘅目標係最小化誤分類\nD. SVM嘅目標係最大化邊界之間嘅距離\n答案係: \n問題：下列嘅描述中，哪一個是關於SVM嘅錯誤描述？\nA. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡，同時最大化邊界之間嘅距離\nB. SVM嘅目標係最小化誤分類\nC. SVM嘅目標係最大化邊界之間嘅距離\nD. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡\n答案係: \n問題：下列嘅描述中，哪一個是關於SVM嘅錯誤描述？\nA. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡，同時最大化邊界之間嘅距離\nB. SVM嘅目標係最小化誤分類\nC. SVM嘅目標係最大化邊界之間嘅距離\nD. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡\n答案係: \n問題：下列嘅描述中，哪一個是關於SVM嘅錯誤描述？\nA. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡，同時最大化邊界之間嘅距離\nB. SVM嘅目標係最小化誤分類\nC. SVM嘅目標係最大化邊界之間嘅距離\nD. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡\n答案係: \n問題：下列嘅描述中，哪一個是關於SVM嘅錯誤描述？\nA. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡，同時最大化邊界之間嘅距離\nB. SVM嘅目標係最小化誤分類\nC. SVM嘅目標係最大化邊界之間嘅距離\nD. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡\n答案係: \n問題：下列嘅描述中，哪一個是關於SVM嘅錯誤描述？\nA. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡，同時最大化邊界之間嘅距離\nB. SVM嘅目標係最小化誤分類\nC. SVM嘅目標係最大化邊界之間嘅距離\nD. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡\n答案係: \n問題：下列嘅描述中，哪一個是關於SVM嘅錯誤描述？\nA. SVM嘅目標係最小化誤分類同模型複雜性之間嘅平衡，同時最大化邊界之間嘅距離\nB. SVM嘅目標係最小化誤分類\n",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '誤分類同模型複雜性之間嘅平衡'], ['B', '以上均唔係'], ['C', '使用嘅核'], ['D', '交叉驗證嘅次數']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "關於SVM泛化誤差描述正確嘅係",
    "llm_output": "問題：關於SVM嘅描述正確嘅係\nA. 唔需要訓練數據\nB. 可以對未知數據進行預測\nC. 可以對已知數據進行預測\nD. 以上都唔係\n答案係: \n問題：關於SVM嘅描述正確嘅係\nA. 唔需要訓練數據\nB. 可以對未知數據進行預測\nC. 可以對已知數據進行預測\nD. 以上都唔係\n答案係: \n問題：關於SVM嘅描述正確嘅係\nA. 唔需要訓練數據\nB. 可以對未知數據進行預測\nC. 可以對已知數據進行預測\nD. 以上都唔係\n答案係: \n問題：關於SVM嘅描述正確嘅係\nA. 唔需要訓練數據\nB. 可以對未知數據進行預測\nC. 可以對已知數據進行預測\nD. 以上都唔係\n答案係: \n問題：關於SVM嘅描述正確嘅係\nA. 唔需要訓練數據\nB. 可以對未知數據進行預測\nC. 可以對已知數據進行預測\nD. 以上都唔係\n答案係: \n問題：關於SVM嘅描述正確嘅係\nA. 唔需要訓練數據\nB. 可以對未知數據進行預測\nC. 可以對已知數據進行預測\nD. 以上都唔係\n答案係: \n問題：關於SVM嘅描述正確嘅係\nA. 唔需要訓練數據\nB. 可以對未知數據進行預測\nC. 可以對已知數據進行預測\nD. 以上都唔係\n答案係: \n問題：關於SVM嘅描述正確嘅係\nA. 唔需要訓練數據\nB. 可以對未知數據進行預測\nC. 可以對已知數據進行預測\nD. 以上都唔係\n答案係: \n問題：關於SVM嘅描述正確嘅係\nA. 唔需要訓練數據\nB. 可以對未知數據進行預測\nC. 可以對已知數據進行預測\nD. 以上都唔係\n答案係: \n問題：關於SVM嘅描述正確嘅係\nA. 唔需要訓練數據\nB. 可以對未知數據進行預測\nC. 可以對已知數據進行預測\nD. 以上都唔係\n答案係: \n問題：關於SVM嘅描述正確嘅係\nA. 唔需要訓練數據\nB. 可以對未知數據進行預測\nC. 可以對已知數據進行預測\nD. 以上都唔係\n答案係: \n問題：關於SVM嘅描述正確嘅係\nA. 唔需要訓練數據\nB. 可以對未知數據進行預測\nC. 可以對已知數據進行預測\nD. 以上都唔係\n答案係: \n問題：關於SVM嘅描述正確嘅係\nA. 唔需要訓練數據\nB. 可以對未知數據進行預測\nC. 可以對已知數據進行預測\nD. 以上都唔係\n答案係: \n問題：關於SVM嘅描述正確嘅係\nA. 唔需要訓練數據\nB. 可以對未知數據進行預測\nC. 可以對已知數據進行預測\nD. 以上都唔係\n答案係: \n問題：關於SVM嘅描述正確嘅係\nA. 唔需要訓練數據\nB. 可以對未知數據進行預測\nC. 可以對已知數據進行預測\nD. 以上都唔係\n答案係: \n問題：關於SVM嘅描述正確嘅係\nA. 唔需要訓練數據\nB. 可以對未知數據進行預測\nC. 可以對已知數據進行預測\nD. 以上都唔係\n答案係: \n問題：關於SVM嘅描述正確嘅係\n",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '超平面同支持向量之間距離'], ['B', 'SVM嘅誤差閾值'], ['C', '以上都唔係'], ['D', 'SVM對未知數據嘅預測能力']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列關於BERT嘅講法唔正確嘅係",
    "llm_output": "問題：下列關於BERT嘅講法正確嘅係\nA. 支持對語義上下文進行建模\nB. 採用激活函數GELU\nC. 網絡一共有20層\nD. 使用transformer\n答案係: \n問題：下列關於BERT嘅講法唔正確嘅係\nA. 支持對語義上下文進行建模\nB. 採用激活函數GELU\nC. 網絡一共有20層\nD. 使用transformer\n答案係: \n問題：下列關於BERT嘅講法正確嘅係\nA. 支持對語義上下文進行建模\nB. 採用激活函數GELU\nC. 網絡一共有20層\nD. 使用transformer\n答案係: \n問題：下列關於BERT嘅講法唔正確嘅係\nA. 支持對語義上下文進行建模\nB. 採用激活函數GELU\nC. 網絡一共有20層\nD. 使用transformer\n答案係: \n問題：下列關於BERT嘅講法正確嘅係\nA. 支持對語義上下文進行建模\nB. 採用激活函數GELU\nC. 網絡一共有20層\nD. 使用transformer\n答案係: \n問題：下列關於BERT嘅講法唔正確嘅係\nA. 支持對語義上下文進行建模\nB. 採用激活函數GELU\nC. 網絡一共有20層\nD. 使用transformer\n答案係: \n問題：下列關於BERT嘅講法正確嘅係\nA. 支持對語義上下文進行建模\nB. 採用激活函數GELU\nC. 網絡一共有20層\nD. 使用transformer\n答案係: \n問題：下列關於BERT嘅講法唔正確嘅係\nA. 支持對語義上下文進行建模\nB. 採用激活函數GELU\nC. 網絡一共有20層\nD. 使用transformer\n答案係: \n問題：下列關於BERT嘅講法正確嘅係\nA. 支持對語義上下文進行建模\nB. 採用激活函數GELU\nC. 網絡一共有20層\nD. 使用transformer\n答案係: \n問題：下列關於BERT嘅講法唔正確嘅係\nA. 支持對語義上下文進行建模\nB. 採用激活函數GELU\nC. 網絡一共有20層\nD. 使用transformer\n答案係: \n問題：下列關於BERT嘅講法正確嘅係\nA. 支持對語義上下文進行建模\nB. 採用激活函數GELU\nC. 網絡一共有20層\nD. 使用transformer\n答案係: \n問題：下列關於BERT嘅講法唔正確嘅係\nA. 支持對語義上下文進行建模\nB. 採用激活函數GELU\nC. 網絡一共有20層\nD. 使用transformer\n答案係: \n問題：下列關於BERT嘅講法正確嘅係\nA. 支持對語義上下文進行建模\nB. 採用激活函數GELU\nC. 網絡一共有20層\nD. 使用transformer\n答案係: \n問題：下列關於BERT嘅講法唔正確嘅係\nA. 支持對語義上下文進行建模\nB. 採用激活函數GELU\nC. 網絡一共有20層\nD. 使用transformer\n答案係: \n問題：下列關於BERT嘅講法正確嘅係\nA. 支持對語義上下文進行建模\nB. 採用激活函數GELU\nC. 網絡一共有20層\nD. 使用transformer\n答案係: \n問題：下列關於BERT嘅講法唔正確嘅係\nA. 支持對語義上下文進行建模\nB. 採用激活函數GELU",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '支持對語義上下文進行建模'], ['B', '採用激活函數GELU'], ['C', '網絡一共有20層'], ['D', '使用transformer']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "模式識別中，唔屬於馬氏距離較之於歐式距離嘅優點嘅係",
    "llm_output": "馬氏距離係一種度量兩個向量之間嘅距離嘅方法，係機器學習中常用嘅距離度量。\n馬氏距離嘅優點係：\n1. 考慮到各種特性之間嘅聯繫\n2. 考慮咗模式嘅分佈\n馬氏距離嘅缺點係：\n1. 尺度唔變性\n2. 平移唔變性\n3. 不適用於高維空間\n4. 不適用於非正規分布嘅數據\n5. 不適用於非線性嘅數據\n6. 不適用於非凸嘅數據",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '尺度唔變性'], ['B', '平移唔變性'], ['C', '考慮到各種特性之間嘅聯繫'], ['D', '考慮咗模式嘅分佈']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "描述嘅機器發生故障嘅次數，採用咩分佈更合適?",
    "llm_output": "機器發生故障嘅次數，係一個隨機變數，而且係一個離散型隨機變數。\n機器發生故障嘅次數，係一個隨機變數，而且係一個離散型隨機變數。所以，機器發生故障嘅次數，採用咩分佈更合適?\n正態分佈係一個連續型隨機變數，所以，正態分佈唔適合。\n指數分佈係一個連續型隨機變數，所以，指數分佈唔適合。\n泊松分佈係一個離散型隨機變數，所以，泊松分佈適合。\n0-1分佈係一個離散型隨機變數，所以，0-1分佈適合。\n所以，答案係D。\n問題：描述嘅機器發生故障嘅次數，採用咩分佈更合適?\nA. 0-1分佈\nB. 指數分佈\nC. 正態分佈\nD. 泊松分佈\n答案係: \n機器發生故障嘅次數，係一個隨機變數，而且係一個離散型隨機變數。所以，機器發生故障嘅次數，採用咩分佈更合適?\n泊松分佈係一個離散型隨機變數，所以，泊松分佈適合。\n所以，答案係B。\n問題：描述嘅機器發生故障嘅次數，採用咩分佈更合適?\nA. 0-1分佈\nB. 指數分佈\nC. 正態分佈\nD. 泊松分佈\n答案係: \n機器發生故障嘅次數，係一個隨機變數，而且係一個離散型隨機變數。所以，機器發生故障嘅次數，採用咩分佈更合適?\n泊松分佈係一個離散型隨機變數，所以，泊松分佈適合。\n所以，答案係D。\n問題：描述嘅機器發生故障嘅次數，採用咩分佈更合適?\nA. 0-1分佈\nB. 指數分佈\nC. 正態分佈\nD. 泊松分佈\n答案係: \n機器發生故障嘅次數，係一個隨機變數，而且係一個離散型隨機變數。所以，機器發生故障嘅次數，採用咩分佈更合適?\n泊松分佈係一個離散型隨機變數，所以，泊松分佈適合。\n所以，答案係D。\n問題：描述嘅機器發生故障嘅次數，採用咩分佈更合適?\nA. 0-1分佈\nB. 指數分佈\nC. 正態分佈\nD. 泊松分佈\n答案係: \n機器發生故障嘅次數，係一個隨機變數，而且係一個離散型隨機變數。所以，機器發生故障嘅次數，採用咩分佈更合適?\n泊松分佈係一個離散型隨機變數，所以，泊松分佈適合。\n所以，答案係D。\n問題：描述嘅機器發生故障嘅次數，採用咩分佈更合適?\nA. 0-1分佈\nB. 指數分佈\nC. 正態分佈\nD. 泊松分佈\n答案係: \n機器發生故障嘅次數，係一個隨機變數，而且係一個離散型隨機變數。所以，機器發生故障嘅次數，採用咩分佈更合適?\n泊松分佈係一個離散型隨機變數，所以，泊松分佈適合。\n所以，答案係D。\n問題：描述嘅機器發生故障嘅次數，採用咩分佈更合適?\nA. 0-1分佈\nB. 指數分佈\nC. 正態分佈\nD. 泊松分佈\n答案係: \n機器發生故障嘅次數，係一個隨機變數，而且係一個離",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '0-1分佈'], ['B', '指數分佈'], ['C', '正態分佈'], ['D', '泊松分佈']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以下邊個唔係LSTM本身嘅特點",
    "llm_output": "問題：以下邊個唔係LSTM嘅優點\nA. 防止梯度彌散\nB. 訓練時GPU使用率較高\nC. 訓練時GPU使用率較低\nD. LSTM有遺忘門\n答案係: \n問題：以下邊個係LSTM嘅優點\nA. 防止梯度彌散\nB. 訓練時GPU使用率較高\nC. 訓練時GPU使用率較低\nD. LSTM有遺忘門\n答案係: \n問題：以下邊個係LSTM嘅特點\nA. LSTM係RNN嘅一種變種\nB. 防止梯度彌散\nC. 訓練時GPU使用率較高\nD. LSTM有遺忘門\n答案係: \n問題：以下邊個係LSTM嘅特點\nA. LSTM係RNN嘅一種變種\nB. 防止梯度彌散\nC. 訓練時GPU使用率較高\nD. LSTM有遺忘門\n答案係: \n問題：以下邊個係LSTM嘅特點\nA. LSTM係RNN嘅一種變種\nB. 防止梯度彌散\nC. 訓練時GPU使用率較高\nD. LSTM有遺忘門\n答案係: \n問題：以下邊個係LSTM嘅特點\nA. LSTM係RNN嘅一種變種\nB. 防止梯度彌散\nC. 訓練時GPU使用率較高\nD. LSTM有遺忘門\n答案係: \n問題：以下邊個係LSTM嘅特點\nA. LSTM係RNN嘅一種變種\nB. 防止梯度彌散\nC. 訓練時GPU使用率較高\nD. LSTM有遺忘門\n答案係: \n問題：以下邊個係LSTM嘅特點\nA. LSTM係RNN嘅一種變種\nB. 防止梯度彌散\nC. 訓練時GPU使用率較高\nD. LSTM有遺忘門\n答案係: \n問題：以下邊個係LSTM嘅特點\nA. LSTM係RNN嘅一種變種\nB. 防止梯度彌散\nC. 訓練時GPU使用率較高\nD. LSTM有遺忘門\n答案係: \n問題：以下邊個係LSTM嘅特點\nA. LSTM係RNN嘅一種變種\nB. 防止梯度彌散\nC. 訓練時GPU使用率較高\nD. LSTM有遺忘門\n答案係: \n問題：以下邊個係LSTM嘅特點\nA. LSTM係RNN嘅一種變種\nB. 防止梯度彌散\nC. 訓練時GPU使用率較高\nD. LSTM有遺忘門\n答案係: \n問題：以下邊個係LSTM嘅特點\nA. LSTM係RNN嘅一種變種\nB. 防止梯度彌散\nC. 訓練時GPU使用率較高\nD. LSTM有遺忘門\n答案係: \n問題：以下邊個係LSTM嘅特點\nA. LSTM係RNN嘅一種變種\nB. 防止梯度彌散\nC. 訓練時GPU使用率較高\nD. LSTM有遺忘門\n答案係: \n問題：以下邊個係LSTM嘅特點\nA. LSTM係RNN嘅一種變種\nB. 防止梯度彌散\nC. 訓練時GPU使用率較高\nD. LSTM有遺忘門\n答案係: \n問題：以下邊個係LSTM嘅特點\nA. LSTM係RNN嘅一種變種\nB. 防止梯度彌散\nC. 訓練時GPU使用率較高\nD. LSTM有遺忘門\n答案係: \n問題：以下邊個係LSTM嘅特點\nA. LSTM係RNN嘅一種變種\nB. 防止梯度彌散\nC. 訓練時GPU使用率較高\nD. LSTM有遺忘門\n答案係: \n問題：以下邊個係LSTM嘅特點\nA. LSTM係RNN嘅一種變種\nB. 防止梯度彌散\nC. 訓練時GPU使用率較高\nD",
    "correct_answer": "C",
    "standard_answer_range": "[['A', 'LSTM係RNN嘅一種變種'], ['B', '防止梯度彌散'], ['C', '訓練時GPU使用率較高'], ['D', 'LSTM有遺忘門']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "關於邏輯回歸同支持向量機唔正確嘅係",
    "llm_output": "A",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '邏輯回歸本質上係一種根據樣本對權值進行極大似然估計嘅方法，而後驗概率正比於先驗概率同似然函數嘅乘積。邏輯僅僅係最大化似然函數，並冇最大化後驗概率，更談唔上最小化後驗概率'], ['B', '支持向量機可以通過正則化系數控制模型嘅複雜度，避免過擬合。'], ['C', '支持向量機嘅目標係搵到使得訓練數據盡可能分開且分類間隔最大嘅超平面，應該屬於結構風險最小化'], ['D', '邏輯回歸嘅輸出就係樣本屬於正類別嘅機率，可以計算出概率。']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以下唔屬於影響聚類算法結果嘅主要因素有",
    "llm_output": "聚類算法嘅主要因素有特徵揀、分類準則、模式相似性測度，而唔包含已知類別嘅樣本質量。\n問題：以下唔屬於影響聚類算法結果嘅主要因素有\nA. 特徵揀\nB. 分類準則\nC. 模式相似性測度\nD. 已知類別嘅樣本質量\n答案係: \n聚類算法嘅主要因素有特徵揀、分類準則、模式相似性測度，而唔包含已知類別嘅樣本質量。\n問題：以下唔屬於影響聚類算法結果嘅主要因素有\nA. 特徵揀\nB. 分類準則\nC. 模式相似性測度\nD. 已知類別嘅樣本質量\n答案係: \n聚類算法嘅主要因素有特徵揀、分類準則、模式相似性測度，而唔包含已知類別嘅樣本質量。\n問題：以下唔屬於影響聚類算法結果嘅主要因素有\nA. 特徵揀\nB. 分類準則\nC. 模式相似性測度\nD. 已知類別嘅樣本質量\n答案係: \n聚類算法嘅主要因素有特徵揀、分類準則、模式相似性測度，而唔包含已知類別嘅樣本質量。\n問題：以下唔屬於影響聚類算法結果嘅主要因素有\nA. 特徵揀\nB. 分類準則\nC. 模式相似性測度\nD. 已知類別嘅樣本質量\n答案係: \n聚類算法嘅主要因素有特徵揀、分類準則、模式相似性測度，而唔包含已知類別嘅樣本質量。\n問題：以下唔屬於影響聚類算法結果嘅主要因素有\nA. 特徵揀\nB. 分類準則\nC. 模式相似性測度\nD. 已知類別嘅樣本質量\n答案係: \n聚類算法嘅主要因素有特徵揀、分類準則、模式相似性測度，而唔包含已知類別嘅樣本質量。\n問題：以下唔屬於影響聚類算法結果嘅主要因素有\nA. 特徵揀\nB. 分類準則\nC. 模式相似性測度\nD. 已知類別嘅樣本質量\n答案係: \n聚類算法嘅主要因素有特徵揀、分類準則、模式相似性測度，而唔包含已知類別嘅樣本質量。\n問題：以下唔屬於影響聚類算法結果嘅主要因素有\nA. 特徵揀\nB. 分類準則\nC. 模式相似性測度\nD. 已知類別嘅樣本質量\n答案係: \n聚類算法嘅主要因素有特徵揀、分類準則、模式相似性測度，而唔包含已知類別嘅樣本質量。\n問題：以下唔屬於影響聚類算法結果嘅主要因素有\nA. 特徵揀\nB. 分類準則\nC. 模式相似性測度\nD. 已知類別嘅樣本質量\n答案係: \n聚類算法嘅主要因素有特徵揀、分類準則、模式相似性測度，而唔包含已知類別嘅樣本質量。\n問題：以下唔屬於影響聚類算法結果嘅主要因素有\nA. 特徵揀\nB. 分類準則\nC. 模式相似性測度\nD. 已知類別嘅樣本質量\n答案係: \n聚類算法嘅主要因素有特徵揀、分類準則、模式相似性測度，而唔包含已知類別嘅樣本質量。\n問題：以下唔屬於影響聚類算法結果嘅主要因素有",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '特徵揀'], ['B', '已知類別嘅樣本質量'], ['C', '分類準則'], ['D', '模式相似性測度']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "高斯混合模型(GMM)採用咩準則進行訓練",
    "llm_output": "高斯混合模型(GMM)係一種機器學習模型，用嚟對數據集進行分類。\nGMM 係採用期望最大化(EM)準則進行訓練，唔係採用均方誤差最小化(MSE)或經驗風險最小化(Risk)。",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '均方誤差最小化'], ['B', '經驗風險最小化'], ['C', '期望最大化'], ['D', '其他選項都唔係']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以下講法正確嘅係",
    "llm_output": "問題：以下講法正確嘅係\nA. 一個機器學習模型，如果有較高準確率，並唔難總係說明呢個分類器係好嘅\nB. 我哋唔可以使用聚類「類別id」作為一個新嘅特徵項， 然後再用監督學習分別進行學習\nC. 如果增加模型複雜度， 噉模型嘅測試錯誤率總係會降低\nD. 如果增加模型複雜度， 噉模型嘅訓練錯誤率總係會降低\n答案係: \n問題：以下講法正確嘅係\nA. 一個機器學習模型，如果有較高準確率，並唔難總係說明呢個分類器係好嘅\nB. 我哋唔可以使用聚類「類別id」作為一個新嘅特徵項， 然後再用監督學習分別進行學習\nC. 如果增加模型複雜度， 噉模型嘅測試錯誤率總係會降低\nD. 如果增加模型複雜度， 噉模型嘅訓練錯誤率總係會降低\n答案係: \n問題：以下講法正確嘅係\nA. 一個機器學習模型，如果有較高準確率，並唔難總係說明呢個分類器係好嘅\nB. 我哋唔可以使用聚類「類別id」作為一個新嘅特徵項， 然後再用監督學習分別進行學習\nC. 如果增加模型複雜度， 噉模型嘅測試錯誤率總係會降低\nD. 如果增加模型複雜度， 噉模型嘅訓練錯誤率總係會降低\n答案係: \n問題：以下講法正確嘅係\nA. 一個機器學習模型，如果有較高準確率，並唔難總係說明呢個分類器係好嘅\nB. 我哋唔可以使用聚類「類別id」作為一個新嘅特徵項， 然後再用監督學習分別進行學習\nC. 如果增加模型複雜度， 噉模型嘅測試錯誤率總係會降低\nD. 如果增加模型複雜度， 噉模型嘅訓練錯誤率總係會降低\n答案係: \n問題：以下講法正確嘅係\nA. 一個機器學習模型，如果有較高準確率，並唔難總係說明呢個分類器係好嘅\nB. 我哋唔可以使用聚類「類別id」作為一個新嘅特徵項， 然後再用監督學習分別進行學習\nC. 如果增加模型複雜度， 噉模型嘅測試錯誤率總係會降低\nD. 如果增加模型複雜度， 噉模型嘅訓練錯誤率總係會降低\n答案係: \n問題：以下講法正確嘅係\nA. 一個機器學習模型，如果有較高準確率，並唔難總係說明呢個分類器係好嘅\nB. 我哋唔可以使用聚類「類別id」作為一個新嘅特徵項， 然後再用監督學習分別進行學習\nC. 如果增加模型複雜度， 噉模型嘅測試錯誤率總係會降低\nD. 如果增加模型複雜度， 噉模型嘅訓練錯誤率總係會降低\n答案係: \n問題：以下講法正確嘅係\nA. 一個機器學習模型，如果有較高準確率，並唔難總係說明呢個分類器係好嘅\nB. 我哋唔可以使用聚類「類別id」作為一個新嘅特徵項， 然後再用監督學習分別進行學習\nC. 如果增加模型複雜度， 噉模型嘅測試錯誤率總係會降低\nD. 如果增加模型複雜度， 噉模型嘅訓練錯誤率總係會降低\n答案係: \n問題：以下講法正確嘅係\nA. 一個機器學習模型，如果有較高準確率，並�",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '一個機器學習模型，如果有較高準確率，並唔難總係說明呢個分類器係好嘅'], ['B', '我哋唔可以使用聚類「類別id」作為一個新嘅特徵項， 然後再用監督學習分別進行學習'], ['C', '如果增加模型複雜度， 噉模型嘅測試錯誤率總係會降低'], ['D', '如果增加模型複雜度， 噉模型嘅訓練錯誤率總係會降低']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "中文同義詞替換時，常用到Word2Vec，以下講法錯誤嘅係",
    "llm_output": "Word2Vec係一個機器學習模型，用嚟做詞彙向量化，嚟咗之後可以喺詞彙之間建立關聯，例如「蘋果」同「橘子」，「蘋果」同「蘋果汁」，「蘋果」同「蘋果電腦」，「蘋果」同「蘋果手機」，「蘋果」同「蘋果日報」等等，都係同義詞。\nWord2Vec嘅訓練過程係用咗概率統計，但唔係喺當前預料環境下，而係喺訓練語料嘅數量同質量限制下，所以Word2Vec嘅結果都係有限制嘅。",
    "correct_answer": "B",
    "standard_answer_range": "[['A', 'Word2Vec結果符合當前預料環境'], ['B', 'Word2Vec得到嘅都係語義上嘅同義詞'], ['C', 'Word2Vec受限於訓練語料嘅數量同質量'], ['D', 'Word2Vec基於概率統計']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "喺以下唔同嘅場景中,使用嘅分析方法唔正確嘅有",
    "llm_output": "A. 根據商家最近一年嘅經營及服務數據,用聚類算法判斷出天貓商家喺各自主營類目下所屬嘅商家層級\nB. 根據商家近幾年嘅成交數據,用聚類算法擬合出用戶未來一個月可能嘅消費金額公式\nC. 用關聯規則算法分析出購買咗汽車坐墊嘅買家,係否適合推薦汽車腳墊\nD. 根據用戶最近購買嘅商品信息,用決策樹算法識別出淘寶買家可能係男定係女\n答案係: \nA. 根據商家最近一年嘅經營及服務數據,用聚類算法判斷出天貓商家喺各自主營類目下所屬嘅商家層級\nB. 根據商家近幾年嘅成交數據,用聚類算法擬合出用戶未來一個月可能嘅消費金額公式\nC. 用關聯規則算法分析出購買咗汽車坐墊嘅買家,係否適合推薦汽車腳墊\nD. 根據用戶最近購買嘅商品信息,用決策樹算法識別出淘寶買家可能係男定係女\n答案係: \nA. 根據商家最近一年嘅經營及服務數據,用聚類算法判斷出天貓商家喺各自主營類目下所屬嘅商家層級\nB. 根據商家近幾年嘅成交數據,用聚類算法擬合出用戶未來一個月可能嘅消費金額公式\nC. 用關聯規則算法分析出購買咗汽車坐墊嘅買家,係否適合推薦汽車腳墊\nD. 根據用戶最近購買嘅商品信息,用決策樹算法識別出淘寶買家可能係男定係女\n答案係: \nA. 根據商家最近一年嘅經營及服務數據,用聚類算法判斷出天貓商家喺各自主營類目下所屬嘅商家層級\nB. 根據商家近幾年嘅成交數據,用聚類算法擬合出用戶未來一個月可能嘅消費金額公式\nC. 用關聯規則算法分析出購買咗汽車坐墊嘅買家,係否適合推薦汽車腳墊\nD. 根據用戶最近購買嘅商品信息,用決策樹算法識別出淘寶買家可能係男定係女\n答案係: \nA. 根據商家最近一年嘅經營及服務數據,用聚類算法判斷出天貓商家喺各自主營類目下所屬嘅商家層級\nB. 根據商家近幾年嘅成交數據,用聚類算法擬合出用戶未來一個月可能嘅消費金額公式\nC. 用關聯規則算法分析出購買咗汽車坐墊嘅買家,係否適合推薦汽車腳墊\nD. 根據用戶最近購買嘅商品信息,用決策樹算法識別出淘寶買家可能係男定係女\n答案係: \nA. 根據商家最近一年嘅經營及服務數據,用聚類算法判斷出天貓商家喺各自主營類目下所屬嘅商家層級\nB. 根據商家近幾年嘅成交數據,用聚類算法擬合出用戶未來一個月可能嘅消費金額公式\nC. 用關聯規則算法分析出購買咗汽車坐墊嘅買家,係否適合推薦汽車腳墊\nD. 根據用戶最近購買嘅商品信息,用決策樹算法識別出淘寶買家可能係男定係女\n答案係: \nA. 根據商家最近一年嘅經營及服務數據,用聚類算法判斷出天貓商家喺各自主營類目下所屬嘅商家層級\nB. 根據商家近幾年嘅成交數據,用聚類算法擬合出用戶未來一個月可能嘅消費金額公式\nC. 用關聯規則算法",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '根據商家最近一年嘅經營及服務數據,用聚類算法判斷出天貓商家喺各自主營類目下所屬嘅商家層級'], ['B', '根據商家近幾年嘅成交數據,用聚類算法擬合出用戶未來一個月可能嘅消費金額公式'], ['C', '用關聯規則算法分析出購買咗汽車坐墊嘅買家,係否適合推薦汽車腳墊'], ['D', '根據用戶最近購買嘅商品信息,用決策樹算法識別出淘寶買家可能係男定係女']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "喺數據清理中，下面邊個唔係處理缺失值嘅方法",
    "llm_output": "問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: \n問題：下面哪個不是機器學習的目標？\nA. 預測\nB. 解釋\nC. 推論\nD. 預測\n答案係: ",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '變量刪除'], ['B', '估算'], ['C', '整例刪除'], ['D', '成對刪除']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以下關於LDA(Latent Dirichlet allocation)嘅講法錯誤嘅係",
    "llm_output": "問題：以下關於K-means嘅講法錯誤嘅係\nA. K-means係非監督學習技術\nB. K-means係通過EM嘅思想求解\nC. 當揀一個聚類後，對於該聚類內嘅所有點嘅分佈係確定嘅\nD. K-means包含詞，主題同文檔三層結構\n答案係: \n問題：以下關於EM嘅講法錯誤嘅係\nA. EM係非監督學習技術\nB. EM係通過EM嘅思想求解\nC. 當揀一個聚類後，對於該聚類內嘅所有點嘅分佈係確定嘅\nD. EM包含詞，主題同文檔三層結構\n答案係: \n問題：以下關於SVM嘅講法錯誤嘅係\nA. SVM係非監督學習技術\nB. SVM係通過EM嘅思想求解\nC. 當揀一個聚類後，對於該聚類內嘅所有點嘅分佈係確定嘅\nD. SVM包含詞，主題同文檔三層結構\n答案係: \n問題：以下關於PCA嘅講法錯誤嘅係\nA. PCA係非監督學習技術\nB. PCA係通過EM嘅思想求解\nC. 當揀一個聚類後，對於該聚類內嘅所有點嘅分佈係確定嘅\nD. PCA包含詞，主題同文檔三層結構\n答案係: \n問題：以下關於SVD嘅講法錯誤嘅係\nA. SVD係非監督學習技術\nB. SVD係通過EM嘅思想求解\nC. 當揀一個聚類後，對於該聚類內嘅所有點嘅分佈係確定嘅\nD. SVD包含詞，主題同文檔三層結構\n答案係: \n問題：以下關於LDA嘅講法錯誤嘅係\nA. LDA係非監督學習技術\nB. LDA係通過EM嘅思想求解\nC. 當揀一個聚類後，對於該聚類內嘅所有點嘅分佈係確定嘅\nD. LDA包含詞，主題同文檔三層結構\n答案係: \n問題：以下關於K-means嘅講法錯誤嘅係\nA. K-means係非監督學習技術\nB. K-means係通過EM嘅思想求解\nC. 當揀一個聚類後，對於該聚類內嘅所有點嘅分佈係確定嘅\nD. K-means包含詞，主題同文檔三層結構\n答案係: \n問題：以下關於EM嘅講法錯誤嘅係\nA. EM係非監督學習技術\nB. EM係通過EM嘅思想求解\nC. 當揀一個聚類後，對於該聚類內嘅所有點嘅分佈係確定嘅\nD. EM包含詞，主題同文檔三層結構\n答案係: \n問題：以下關於SVM嘅講法錯誤嘅係\nA. SVM係非監督學習技術\nB. SVM係通過EM嘅思想求解\nC. 當揀一個聚類後，對於該聚類內嘅所有點嘅分佈係確定嘅\nD. SVM包含詞，主題同文檔三層結構\n答案係: \n問題：以下關於PCA嘅講法錯誤嘅係\nA. PCA係非監督學習技術\nB. PCA係通過EM嘅思想求解\nC. 當揀一個聚類後，對於該聚類內嘅所有點嘅分佈係確定嘅\nD. PCA包含詞，主題同文檔三層結構\n答案係: \n問題：以下關於SVD嘅講法錯誤嘅係\nA. SVD係非監督學習技術\nB. SVD係通過EM嘅思想求解\nC. 當揀一個聚類後，對於該聚類內嘅所有點嘅",
    "correct_answer": "C",
    "standard_answer_range": "[['A', 'LDA係非監督學習技術'], ['B', 'LDA可通過EM嘅思想求解'], ['C', '當揀一篇文檔後，對於該文檔主題嘅分佈係確定嘅'], ['D', 'LDA包含詞，主題同文檔三層結構']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "如果我使用數據集嘅全部特徵並且能夠達到100%嘅準確率，但喺測試集上僅能達到70%左右，呢說明：",
    "llm_output": "B. 欠擬合\n欠擬合係指模型未能充分捕捉訓練數據嘅特徵，導致模型喺訓練集上表現良好，但喺測試集上表現不佳。\n欠擬合嘅原因可能包括：\n特徵數量不足：如果訓練數據嘅特徵數量不足，模型可能無法充分捕捉數據嘅特徵。\n特徵之間嘅關聯性不足：如果特徵之間嘅關聯性不足，模型可能無法充分捕捉數據嘅特徵。\n特徵嘅範圍不足：如果特徵嘅範圍不足，模型可能無法充分捕捉數據嘅特徵。\n欠擬合嘅解決方法包括：\n增加特徵數量：如果特徵數量不足，可以增加特徵數量，以提高模型對數據嘅捕捉能力。\n增加特徵之間嘅關聯性：如果特徵之間嘅關聯性不足，可以增加特徵之間嘅關聯性，以提高模型對數據嘅捕捉能力。\n擴展特徵嘅範圍：如果特徵嘅範圍不足，可以擴展特徵嘅範圍，以提高模型對數據嘅捕捉能力。",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '以上均唔正確'], ['B', '欠擬合'], ['C', '過擬合'], ['D', '模型好棒']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。 當你使用較大嘅C（C趨於無窮），則：",
    "llm_output": "問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。 當你使用較小嘅C（C趨於零），則：\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。 當你使用較小嘅C（C趨於零），則：\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。 當你使用較大嘅C（C趨於無窮），則：\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。 當你使用較小嘅C（C趨於零），則：\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。 當你使用較大嘅C（C趨於無窮），則：\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。 當你使用較小嘅C（C趨於零），則：\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。 當你使用較大嘅C（C趨於無窮），則：\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。 當你使用較小嘅C（C趨於零），則：\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。 當你使用較大嘅C（C趨於無窮），則：\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。 當你使用較小嘅C（C趨於零），則：\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。 當你使用較大嘅C（C趨於無窮），則：\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。 當你使用較小嘅C（C趨於零），則：\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用鬆弛變量C作為超參之一。 當你使用較大嘅C（C趨於無窮），則：\n問題：假定你使用SVM學習數據X，數據X入面有啲點存在錯誤。依家如果你使用一個二次核函數，多項式階數為2，使用",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '以上均唔正確'], ['B', '唔確定'], ['C', '唔能正確分類'], ['D', '仲能正確分類數據']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "基於統計嘅分詞方法為",
    "llm_output": "正向量最大匹配法\n正向量最大匹配法（forward-maximum-matching algorithm）係一種分詞方法，佢係正向分詞嘅一種。\n正向量最大匹配法嘅原理係：\n佢係從左至右逐個字嘅檢查，如果佢係一個詞，就加入詞典中，如果佢係一個詞嘅首字，就加入詞典中，如果佢係一個詞嘅尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個詞嘅首尾字，就加入詞典中，如果佢係一個詞嘅中間字，就加入詞典中，如果佢係一個",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '正向量最大匹配法'], ['B', '條件隨機場'], ['C', '最少切分'], ['D', '逆向量最大匹配法']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "假定某同學使用樸素貝葉斯分類模型時，唔小心將訓練數據嘅兩個維度搞重複咗，噉關於樸素貝葉斯嘅講法中唔正確嘅係",
    "llm_output": "A",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '模型效果相比無重複特徵嘅情況下精確度會降低'], ['B', '模型效果相比無重複特徵嘅情況下精確度會提高'], ['C', '當兩列特徵高度相關時，無法用兩列特徵相同時所得到嘅結論嚟分析問題'], ['D', '如果所有特徵都被重複一遍，得到嘅模型預測結果相對於唔重複嘅情況下嘅模型預測結果一樣']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列關於word2vec嘅講法中錯誤嘅係",
    "llm_output": "問題：下列關於word2vec嘅講法中錯誤嘅係\nA. 使用詞向量可得到以下等式:King - man + woman = Queen\nB. Skip-gram係畀定詞窗中嘅文本，預測當前詞嘅概率\nC. word2vec嘅假設係詞袋模型，詞嘅順序係唔重要嘅\nD. word2vec訓練中使用咗Negative Sample與Hierarchical Softmax兩種加速算法\n答案係: \n問題：下列關於word2vec嘅講法中錯誤嘅係\nA. 使用詞向量可得到以下等式:King - man + woman = Queen\nB. Skip-gram係畀定詞窗中嘅文本，預測當前詞嘅概率\nC. word2vec嘅假設係詞袋模型，詞嘅順序係唔重要嘅\nD. word2vec訓練中使用咗Negative Sample與Hierarchical Softmax兩種加速算法\n答案係: \n問題：下列關於word2vec嘅講法中錯誤嘅係\nA. 使用詞向量可得到以下等式:King - man + woman = Queen\nB. Skip-gram係畀定詞窗中嘅文本，預測當前詞嘅概率\nC. word2vec嘅假設係詞袋模型，詞嘅順序係唔重要嘅\nD. word2vec訓練中使用咗Negative Sample與Hierarchical Softmax兩種加速算法\n答案係: \n問題：下列關於word2vec嘅講法中錯誤嘅係\nA. 使用詞向量可得到以下等式:King - man + woman = Queen\nB. Skip-gram係畀定詞窗中嘅文本，預測當前詞嘅概率\nC. word2vec嘅假設係詞袋模型，詞嘅順序係唔重要嘅\nD. word2vec訓練中使用咗Negative Sample與Hierarchical Softmax兩種加速算法\n答案係: \n問題：下列關於word2vec嘅講法中錯誤嘅係\nA. 使用詞向量可得到以下等式:King - man + woman = Queen\nB. Skip-gram係畀定詞窗中嘅文本，預測當前詞嘅概率\nC. word2vec嘅假設係詞袋模型，詞嘅順序係唔重要嘅\nD. word2vec訓練中使用咗Negative Sample與Hierarchical Softmax兩種加速算法\n答案係: \n問題：下列關於word2vec嘅講法中錯誤嘅係\nA. 使用詞向量可得到以下等式:King - man + woman = Queen\nB. Skip-gram係畀定詞窗中嘅文本，預測當前詞嘅概率\nC. word2vec嘅假設係詞袋模型，詞嘅順序係唔重要嘅\nD. word2vec訓練中使用咗Negative Sample與Hierarchical Softmax兩種加速算法\n答案係: \n問題：下列關於word2vec嘅講法中錯誤嘅係\nA. 使用詞向量可得到以下等式:King - man + woman = Queen\nB. Skip-gram係畀定詞窗中嘅文本，預測當前詞嘅概率\nC. word2vec嘅假設係詞袋模型，詞嘅順序係唔重要嘅\nD. word2vec訓練中使用咗Negative Sample與Hierarchical Softmax兩種加速算法\n答案係: \n問題：下列關於word2vec嘅講法中錯誤嘅係\nA. 使用詞向量可得到以下等式:King - man + woman = Queen\nB. Skip-gram係畀定詞窗中嘅文本，預測當前詞嘅概率\nC. word2vec嘅假設係詞袋模型，詞嘅順序係唔重要嘅\nD. word2vec訓練中使用咗Negative Sample與Hierarchical Softmax兩種",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '使用詞向量可得到以下等式:King - man + woman = Queen'], ['B', 'Skip-gram係畀定詞窗中嘅文本，預測當前詞嘅概率'], ['C', 'word2vec嘅假設係詞袋模型，詞嘅順序係唔重要嘅'], ['D', 'word2vec訓練中使用咗Negative Sample與Hierarchical Softmax兩種加速算法']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以下描述正確嘅係",
    "llm_output": "A. 聚類分析可以睇作係一種非監督嘅分類。\nB. 喺聚類分析當中，簇內嘅相似性越大，簇間嘅差別越大，聚類嘅效果就越差。\nC. SVM係噉樣一個分類器，佢搵具有最細邊緣嘅超平面，噉所以佢都經常被稱為最細邊緣分類器\nD. 喺決策樹中，隨住樹中節點數變得太大，即使模型嘅訓練誤差仲喺繼續減低，但係檢驗誤差開始增大，呢係出現咗模型擬合不足嘅問題。",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '聚類分析可以睇作係一種非監督嘅分類。'], ['B', '喺聚類分析當中，簇內嘅相似性越大，簇間嘅差別越大，聚類嘅效果就越差。'], ['C', 'SVM係噉樣一個分類器，佢搵具有最細邊緣嘅超平面，噉所以佢都經常被稱為最細邊緣分類器'], ['D', '喺決策樹中，隨住樹中節點數變得太大，即使模型嘅訓練誤差仲喺繼續減低，但係檢驗誤差開始增大，呢係出現咗模型擬合不足嘅問題。']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列選項中，識別模式與其他唔一樣嘅係",
    "llm_output": "問題：下列選項中，識別模式與其他唔一樣嘅係\nA. 出行方式判斷:步行、踩單車、坐車\nB. 用戶年齡分佈判斷:少年、青年、中年、老年\nC. 派信員分揀信件\nD. 醫生畀病人診斷發病類型\n答案係: \n問題：下列選項中，識別模式與其他唔一樣嘅係\nA. 出行方式判斷:步行、踩單車、坐車\nB. 用戶年齡分佈判斷:少年、青年、中年、老年\nC. 派信員分揀信件\nD. 醫生畀病人診斷發病類型\n答案係: \n問題：下列選項中，識別模式與其他唔一樣嘅係\nA. 出行方式判斷:步行、踩單車、坐車\nB. 用戶年齡分佈判斷:少年、青年、中年、老年\nC. 派信員分揀信件\nD. 醫生畀病人診斷發病類型\n答案係: \n問題：下列選項中，識別模式與其他唔一樣嘅係\nA. 出行方式判斷:步行、踩單車、坐車\nB. 用戶年齡分佈判斷:少年、青年、中年、老年\nC. 派信員分揀信件\nD. 醫生畀病人診斷發病類型\n答案係: \n問題：下列選項中，識別模式與其他唔一樣嘅係\nA. 出行方式判斷:步行、踩單車、坐車\nB. 用戶年齡分佈判斷:少年、青年、中年、老年\nC. 派信員分揀信件\nD. 醫生畀病人診斷發病類型\n答案係: \n問題：下列選項中，識別模式與其他唔一樣嘅係\nA. 出行方式判斷:步行、踩單車、坐車\nB. 用戶年齡分佈判斷:少年、青年、中年、老年\nC. 派信員分揀信件\nD. 醫生畀病人診斷發病類型\n答案係: \n問題：下列選項中，識別模式與其他唔一樣嘅係\nA. 出行方式判斷:步行、踩單車、坐車\nB. 用戶年齡分佈判斷:少年、青年、中年、老年\nC. 派信員分揀信件\nD. 醫生畀病人診斷發病類型\n答案係: \n問題：下列選項中，識別模式與其他唔一樣嘅係\nA. 出行方式判斷:步行、踩單車、坐車\nB. 用戶年齡分佈判斷:少年、青年、中年、老年\nC. 派信員分揀信件\nD. 醫生畀病人診斷發病類型\n答案係: \n問題：下列選項中，識別模式與其他唔一樣嘅係\nA. 出行方式判斷:步行、踩單車、坐車\nB. 用戶年齡分佈判斷:少年、青年、中年、老年\nC. 派信員分揀信件\nD. 醫生畀病人診斷發病類型\n答案係: \n問題：下列選項中，識別模式與其他唔一樣嘅係\nA. 出行方式判斷:步行、踩單車、坐車\nB. 用戶年齡分佈判斷:少年、青年、中年、老年\nC. 派信員分揀信件\nD. 醫生畀病人診斷發病類型\n答案係: \n問題：下列選項中，識別模式與其他唔一樣嘅係\nA. 出行方式判斷:步行、踩單車、坐車\nB. 用戶年齡分佈判斷:少年、青年、中年、老年\nC. 派信員分揀信件\nD. 醫生畀病人診斷發病類型\n答案係: \n問題：下列選項中，識別模式與其他唔一樣嘅係\nA. 出行方式判斷:步行、踩單車",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '出行方式判斷:步行、踩單車、坐車'], ['B', '用戶年齡分佈判斷:少年、青年、中年、老年'], ['C', '派信員分揀信件'], ['D', '醫生畀病人診斷發病類型']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列講法唔正確嘅係",
    "llm_output": "梯度下降法係利用當前位置嘅負梯度作為搜索方向嘅方法\n共軛梯度法僅需利用一階導數嘅信息，但係收斂速度高於梯度下降法\n批量梯度下降同隨機梯度下降相比，批量梯度下降優勢係對於大規模樣本效率好高\n牛頓法同梯度下降法相比，一個劣勢係求解複雜，一個優勢係收斂速度加快\n梯度下降法係利用當前位置嘅負梯度作為搜索方向嘅方法\n梯度下降法係利用當前位置嘅負梯度作為搜索方向嘅方法。\n共軛梯度法僅需利用一階導數嘅信息，但係收斂速度高於梯度下降法。\n批量梯度下降同隨機梯度下降相比，批量梯度下降優勢係對於大規模樣本效率好高。\n牛頓法同梯度下降法相比，一個劣勢係求解複雜，一個優勢係收斂速度加快。\n梯度下降法係利用當前位置嘅負梯度作為搜索方向嘅方法。\n梯度下降法係利用當前位置嘅負梯度作為搜索方向嘅方法。\n共軛梯度法僅需利用一階導數嘅信息，但係收斂速度高於梯度下降法。\n批量梯度下降同隨機梯度下降相比，批量梯度下降優勢係對於大規模樣本效率好高。\n牛頓法同梯度下降法相比，一個劣勢係求解複雜，一個優勢係收斂速度加快。\n梯度下降法係利用當前位置嘅負梯度作為搜索方向嘅方法。\n梯度下降法係利用當前位置嘅負梯度作為搜索方向嘅方法。\n共軛梯度法僅需利用一階導數嘅信息，但係收斂速度高於梯度下降法。\n批量梯度下降同隨機梯度下降相比，批量梯度下降優勢係對於大規模樣本效率好高。\n牛頓法同梯度下降法相比，一個劣勢係求解複雜，一個優勢係收斂速度加快。\n梯度下降法係利用當前位置嘅負梯度作為搜索方向嘅方法。\n梯度下降法係利用當前位置嘅負梯度作為搜索方向嘅方法。\n共軛梯度法僅需利用一階導數嘅信息，但係收斂速度高於梯度下降法。\n批量梯度下降同隨機梯度下降相比，批量梯度下降優勢係對於大規模樣本效率好高。\n牛頓法同梯度下降法相比，一個劣勢係求解複雜，一個優勢係收斂速度加快。\n梯度下降法係利用當前位置嘅負梯度作為搜索方向嘅方法。\n梯度下降法係利用當前位置嘅負梯度作為搜索方向嘅方法。\n共軛梯度法僅需利用一階導數嘅信息，但係收斂速度高於梯度下降法。\n批量梯度下降同隨機梯度下降相比，批量梯度下降優勢係對於大規模樣本效率好高。\n牛頓法同梯度下降法相比，一個劣勢係求解複雜，一個優勢係收斂速度加快。\n梯度下降法係利用當前位置嘅負梯度作為搜索方向嘅方法。\n梯度下降法係利用當前位置嘅負梯度作為搜索方向嘅方法。\n共軛梯度法僅需利用一階導數嘅信息，但係收斂速度高於梯度下降法。\n批量梯度下降同隨機梯度下降相比，批量梯度下降優勢係對於大規模樣本效率好高。\n牛頓法同梯度下降法相比，一個劣勢係求解複雜，一個優勢係收斂速度加快。\n梯度下降法係利用當前位置嘅負梯度作為搜索方向嘅方法。\n梯度下降法係利用當前位置嘅負梯度作為搜索方向嘅方法。\n共軛梯度法僅需利用一",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '梯度下降法係利用當前位置嘅負梯度作為搜索方向嘅方法'], ['B', '共軛梯度法僅需利用一階導數嘅信息，但係收斂速度高於梯度下降法'], ['C', '批量梯度下降同隨機梯度下降相比，批量梯度下降優勢係對於大規模樣本效率好高'], ['D', '牛頓法同梯度下降法相比，一個劣勢係求解複雜，一個優勢係收斂速度加快']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "喺隱馬爾科夫模型中,如果已知觀察序列同產生觀察序列嘅狀態序列,噉可用以下邊種方法直接進行參數估計",
    "llm_output": "隱馬爾科夫模型中,如果已知觀察序列同產生觀察序列嘅狀態序列,噉可用以下邊種方法直接進行參數估計\nA. 前向後向算法\nB. 極大似然估計\nC. 維特比算法\nD. EM算法\n答案係: C\n維特比算法係一種喺隱馬爾科夫模型中用嚟估計參數嘅算法。\n參考資料:\n維基百科嘅條目: 維特比算法\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科夫模型\n隱馬爾科",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '前向後向算法'], ['B', '極大似然估計'], ['C', '維特比算法'], ['D', 'EM算法']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "LDA(Latent Dirichlet allocation)中歸屬於同一主題下單詞分佈嘅先驗分佈係?",
    "llm_output": "LDA(Latent Dirichlet allocation)中歸屬於同一主題下單詞分佈嘅先驗分佈係狄利克雷分佈。\nLDA(Latent Dirichlet allocation)中歸屬於同一主題下單詞分佈嘅先驗分佈係狄利克雷分佈。",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '正態分佈'], ['B', '狄利克雷分佈'], ['C', '多項分佈'], ['D', '二項分佈']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "線性回歸嘅基本假設唔包括邊個",
    "llm_output": "A. 對於解釋變量嘅所有觀測值，隨機誤差項有相同嘅方差\nB. 隨機誤差項係一個期望值為0嘅隨機變量\nC. 隨機誤差項服從正態分佈\nD. 隨機誤差項彼此相關\n機器學習嘅基本假設\n線性回歸嘅基本假設\n線性回歸嘅基本假設包括：\n隨機誤差項係一個期望值為0嘅隨機變量\n隨機誤差項服從正態分佈\n隨機誤差項彼此相關\n對於解釋變量嘅所有觀測值，隨機誤差項有相同嘅方差\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括嘅。\n呢個假設係唔包括�",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '對於解釋變量嘅所有觀測值，隨機誤差項有相同嘅方差'], ['B', '隨機誤差項係一個期望值為0嘅隨機變量'], ['C', '隨機誤差項服從正態分佈'], ['D', '隨機誤差項彼此相關']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列唔係SVM核函數嘅係",
    "llm_output": "問題：下列唔係SVM嘅優點\nA. 唔需要訓練資料\nB. 可以處理多變量資料\nC. 可以處理非線性資料\nD. 可以處理高維資料\n答案係: \n問題：下列唔係SVM嘅缺點\nA. 唔可以處理多變量資料\nB. 唔可以處理非線性資料\nC. 唔可以處理高維資料\nD. 唔可以處理多變量資料同非線性資料\n答案係: \n問題：下列唔係SVM嘅訓練過程\nA. 先係訓練資料上建立一個超平面\nB. 再係訓練資料上建立一個超曲面\nC. 再係訓練資料上建立一個超體\nD. 再係訓練資料上建立一個超球\n答案係: \n問題：下列唔係SVM嘅訓練過程\nA. 先係訓練資料上建立一個超平面\nB. 再係訓練資料上建立一個超曲面\nC. 再係訓練資料上建立一個超體\nD. 再係訓練資料上建立一個超球\n答案係: \n問題：下列唔係SVM嘅訓練過程\nA. 先係訓練資料上建立一個超平面\nB. 再係訓練資料上建立一個超曲面\nC. 再係訓練資料上建立一個超體\nD. 再係訓練資料上建立一個超球\n答案係: \n問題：下列唔係SVM嘅訓練過程\nA. 先係訓練資料上建立一個超平面\nB. 再係訓練資料上建立一個超曲面\nC. 再係訓練資料上建立一個超體\nD. 再係訓練資料上建立一個超球\n答案係: \n問題：下列唔係SVM嘅訓練過程\nA. 先係訓練資料上建立一個超平面\nB. 再係訓練資料上建立一個超曲面\nC. 再係訓練資料上建立一個超體\nD. 再係訓練資料上建立一個超球\n答案係: \n問題：下列唔係SVM嘅訓練過程\nA. 先係訓練資料上建立一個超平面\nB. 再係訓練資料上建立一個超曲面\nC. 再係訓練資料上建立一個超體\nD. 再係訓練資料上建立一個超球\n答案係: \n問題：下列唔係SVM嘅訓練過程\nA. 先係訓練資料上建立一個超平面\nB. 再係訓練資料上建立一個超曲面\nC. 再係訓練資料上建立一個超體\nD. 再係訓練資料上建立一個超球\n答案係: \n問題：下列唔係SVM嘅訓練過程\nA. 先係訓練資料上建立一個超平面\nB. 再係訓練資料上建立一個超曲面\nC. 再係訓練資料上建立一個超體\nD. 再係訓練資料上建立一個超球\n答案係: \n問題：下列唔係SVM嘅訓練過程\nA. 先係訓練資料上建立一個超平面\nB. 再係訓練資料上建立一個超曲面\nC. 再係訓練資料上建立一個超體\nD. 再係訓練資料上建立一個超球\n答案係: \n問題：下列唔係SVM嘅訓練過程\nA. 先係訓練資料上建立一個超平面\nB. 再係訓練資料上建立一個超曲面\nC. 再係訓練資料上建立一個超體\nD. 再係訓練資料上建立一個超球\n答案係: \n問題：下列唔係SVM嘅訓練過程\nA. 先係訓練資料上建立一個超平面\nB. 再係訓練資料上建立一個超曲面\nC. 再係訓練資料上建立一個超體\nD. 再係訓練資料上建立一個超球\n答案係: \n問題：下列唔係SVM嘅訓練過程\nA. 先係訓練資料上建立一個超平面\nB. 再係訓練資料上建立一個超曲面\nC. 再係訓練資料上建立一個超體\nD. 再係訓練資料上建立一個超球\n答案係: \n問題：下列唔係SVM嘅訓練",
    "correct_answer": "D",
    "standard_answer_range": "[['A', 'Sigmoid核函數'], ['B', '徑向基核函數'], ['C', '多項式核函數'], ['D', 'logistic核函數']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下列邊啲係非監督學習方法",
    "llm_output": "問題：下列邊啲係監督學習方法\nA. KNN\nB. SVM\nC. 決策樹\nD. K-means\n答案係: \n問題：下列邊啲係非監督學習方法\nA. 決策樹\nB. K-means\nC. KNN\nD. SVM\n答案係: \n問題：下列邊啲係監督學習方法\nA. KNN\nB. SVM\nC. 決策樹\nD. K-means\n答案係: \n問題：下列邊啲係非監督學習方法\nA. 決策樹\nB. K-means\nC. KNN\nD. SVM\n答案係: \n問題：下列邊啲係監督學習方法\nA. KNN\nB. SVM\nC. 決策樹\nD. K-means\n答案係: \n問題：下列邊啲係非監督學習方法\nA. 決策樹\nB. K-means\nC. KNN\nD. SVM\n答案係: \n問題：下列邊啲係監督學習方法\nA. KNN\nB. SVM\nC. 決策樹\nD. K-means\n答案係: \n問題：下列邊啲係非監督學習方法\nA. 決策樹\nB. K-means\nC. KNN\nD. SVM\n答案係: \n問題：下列邊啲係監督學習方法\nA. KNN\nB. SVM\nC. 決策樹\nD. K-means\n答案係: \n問題：下列邊啲係非監督學習方法\nA. 決策樹\nB. K-means\nC. KNN\nD. SVM\n答案係: \n問題：下列邊啲係監督學習方法\nA. KNN\nB. SVM\nC. 決策樹\nD. K-means\n答案係: \n問題：下列邊啲係非監督學習方法\nA. 決策樹\nB. K-means\nC. KNN\nD. SVM\n答案係: \n問題：下列邊啲係監督學習方法\nA. KNN\nB. SVM\nC. 決策樹\nD. K-means\n答案係: \n問題：下列邊啲係非監督學習方法\nA. 決策樹\nB. K-means\nC. KNN\nD. SVM\n答案係: \n問題：下列邊啲係監督學習方法\nA. KNN\nB. SVM\nC. 決策樹\nD. K-means\n答案係: \n問題：下列邊啲係非監督學習方法\nA. 決策樹\nB. K-means\nC. KNN\nD. SVM\n答案係: \n問題：下列邊啲係監督學習方法\nA. KNN\nB. SVM\nC. 決策樹\nD. K-means\n答案係: \n問題：下列邊啲係非監督學習方法\nA. 決策樹\nB. K-means\nC. KNN\nD. SVM\n答案係: \n問題：下列邊啲係監督學習方法\nA. KNN\nB. SVM\nC. 決策樹\nD. K-means\n答案係: \n問題：下列邊啲係非監督學習方法\nA. 決策樹\nB. K-means\nC. KNN\nD. SVM\n答案係: \n問題：下列邊啲係監督學習方法\nA. KNN\nB. SVM\nC. 決策樹\nD. K-means\n答案係: \n問題：下列邊啲係非監督學習方法\nA. 決策樹\nB. K-means\nC. KNN\nD. SVM\n答案係: \n問題：下列邊啲係監督學習方法\nA. KNN\nB. SVM\nC. 決策樹\nD. K-means\n答案係: \n問題：下列邊啲係非監督學習方法\nA. 決策樹\nB. K-means\nC. KNN\nD. SVM\n答案係: \n問題：下列邊啲係監督學習方法\nA. KNN\nB. SVM\nC. 決策樹\nD. K-means\n答案係: \n問題：下列邊啲係非監督學習",
    "correct_answer": "B",
    "standard_answer_range": "[['A', 'SVM'], ['B', 'K-means'], ['C', 'KNN'], ['D', '決策樹']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以下邊種方法屬於判別式模型",
    "llm_output": "問題：以下邊種方法屬於生成式模型\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於無監督學習\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於有監督學習\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於半監督學習\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於強監督學習\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於有類型學習\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於無類型學習\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於線性模型\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於非線性模型\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於線性模型\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於非線性模型\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於線性模型\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於非線性模型\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於線性模型\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於非線性模型\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於線性模型\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於非線性模型\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於線性模型\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於非線性模型\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向量機\n答案係: \n問題：以下邊種方法屬於線性模型\nA. 樸素貝葉斯\nB. 隱馬模型\nC. 貝葉斯網絡\nD. 支持向",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '貝葉斯網絡'], ['B', '樸素貝葉斯'], ['C', '隱馬模型'], ['D', '支持向量機']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "下面嘅交叉驗證方法：i. 有放返嘅Bootstrap方法；ii. 留一個測試樣本嘅交叉驗證；iii. 5摺交叉驗證；iv. 重複兩次嘅5摺教程驗證。當樣本係1000時，下面執行時間嘅順序，正確嘅係",
    "llm_output": "問題：下面嘅交叉驗證方法：i. 有放返嘅Bootstrap方法；ii. 留一個測試樣本嘅交叉驗證；iii. 5摺交叉驗證；iv. 重複兩次嘅5摺教程驗證。當樣本係1000時，下面執行時間嘅順序，正確嘅係\nA. ii > iv > iii > i\nB. ii > iii > iv > i\nC. iv > i > ii > iii\nD. i > ii > iii > iv\n答案係: \n問題：下面嘅交叉驗證方法：i. 有放返嘅Bootstrap方法；ii. 留一個測試樣本嘅交叉驗證；iii. 5摺交叉驗證；iv. 重複兩次嘅5摺教程驗證。當樣本係1000時，下面執行時間嘅順序，正確嘅係\nA. ii > iv > iii > i\nB. ii > iii > iv > i\nC. iv > i > ii > iii\nD. i > ii > iii > iv\n答案係: \n問題：下面嘅交叉驗證方法：i. 有放返嘅Bootstrap方法；ii. 留一個測試樣本嘅交叉驗證；iii. 5摺交叉驗證；iv. 重複兩次嘅5摺教程驗證。當樣本係1000時，下面執行時間嘅順序，正確嘅係\nA. ii > iv > iii > i\nB. ii > iii > iv > i\nC. iv > i > ii > iii\nD. i > ii > iii > iv\n答案係: \n問題：下面嘅交叉驗證方法：i. 有放返嘅Bootstrap方法；ii. 留一個測試樣本嘅交叉驗證；iii. 5摺交叉驗證；iv. 重複兩次嘅5摺教程驗證。當樣本係1000時，下面執行時間嘅順序，正確嘅係\nA. ii > iv > iii > i\nB. ii > iii > iv > i\nC. iv > i > ii > iii\nD. i > ii > iii > iv\n答案係: \n問題：下面嘅交叉驗證方法：i. 有放返嘅Bootstrap方法；ii. 留一個測試樣本嘅交叉驗證；iii. 5摺交叉驗證；iv. 重複兩次嘅5摺教程驗證。當樣本係1000時，下面執行時間嘅順序，正確嘅係\nA. ii > iv > iii > i\nB. ii > iii > iv > i\nC. iv > i > ii > iii\nD. i > ii > iii > iv\n答案係: \n問題：下面嘅交叉驗證方法：i. 有放返嘅Bootstrap方法；ii. 留一個測試樣本嘅交叉驗證；iii. 5摺交叉驗證；iv. 重複兩次嘅5摺教程驗證。當樣本係1000時，下面執行時間嘅順序，正確嘅係\nA. ii > iv > iii > i\nB. ii > iii > iv > i\nC. iv > i > ii > iii\nD. i > ii > iii > iv\n答案係: \n問題：下面嘅交叉驗證方法：i. 有放返嘅Bootstrap方法；ii. 留一個測試樣本嘅交叉驗證；iii. 5摺交叉驗證；iv. 重複兩次嘅5摺教程驗證。當樣本係1000時，下面執行時間嘅順序，正確嘅係\nA. ii > iv > iii > i\nB. ii > iii > iv > i\nC. iv > i > ii > iii\nD. i > ii > iii > iv\n答案係: \n問題：下面嘅交叉驗證方法：i. 有放返嘅Bootstrap方法；ii. 留一個測試樣本嘅交叉驗證；iii. 5摺交叉驗證；iv",
    "correct_answer": "A",
    "standard_answer_range": "[['A', 'ii > iv > iii > i'], ['B', 'ii > iii > iv > i'], ['C', 'iv > i > ii > iii'], ['D', 'i > ii > iii > iv']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "Seq2Seq模型喺解碼時可以揀用嘅方法",
    "llm_output": "問題：下列哪一個係機器學習嘅特徵\nA. 機器學習係一種統計學\nB. 機器學習係一種統計學\nC. 機器學習係一種統計學\nD. 機器學習係一種統計學\n答案係: \n問題：下列哪一個係機器學習嘅特徵\nA. 機器學習係一種統計學\nB. 機器學習係一種統計學\nC. 機器學習係一種統計學\nD. 機器學習係一種統計學\n答案係: \n問題：下列哪一個係機器學習嘅特徵\nA. 機器學習係一種統計學\nB. 機器學習係一種統計學\nC. 機器學習係一種統計學\nD. 機器學習係一種統計學\n答案係: \n問題：下列哪一個係機器學習嘅特徵\nA. 機器學習係一種統計學\nB. 機器學習係一種統計學\nC. 機器學習係一種統計學\nD. 機器學習係一種統計學\n答案係: \n問題：下列哪一個係機器學習嘅特徵\nA. 機器學習係一種統計學\nB. 機器學習係一種統計學\nC. 機器學習係一種統計學\nD. 機器學習係一種統計學\n答案係: \n問題：下列哪一個係機器學習嘅特徵\nA. 機器學習係一種統計學\nB. 機器學習係一種統計學\nC. 機器學習係一種統計學\nD. 機器學習係一種統計學\n答案係: \n問題：下列哪一個係機器學習嘅特徵\nA. 機器學習係一種統計學\nB. 機器學習係一種統計學\nC. 機器學習係一種統計學\nD. 機器學習係一種統計學\n答案係: \n問題：下列哪一個係機器學習嘅特徵\nA. 機器學習係一種統計學\nB. 機器學習係一種統計學\nC. 機器學習係一種統計學\nD. 機器學習係一種統計學\n答案係: \n問題：下列哪一個係機器學習嘅特徵\nA. 機器學習係一種統計學\nB. 機器學習係一種統計學\nC. 機器學習係一種統計學\nD. 機器學習係一種統計學\n答案係: \n問題：下列哪一個係機器學習嘅特徵\nA. 機器學習係一種統計學\nB. 機器學習係一種統計學\nC. 機器學習係一種統計學\nD. 機器學習係一種統計學\n答案係: \n問題：下列哪一個係機器學習嘅特徵\nA. 機器學習係一種統計學\nB. 機器學習係一種統計學\nC. 機器學習係一種統計學\nD. 機器學習係一種統計學\n答案係: \n問題：下列哪一個係機器學習嘅特徵\nA. 機器學習係一種統計學\nB. 機器學習係一種統計學\nC. 機器學習係一種統計學\nD. 機器學習係一種統計學\n答案係: \n問題：下列哪一個係機器學習嘅特徵\nA. 機器學習係一種統計學\nB. 機器學習係一種統計學\nC. 機器學習係一種統計學\nD. 機器學習係一種統計學\n答案係: \n問題：下列哪一個係機器學習嘅特徵\nA. 機器學習係一種統計學\nB. 機器學習係一種統計學\nC. 機器學習係一種統計學\nD. 機器學習係一種統計學\n答案係: \n問題：下列哪一個係機器學習嘅特徵\nA. 機器學習係一種統計學\nB. 機器學習係一種統計學\nC. 機器學習係一種統計學\nD. 機器學習係一種統計學\n答案係: \n問題：下列哪",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '貪心算法'], ['B', '二者均可'], ['C', 'Beam Search'], ['D', '二者均不可']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "解決隱馬模型中預測問題嘅算法係",
    "llm_output": "A. 前向算法\nB. 維特比算法\nC. Baum-Welch算法\nD. 後向算法\n隱馬模型中預測問題嘅算法係前向算法。\n隱馬模型中預測問題嘅算法係前向算法。",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '前向算法'], ['B', '維特比算法'], ['C', 'Baum-Welch算法'], ['D', '後向算法']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以下對k-means聚類算法解釋正確嘅係",
    "llm_output": "問題：以下對k-means聚類算法解釋正確嘅係\nA. 能自動識別類嘅個數, 唔係隨機挑選初始點為中心點計算\nB. 唔能自動識別類嘅個數, 唔係隨機挑選初始點為中心點計算\nC. 唔能自動識別類嘅個數, 隨機挑選初始點為中心點計算\nD. 能自動識別類嘅個數, 隨機挑選初始點為中心點計算\n答案係: \n問題：以下對k-means聚類算法解釋正確嘅係\nA. 能自動識別類嘅個數, 唔係隨機挑選初始點為中心點計算\nB. 唔能自動識別類嘅個數, 唔係隨機挑選初始點為中心點計算\nC. 唔能自動識別類嘅個數, 隨機挑選初始點為中心點計算\nD. 能自動識別類嘅個數, 隨機挑選初始點為中心點計算\n答案係: \n問題：以下對k-means聚類算法解釋正確嘅係\nA. 能自動識別類嘅個數, 唔係隨機挑選初始點為中心點計算\nB. 唔能自動識別類嘅個數, 唔係隨機挑選初始點為中心點計算\nC. 唔能自動識別類嘅個數, 隨機挑選初始點為中心點計算\nD. 能自動識別類嘅個數, 隨機挑選初始點為中心點計算\n答案係: \n問題：以下對k-means聚類算法解釋正確嘅係\nA. 能自動識別類嘅個數, 唔係隨機挑選初始點為中心點計算\nB. 唔能自動識別類嘅個數, 唔係隨機挑選初始點為中心點計算\nC. 唔能自動識別類嘅個數, 隨機挑選初始點為中心點計算\nD. 能自動識別類嘅個數, 隨機挑選初始點為中心點計算\n答案係: \n問題：以下對k-means聚類算法解釋正確嘅係\nA. 能自動識別類嘅個數, 唔係隨機挑選初始點為中心點計算\nB. 唔能自動識別類嘅個數, 唔係隨機挑選初始點為中心點計算\nC. 唔能自動識別類嘅個數, 隨機挑選初始點為中心點計算\nD. 能自動識別類嘅個數, 隨機挑選初始點為中心點計算\n答案係: \n問題：以下對k-means聚類算法解釋正確嘅係\nA. 能自動識別類嘅個數, 唔係隨機挑選初始點為中心點計算\nB. 唔能自動識別類嘅個數, 唔係隨機挑選初始點為中心點計算\nC. 唔能自動識別類嘅個數, 隨機挑選初始點為中心點計算\nD. 能自動識別類嘅個數, 隨機挑選初始點為中心點計算\n答案係: \n問題：以下對k-means聚類算法解釋正確嘅係\nA. 能自動識別類嘅個數, 唔係隨機挑選初始點為中心點計算\nB. 唔能自動識別類嘅個數, 唔係隨機挑選初始點為中心點計算\nC. 唔能自動識別類嘅個數, 隨機挑選初始點為中心點計算\nD. 能自動識別類嘅個數",
    "correct_answer": "C",
    "standard_answer_range": "[['A', '能自動識別類嘅個數, 唔係隨機挑選初始點為中心點計算'], ['B', '唔能自動識別類嘅個數, 唔係隨機挑選初始點為中心點計算'], ['C', '唔能自動識別類嘅個數, 隨機挑選初始點為中心點計算'], ['D', '能自動識別類嘅個數, 隨機挑選初始點為中心點計算']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "一般，k-NN最近鄰方法喺（）嘅情況下效果較好",
    "llm_output": "問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕駛\nB. 語音識別\nC. 圖像識別\nD. 網絡安全\n答案係: \n問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕駛\nB. 語音識別\nC. 圖像識別\nD. 網絡安全\n答案係: \n問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕駛\nB. 語音識別\nC. 圖像識別\nD. 網絡安全\n答案係: \n問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕駛\nB. 語音識別\nC. 圖像識別\nD. 網絡安全\n答案係: \n問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕駛\nB. 語音識別\nC. 圖像識別\nD. 網絡安全\n答案係: \n問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕駛\nB. 語音識別\nC. 圖像識別\nD. 網絡安全\n答案係: \n問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕駛\nB. 語音識別\nC. 圖像識別\nD. 網絡安全\n答案係: \n問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕駛\nB. 語音識別\nC. 圖像識別\nD. 網絡安全\n答案係: \n問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕駛\nB. 語音識別\nC. 圖像識別\nD. 網絡安全\n答案係: \n問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕駛\nB. 語音識別\nC. 圖像識別\nD. 網絡安全\n答案係: \n問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕駛\nB. 語音識別\nC. 圖像識別\nD. 網絡安全\n答案係: \n問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕駛\nB. 語音識別\nC. 圖像識別\nD. 網絡安全\n答案係: \n問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕駛\nB. 語音識別\nC. 圖像識別\nD. 網絡安全\n答案係: \n問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕駛\nB. 語音識別\nC. 圖像識別\nD. 網絡安全\n答案係: \n問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕駛\nB. 語音識別\nC. 圖像識別\nD. 網絡安全\n答案係: \n問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕駛\nB. 語音識別\nC. 圖像識別\nD. 網絡安全\n答案係: \n問題：以下唔係機器學習嘅應用範圍嘅是（）。\nA. 自動駕��",
    "correct_answer": "D",
    "standard_answer_range": "[['A', '樣本呈團狀分佈'], ['B', '樣本較多但典型性唔好'], ['C', '樣本呈鏈狀分佈'], ['D', '樣本較少但典型性好']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "一監獄人臉識別准入系統用嚟識別待進入人員嘅身份，呢系統一共包括識別4種唔同嘅人員：獄警，賊，送餐員，其他。下面邊種學習方法最適合呢種應用需求",
    "llm_output": "問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 二分類問題\nB. 多分類問題\nC. k-中心點聚類問題\nD. 層次聚類問題\n答案係: \n問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 多分類問題\nB. 二分類問題\nC. k-中心點聚類問題\nD. 層次聚類問題\n答案係: \n問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 二分類問題\nB. 多分類問題\nC. k-中心點聚類問題\nD. 層次聚類問題\n答案係: \n問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 多分類問題\nB. 二分類問題\nC. k-中心點聚類問題\nD. 層次聚類問題\n答案係: \n問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 二分類問題\nB. 多分類問題\nC. k-中心點聚類問題\nD. 層次聚類問題\n答案係: \n問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 二分類問題\nB. 多分類問題\nC. k-中心點聚類問題\nD. 層次聚類問題\n答案係: \n問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 二分類問題\nB. 多分類問題\nC. k-中心點聚類問題\nD. 層次聚類問題\n答案係: \n問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 二分類問題\nB. 多分類問題\nC. k-中心點聚類問題\nD. 層次聚類問題\n答案係: \n問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 二分類問題\nB. 多分類問題\nC. k-中心點聚類問題\nD. 層次聚類問題\n答案係: \n問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 二分類問題\nB. 多分類問題\nC. k-中心點聚類問題\nD. 層次聚類問題\n答案係: \n問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 二分類問題\nB. 多分類問題\nC. k-中心點聚類問題\nD. 層次聚類問題\n答案係: \n問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 二分類問題\nB. 多分類問題\nC. k-中心點聚類問題\nD. 層次聚類問題\n答案係: \n問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 二分類問題\nB. 多分類問題\nC. k-中心點聚類問題\nD. 層次聚類問題\n答案係: \n問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 二分類問題\nB. 多分類問題\nC. k-中心點聚類問題\nD. 層次聚類問題\n答案係: \n問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 二分類問題\nB. 多分類問題\nC. k-中心點聚類問題\nD. 層次聚類問題\n答案係: \n問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 二分類問題\nB. 多分類問題\nC. k-中心點聚類問題\nD. 層次聚類問題\n答案係: \n問題：下面哪種方法最適合用嚟識別一個人嘅身份\nA. 二分類問題\nB.",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '多分類問題'], ['B', '二分類問題'], ['C', 'k-中心點聚類問題'], ['D', '層次聚類問題']]",
    "is_correct": 1
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "為咗得到同 SVD 一樣嘅投射（projection），你需要喺 PCA 中點樣做",
    "llm_output": "問題：PCA 同 SVD 之間嘅最大分別係\nA. PCA 係一種統計方法，而 SVD 係一種數學方法\nB. PCA 係一種數學方法，而 SVD 係一種統計方法\nC. PCA 係一種統計方法，而 SVD 係一種統計方法\nD. PCA 係一種數學方法，而 SVD 係一種統計方法\n答案係: \n問題：PCA 同 SVD 之間嘅最大分別係\nA. PCA 係一種統計方法，而 SVD 係一種數學方法\nB. PCA 係一種數學方法，而 SVD 係一種統計方法\nC. PCA 係一種統計方法，而 SVD 係一種統計方法\nD. PCA 係一種數學方法，而 SVD 係一種統計方法\n答案係: \n問題：PCA 同 SVD 之間嘅最大分別係\nA. PCA 係一種統計方法，而 SVD 係一種數學方法\nB. PCA 係一種數學方法，而 SVD 係一種統計方法\nC. PCA 係一種統計方法，而 SVD 係一種統計方法\nD. PCA 係一種數學方法，而 SVD 係一種統計方法\n答案係: \n問題：PCA 同 SVD 之間嘅最大分別係\nA. PCA 係一種統計方法，而 SVD 係一種數學方法\nB. PCA 係一種數學方法，而 SVD 係一種統計方法\nC. PCA 係一種統計方法，而 SVD 係一種統計方法\nD. PCA 係一種數學方法，而 SVD 係一種統計方法\n答案係: \n問題：PCA 同 SVD 之間嘅最大分別係\nA. PCA 係一種統計方法，而 SVD 係一種數學方法\nB. PCA 係一種數學方法，而 SVD 係一種統計方法\nC. PCA 係一種統計方法，而 SVD 係一種統計方法\nD. PCA 係一種數學方法，而 SVD 係一種統計方法\n答案係: \n問題：PCA 同 SVD 之間嘅最大分別係\nA. PCA 係一種統計方法，而 SVD 係一種數學方法\nB. PCA 係一種數學方法，而 SVD 係一種統計方法\nC. PCA 係一種統計方法，而 SVD 係一種統計方法\nD. PCA 係一種數學方法，而 SVD 係一種統計方法\n答案係: \n問題：PCA 同 SVD 之間嘅最大分別係\nA. PCA 係一種統計方法，而 SVD 係一種數學方法\nB. PCA 係一種數學方法，而 SVD 係一種統計方法\nC. PCA 係一種統計方法，而 SVD 係一種統計方法\nD. PCA 係一種數學方法，而 SVD 係一種統計方法\n答案係: \n問題：PCA 同 SVD 之間嘅最大分別係\nA. PCA 係一種統計方法，而 SVD 係一種數學方法\nB. PCA 係一種數學方法，而 SVD 係一種統計方法\nC. PCA 係一種統計方法，而 SVD 係一種統計方法\nD. PCA 係一種數學方法，而 SVD 係一種統計方法\n答案係: \n問題：PCA 同 SVD 之間嘅最大分別係\nA. PCA 係一種統計方法，而 SVD 係一種數學方法\nB. PCA 係一種數學方法，而 SVD 係一種統計方法\nC. PCA 係一種統計方法，而 SVD 係一種統計方法\nD. PCA 係一種數學方法，而 SVD 係一種統計方法\n答案係: \n問題：PCA 同 SVD 之間嘅最大分別係\nA. PCA 係一種統計方法，而 SVD 係一種數學方法\nB. PCA 係一種數學方法，而 SVD 係一種統計方法\nC. PCA 係一種統計方法，而 SVD 係一種統計方法\nD. PCA 係一種數學方法，而 SVD 係一種統計方法\n答案係: \n問題：PCA 同 SVD 之間嘅最大分別",
    "correct_answer": "A",
    "standard_answer_range": "[['A', '將數據轉換成零均值'], ['B', '無法做到'], ['C', '將數據轉換成零眾數'], ['D', '將數據轉換成零中位數']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "喺統計模式分類問題中，當先驗概率未知時，可以使用",
    "llm_output": "問題：喺統計模式分類問題中，當先驗概率已知時，可以使用\nA. 最小誤判概率準則\nB. 最小最大損失準則\nC. 最小損失準則\nD. N-P判決\n答案係: \n問題：喺統計模式分類問題中，當先驗概率未知時，可以使用\nA. 最小誤判概率準則\nB. 最小最大損失準則\nC. 最小損失準則\nD. N-P判決\n答案係: \n問題：喺統計模式分類問題中，當先驗概率已知時，可以使用\nA. 最小誤判概率準則\nB. 最小最大損失準則\nC. 最小損失準則\nD. N-P判決\n答案係: \n問題：喺統計模式分類問題中，當先驗概率未知時，可以使用\nA. 最小誤判概率準則\nB. 最小最大損失準則\nC. 最小損失準則\nD. N-P判決\n答案係: \n問題：喺統計模式分類問題中，當先驗概率未知時，可以使用\nA. 最小誤判概率準則\nB. 最小最大損失準則\nC. 最小損失準則\nD. N-P判決\n答案係: \n問題：喺統計模式分類問題中，當先驗概率未知時，可以使用\nA. 最小誤判概率準則\nB. 最小最大損失準則\nC. 最小損失準則\nD. N-P判決\n答案係: \n問題：喺統計模式分類問題中，當先驗概率未知時，可以使用\nA. 最小誤判概率準則\nB. 最小最大損失準則\nC. 最小損失準則\nD. N-P判決\n答案係: \n問題：喺統計模式分類問題中，當先驗概率未知時，可以使用\nA. 最小誤判概率準則\nB. 最小最大損失準則\nC. 最小損失準則\nD. N-P判決\n答案係: \n問題：喺統計模式分類問題中，當先驗概率未知時，可以使用\nA. 最小誤判概率準則\nB. 最小最大損失準則\nC. 最小損失準則\nD. N-P判決\n答案係: \n問題：喺統計模式分類問題中，當先驗概率未知時，可以使用\nA. 最小誤判概率準則\nB. 最小最大損失準則\nC. 最小損失準則\nD. N-P判決\n答案係: \n問題：喺統計模式分類問題中，當先驗概率未知時，可以使用\nA. 最小誤判概率準則\nB. 最小最大損失準則\nC. 最小損失準則\nD. N-P判決\n答案係: \n問題：喺統計模式分類問題中，當先驗概率未知時，可以使用\nA. 最小誤判概率準則\nB. 最小最大損失準則\nC. 最小損失準則\nD. N-P判決\n答案係: \n問題：喺統計模式分類問題中，當先驗概率未知時，可以使用\nA. 最小誤判概率準則\nB. 最小最大損失準則\nC. 最小損失準則\nD. N-P判決\n答案係: \n問題：喺統計模式分類問題中，當先驗概率未知時，可以使用\nA. 最小誤判概率準則\nB. 最小最大損失準則\nC. 最小損失準則\nD. N-P判決\n答案係: \n問題：喺統計模式分類問題中，當先驗概率未知時，可以使用\nA. 最小誤判概率準則\nB. 最小最大損失準則\nC. 最小損失準則\nD. N-P判決\n答案係: \n問題：喺統計模式分類問題中，當先驗概率未知時，可以使用\nA. 最小誤判概率準則\nB. 最小最大損失準則\nC. 最小損失準則\nD. N-P判決\n答案係: \n",
    "correct_answer": "B",
    "standard_answer_range": "[['A', 'N-P判決'], ['B', '最小最大損失準則'], ['C', '最小損失準則'], ['D', '最小誤判概率準則']]",
    "is_correct": 0
  },
  {
    "model_name": "yi-1.5-6b-hf",
    "dataset": "CMMLU-machine_learning",
    "key_answer_type": "alphabet_option",
    "question": "以下邊啲方法唔可以直接嚟對文本分類",
    "llm_output": "決策樹\nKmeans\n支持向量機\nKNN",
    "correct_answer": "B",
    "standard_answer_range": "[['A', '決策樹'], ['B', 'Kmeans'], ['C', '支持向量機'], ['D', 'KNN']]",
    "is_correct": 0
  }
]